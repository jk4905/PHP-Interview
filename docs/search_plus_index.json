{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-18 12:15:05 "},"操作系统/简单知识.html":{"url":"操作系统/简单知识.html","title":"简单知识","keywords":"","body":"简单知识 进程与线程的区别： 概念上： 进程：一个程序分配资源的基本单位。 线程：一个进程内的基本调度单位，一个进程中可以包含多个线程。 从执行过程来看： 进程：拥有独立的内存单元，多个线程共享内存，从而提高应用程序的运行效率。 线程不能单独执行，必须存在于应用程序当中。 从逻辑角度来看 多线程的意义在于一个应用程序，有多个执行部分可以同时执行。但是，操纵系统并没有将多个线程看做是多个独立的应用，来实现进程的调度和管理及资源分配。 抢占式调度和非抢占式调度 非抢占式：分派程序一旦把处理机分配给某进程后便让它一直执行下去。 抢占式：操作系统将正在运行的进程强行暂停，由调度程序将 CPU 资源分配给其他就绪进程的方式。 协程： 可以理解为用户级线程。 协程与线程的区别是：线程是抢占式调度，协程是协同式调度，协程避免了无意义的调度，由此提高性能。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 17:28:34 "},"操作系统/Nginx.html":{"url":"操作系统/Nginx.html","title":"Nginx","keywords":"","body":"Nginx 实现负载均衡的几种方式 轮询 (默认) 请求按时间顺序逐一分配到不同服务器上，如果服务器挂了，自动剔除。 upstream backserver { server 192.168.0.14; server 192.168.0.15; } 加权轮询 指定轮询几率 upstream backserver { server 192.168.0.14 weight=3; server 192.168.0.15 weight=7; } ip_hash 上述方式存在一些问题，如用户第一次登录了一个服务器，第二次被定为到另一个服务器，那么登录信息将丢失。（虽然 session 可以存到 redis 中解决） 那么 ip_hash 可以很好解决这个问题，ip_hash 是将用户的 ip 地址通过 hash 分配给服务器。这样同一个用户就会指定到同一个服务器上进行访问。 upstream backserver { ip_hash; server 192.168.0.14; server 192.168.0.15; } 参考 【Nginx】实现负载均衡的几种方式 Nginx负载均衡配置 Nginx 的反向代理 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-22 14:34:19 "},"面试/分布式.html":{"url":"面试/分布式.html","title":"分布式","keywords":"","body":"高并发下，先操作数据库还是先操作缓存 读取缓存中是否有相关数据 如果缓存中有相关数据，则返回 如果缓存中没有相关数据，则从数据库中读取相关数据放入缓存中，再返回 如果有更新数据，先更新数据，再删除缓存 为了保证第四部删除缓存成功，使用 binlog 异步删除 如果是主数据库，binlog 取自于从库 如果是一主多从，每个从库都采集 binlog，然后消费端收到最后一台 binlog 后再删除缓存。 参考： 5个方案告诉你：高并发环境下，先操作数据库还是先操作缓存？ Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-28 22:32:13 "},"面试/高并发下，先操作数据库还是先操作缓存.html":{"url":"面试/高并发下，先操作数据库还是先操作缓存.html","title":"高并发下，先操作数据库还是先操作缓存","keywords":"","body":"面试题 redis Redis的原子性？什么是原子操作？ 答原子性（atomicity）:一个事务是一个不可分割的最小工作单位,要么都成功要么都失败。 原子操作是指你的一个业务逻辑必须是不可拆分的.比如你给别人转钱,你的账号扣钱,别人的账号。 增加钱,这个业务逻辑就是原子性的,这个操作就是原子操作,要么都成功要么都失败 redis事务无法实现原子性，只能实现隔离性和一致性。因为redis不支持回滚操作，所以如果事务中一条命令执行失败，既不会导致前面成功的命令回滚，也无法中断后面命令的继续执行。 数据库隔离级别 nginx 问nginx守护线程啥的。 php-fpm 两个面试官，第一个会问一些运维知识，服务器容灾，处理亿级并发量的问题，第二个问的比较少，偏php基本知识。大致他们的要求如下： 1.有独立开发过php框架，有一定的架构能力； 2.有相关运维能力，搭建api和服务器环境能抗住亿级pv； 3.有相关的api开发经验。 比如mysql索引有几个文件之类的 面试官会问很多底层的东西，如果想找一个处理10亿请求的架构师 3、面试自我介绍， 完成后问问题， 我是面试PHP， 会问PHP底层实现机制 4、运维能力，如果达到一定的运维能力， 多服务器部署，安全维护 QPS 多少 4核 8G 单台 400QPS pm = static pm.max_children = 500 PHP-FPM 数量 pm.max_requests = 1000 缓存穿透 缓存雪崩 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-28 22:19:37 "},"面试/git pull --rebase.html":{"url":"面试/git pull --rebase.html","title":"Git Pull Rebase","keywords":"","body":"git pull --rebase git pull = git fetch + git merge git pull --rebase = git fetch + git rebase 现在来看看git merge和git rebase的区别。 假设有3次提交A,B,C。 在远程分支origin的基础上创建一个名为\"mywork\"的分支并提交了，同时有其他人在\"origin\"上做了一些修改并提交了。 其实这个时候E不应该提交，因为提交后会发生冲突。如何解决这些冲突呢？有以下两种方法： 1、git merge 用git pull命令把\"origin\"分支上的修改pull下来与本地提交合并（merge）成版本M，但这样会形成图中的菱形，让人很困惑。 2、git rebase 创建一个新的提交R，R的文件内容和上面M的一样，但我们将E提交废除，当它不存在（图中用虚线表示）。由于这种删除，小李不应该push其他的repository.rebase的好处是避免了菱形的产生，保持提交曲线为直线，让大家易于理解。 区别： merge 遇到冲突后就不会进行下去了，手动修改冲突后，add 修改，commit 就可以了。 rebase 操作的话，会终端 rebase，同时提示解决冲突。解决冲突后，将 add 修改后，执行 git rebase -continue 继续操作，或者 git rebase -skip 忽略冲突。 参考： git pull和git pull --rebase的不同 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"面试/php 面试题.html":{"url":"面试/php 面试题.html","title":"php 面试题","keywords":"","body":"PHP 面试题 foreach 的用法，与引用 &$value) { # code... } var_dump($a); foreach ($a as $key => $value) { var_dump($a); exit; } var_dump($a); 输出两次的值： 1 2 3 1 2 2 为什么？ 因为在第一个循环中，$value 加了引用符号，那么 $value 引用值。 在第一个 foreach 中： 第一次循环 $value 是 $a[0] 的引用。 第二次循环 $value 是 $a[1] 的引用。 第三次循环 $value 是 $a[2] 的引用。 而循环结束时 $value 并没有被销毁，是 $a[2] 的引用，所以对 $value 赋值就是对 $a[2] 赋值。 在第二个 foreach 中，继续为 $value 赋值： 第一次循环，结束后 $value = $a[0] = $a[2] = 1,所以此时的 $a = [1,2,1]; 第二次循环，结束后 $value = $a[1] = $a[2] = 2,所以此时的 $a = [1,2,2]; 第二次循环，结束后 $value = $a[2] = $a[2] = 2,所以此时的 $a = [1,2,2]; 参考： php 中的引用(&)与foreach结合后的一个注意点 延迟动态绑定 输出：B 原因：static:: 不再被解析为定义当前方法所在的类，而是运行时所在的类。 而 self:: 或 CLASS 对当前类的引用，取决于定义当前方法的类。 输出 ACC array+array 与 array_merge 区别 'PHP', 'apache']; $arr2 = ['a'=>'PHP2', 'MySQl', 'HTML', 'CSS']; $mergeArr = array_merge($arr1, $arr2); $plusArr = $arr1 + $arr2; var_dump($mergeArr); var_dump($plusArr); echo PHP_EOL; echo PHP_EOL; echo PHP_EOL; $arr1 = ['PHP', 'apache']; $arr2 = ['PHP2', 'MySQl', 'HTML', 'CSS']; $mergeArr = array_merge($arr1, $arr2); $plusArr = $arr1 + $arr2; var_dump($mergeArr); var_dump($plusArr); 结果： array(5) { [\"a\"]=> string(4) \"PHP2\" [0]=> string(6) \"apache\" [1]=> string(5) \"MySQl\" [2]=> string(4) \"HTML\" [3]=> string(3) \"CSS\" } array(4) { [\"a\"]=> string(3) \"PHP\" [0]=> string(6) \"apache\" [1]=> string(4) \"HTML\" [2]=> string(3) \"CSS\" } array(6) { [0]=> string(3) \"PHP\" [1]=> string(6) \"apache\" [2]=> string(4) \"PHP2\" [3]=> string(5) \"MySQl\" [4]=> string(4) \"HTML\" [5]=> string(3) \"CSS\" } array(4) { [0]=> string(3) \"PHP\" [1]=> string(6) \"apache\" [2]=> string(4) \"HTML\" [3]=> string(3) \"CSS\" } [Finished in 0.0s] 区别： array_merge 会重新给下标排序。array+array 不会 当下标是数字时，array_merge 不会覆盖相同下标的值，而是按顺序重新排下标；array+array 则会以最先出现的值为结果，后面的值抛弃。 当下标为字母时，array_merge 后面的会覆盖相同值。而 array+array 则跟之前一样，会以最开始出现的值为结果，后面的值抛弃。 unset 输出结果： 1 unset 只会销毁 $b 的类型，没有修改 $a。 参考 系统的讲解 - SSO单点登录 array_map 与 array_walk 区别 array_map 是 array_map('函数名','数组') ,而 array_walk 是 array_walk('数组'，'函数名') array_map 可以使用自定义函数也可以使用系统函数，array_walk 只能使用系统函数 array_map 不改变原数组，而是得到一个新数组，而 array_walk 是改变原数组的值。 array_map 有返回值，array_walk 没有返回值。 foreach 的效率比 for 高。 array_walk 效率比 foreach 高。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-28 19:43:21 "},"数据结构/红黑树.html":{"url":"数据结构/红黑树.html","title":"红黑树","keywords":"","body":"红黑树 特性： 节点是红色或者黑色。 叶子节点是黑色的空节点。 根节点是黑色。 每个红色的节点下面都有两个黑色的节点。 从根节点到每个叶子节点的黑色节点数量相同。 参考： 最容易懂得红黑树 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:43:36 "},"网络协议/网络安全.html":{"url":"网络协议/网络安全.html","title":"网络安全","keywords":"","body":"网络安全 XSS 攻击 XSS 也叫跨脚本攻击（Cross Stie Scriping），攻击者会在 web 网页中插入恶意的 JS 代码，当用户浏览该页时，恶意代码就会被执行，达到攻击者目的。 如：在 form 表单中加入超链接标签，跳转到恶意网站。 \\\\XSS反射演示 然后输入，一个 js 代码，比如 alert('hack')。 输入之后点击 test： 插入的内容就变成 script 标签了。 数据流向：浏览器 -> 后端 -> 浏览器。 预防办法： 对用户提交的数据进行过滤 网页显示数据时，对数据进行处理使用 htmlspecialchars() 进行输出。 Test\", ENT_QUOTES); echo $new; // &lt;a href=&#039;test&#039;&gt;Test&lt;/a&gt; ?> XSS跨站脚本攻击(一)----XSS攻击的三种类型 SQL 注入 SQL 语句中没有过滤传入的参数，篡改了 SQL 语句，达到攻击的目的。 如： select * from user where name='abc' and password='123'; # 攻击者使用传入 password 为 ' or 1=1； select * from user where name='abc' and password='' or 1=1; # 或者使用 # 或 -- ,输入完用户名后直接跟 # 号或者双横线，后面密码任意，即注释了密码。 select * from user where name='abc'#' and password='123123'; 预防： 严格校验参数，正则等。 转义特殊字符，使用 addslashes()。 利用 PDO 的预编译机制。可以理解为整体替换 如果 $name 变量包含了’Sarah’; DELETE FROM employees 这个结果只会简单的搜索字符串“‘Sarah’; DELETE FROM employees”，所以你不会得到一张空表。 CSRF 攻击 CSRF 跨站请求伪造。 登录受信任的网站 A，并在本地生成了 cookie。 再不登出 A 的情况下，访问了危险网站 B。 B 网站以用户身份访问 网站 A ，并执行恶意请求。 预防： CSRF-TOKEN：该令牌用于验证用户是否是向应用程序发送请求的用户。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"网络协议/HTTP协议.html":{"url":"网络协议/HTTP协议.html","title":"HTTP协议","keywords":"","body":"HTTP 协议 常见状态码： 200：请求成功。 201：成功请求并创建新的资源。 202：已接收但并未完成。 203：非授权信息。 204：无内容 300： 301：永久重定向 302：临时重定向 304：所请求的资源未修改。 解决：cache-control：no-cache 400：服务器不理解请求的语法 401：未授权。 403：禁止 404：未找到 405：方法被禁用 500：服务器内部错误 503：服务不可用 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-28 20:26:48 "},"网络协议/TCP-IP.html":{"url":"网络协议/TCP-IP.html","title":"TCP IP","keywords":"","body":"TCP/IP OSI 七层模型 硬件层：在物理通道上进行比特流传输。(以太网, IEEE 802.2 等) 数据链路层：实现无差错地将数据帧（一组电信号）从一个节点传到另一个节点上。（Wi-Fi(IEEE 802.11) , WiMAX(IEEE 802.16), GPRS, HDLC, PPP 等协议） 网络层：实现将数据分组从一个主机传送到另一个主机上。(IP, ICMP, IGMP, ARP, RARP, OSPF 等协议) IP 协议：判断 IP 地址是否在本地网络。 ARP 协议：地址解析协议，根据 IP 地址获取 MAC 地址。 路由协议：通过 IP 地址判断两台主机是否在同一个网络，在就发送数据包，不在就将数据包发送给网关，网关进行多次转发找到目标 IP，再通过 ARP 获取 MAC 地址。 传输层：实现某台主机的某进程与另一台主机的某进程之间的数据传输。(TCP, UDP 等协议) 会话层：实现不同机器上的用户建立、维护、终止会话关系。（ZIP, ASP, SSH 等协议） 表示层：确保各种通信设备之间能够互相操作，不考虑数据内部表示。（SSL 等协议） 应用层：使用户能够访问网络，为各类应用提供相应的服务和接口支持。（HTTP, FTP, SMTP, POP3, DHCP, DNS等协议） TCP/IP 四层模型 链路层：对 0 和 1 进行分组，定义数据帧，确认主机物理位置，传输数据。 网络层：定义 IP 地址，确认主机所在网络位置，并通过 IP 进行 MAC 寻址，对外网数据包进行路由转发。 传输层：定义接口，确认主机上应用程序的身份，并将数据包交给对应的应用程序。 应用层：定义数据格式，并按照对应格式解析数据 一句话解释模型： 首先，应用层对请求包做格式定义；传输层加上端口号，确定双方的通信的应用程序；网络协议加上 IP 地址，确定双方的网络位置；链路层加上双方 MAC 地址，确认双方的物理位置，同时将数据分组，形成数据帧，以广播的形式发送给对方主机；对于不同网段，首先数据包会转发给网关路由器，经过多次转发，送达到目标主机；目标主机收到数据包后，将数据进行组装，然后一层一层的解析，最终被应用层协议解析交给服务器处理。 TCP 与 UDP 的区别 TCP 面向连接，可靠，速度慢，效率低。 UDP 无连接，不可靠，速度快，效率高。 UDP UDP： 不需要大量数据结构进行处理。 轻信他人，随便谁都可以给它传数据。 需要处理速度快，容忍丢包。 应用场景： 直播。对实时性要求高，宁可丢包也不要卡顿。 实时游戏：降低延迟。 TCP 介绍： 序号：Seq 号，32 位。标识源端对目的端发送的字节流。 确认序号：Ack 号，32 为，只有 ACK 标志位为 1 时，确认序号才有效。Ack=Seq+1。 标志位： URG：紧急指针（urgent pointer）有效。 ACK：确认序号有效。 PSH：接收方应该尽快将这个报文交给应用层。 RST：重置连接。 SYN：发起一个新连接。 FIN：释放一个连接。 注意： ACK 与 Ack 不是同一个东西。ACK 是标志位中的。Ack 是确认序号。 TCP 的三次握手 如 Client（C） Server（S） 是两个人，分别站在马路对面，准备隔空喊话： C：向 S 招手（发送 SYN） S：看到了 C 的招手，向 C 点头微笑（ACK）。这个时候 C 进入了 established 状态。但是 S 还不确定 C 是不是在对别人招手。所以向 C 招手（发送 SYN）。 C：看到了 S 的招手，向 S 点头微笑（ACK ）。 S：S 看到后，进入了 established 状态。 TCP 数据传输 当 S C 建立连接后，就可以传数据了。 C 喊一句 S (传 data)，S 就回复一句（ACK）。 当 C 喊了第一句 S 没听到，就需要重新再喊一次（重传）。 既然重传，那么 S 也可能听到两次 C 的话，那么需要去重。 TCP 的四次挥手 TCP 的四次挥手分解成 C 挥手（FIN）- S 伤感微笑（ack）- S 挥手（FIN）- C 伤感微笑（ack） 为什么要四次挥手 当 C 第一次挥手时，仅仅表示不再给 S 说话了，但是 C 还是能听到 S 的说话。等 S 说完了（数据发送完成），那么 S 就挥手给 C，C 微笑后 S 离开。 四次挥手释放连接时，等待2MSL的意义？ 确保 C 发送的最后报文 （微笑）能够到达 S，但是可能报文会丢失（S 没看到）。如果超时 A 会重传。 2MSL 是指 两倍的最大存活时间(Maximum Segment Lifetime)。即 一个发送和一个接受说需要的最大时间。如果超过了，则表示 C 没有再次收到 S 的消息，就表明连接已经断开了。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Laravel/定义辅助函数.html":{"url":"Laravel/定义辅助函数.html","title":"定义辅助函数","keywords":"","body":"辅助函数 有时我们需要使用自定义的函数，并全局使用。 假如我们在 boostrap/helpers.php 中自定义函数 这个时候，打开 tinker 发现是找不到该函数的。 这个时候我们利用 composer 的 autoload 来引入。 首先打开 composer.json 文件，在 autoload 中添加 files，并加入 boostrap/helpers.php 文件。 \"autoload\": { \"classmap\": [ \"database/seeds\", \"database/factories\" ], \"psr-4\": { \"App\\\\\": \"app/\" }, \"files\": [ \"bootstrap/helpers.php\" ] }, 然后执行 composer dumpautoload 之后就可以打开了。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Laravel/基础知识.html":{"url":"Laravel/基础知识.html","title":"基础知识","keywords":"","body":"基础知识 迁移的好处 命令： # 创建迁移 php artisan make:migration # 执行迁移 php artisan migrate # 回滚迁移 php artisan migrate:rollback # 重置数据库 php artisan migrate:refresh 好处： 多人并行开发 代码版本管理 数据库版本控制 - 如：更新/重置/回滚 兼容多种数据库 部署方便 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-20 13:49:59 "},"Laravel/请求周期.html":{"url":"Laravel/请求周期.html","title":"请求周期","keywords":"","body":"生命周期 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 16:48:40 "},"Laravel/实现方式.html":{"url":"Laravel/实现方式.html","title":"实现方式","keywords":"","body":"项目中的实现方式 购物车 存入 cart_items 表中。表中字段： 大批量同步数据到 Elasticsearch 中 将数据通过 chunkById 进行分组，避免加载过多数据，并使用 bulk 命令导入到 Elasticsearch中（bulk 不会影响和中断其他操作）。 分面搜索 就是将商品具有的相同属性，作为筛选项进行筛选。在查询条件中增加 aggs。 aggs： 首先确定字段是 properites。 properties 本身是一个 nested（嵌套）的字段。 取出类型名称。 取出类型值。 相似商品 通过 should 来实现，在设置索引是，给字段加权，会得到一个分数，然后排序取得得分最高的前几个。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Laravel/事件与监听.html":{"url":"Laravel/事件与监听.html","title":"事件与监听","keywords":"","body":"事件 创建一个事件 php artisan make:event 创建后，会得到 app/Events/.php 文件。事件本身不需要什么逻辑。 use App\\Models\\Order; . . . class OrderPaid { use Dispatchable, InteractsWithSockets, SerializesModels; protected $order; public function __construct(Order $order) { $this->order = $order; } public function getOrder() { return $this->order; } } 然后创建监听者： php artisan make:listener --event= 会得到一个 app/Listeners/.php 当有了事件和监听者之后，还需要将两者关联起来。 在 app/Providers/EventServiceProvider.php 文件中。 use App\\Events\\OrderPaid; use App\\Listeners\\UpdateProductSoldCount; . . . protected $listen = [ . . . OrderPaid::class => [ UpdateProductSoldCount::class, ], ]; Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-20 21:32:30 "},"Laravel/延迟任务.html":{"url":"Laravel/延迟任务.html","title":"延迟任务","keywords":"","body":"延迟任务 当触发一个延迟任务时，laravel 会自动换算指定时间戳，将时间戳和任务序列化到队列中，laravel 队列不断查询队列中满足条件的任务。 生成一个任务： php artisan make:job 编辑 app/Jobs/.php order = $order; // 设置延迟的时间，delay() 方法的参数代表多少秒之后执行 $this->delay($delay); } // 定义这个任务类具体的执行逻辑 // 当队列处理器从队列中取出任务时，会调用 handle() 方法 public function handle() { // 判断对应的订单是否已经被支付 // 如果已经支付则不需要关闭订单，直接退出 if ($this->order->paid_at) { return; } // 通过事务执行 sql \\DB::transaction(function() { // 将订单的 closed 字段标记为 true，即关闭订单 $this->order->update(['closed' => true]); // 循环遍历订单中的商品 SKU，将订单中的数量加回到 SKU 的库存中去 foreach ($this->order->items as $item) { $item->productSku->addStock($item->amount); } }); } } 最终在控制器触发任务即可： use App\\Jobs\\CloseOrder; . . . public function store(OrderRequest $request) { . . . // 触发任务 $this->dispatch(new CloseOrder($order, config('app.order_ttl'))); return $order; } 执行原理 当触发任务时，laravel 会将任务 zadd 到一个名叫 queues:default:delayed 的 zset 数据结构中。 然后通过 zrangebyscore 和 zrangebyrank 取出 -inf 到 now 的任务。用 rpush 存入 list 数据结构中。 然后把 list 队列中的任务 lpop 出来，尝试次数+1 然后 zadd 到 queues:default:reserved ，程序处理这里面的任务。 处理完成后删除 queues:default:reserved 的任务。 参考： Laravel 的消息队列剖析 Laravel 基于redis队列的解析 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-21 16:00:14 "},"Laravel/语义化版本.html":{"url":"Laravel/语义化版本.html","title":"语义化版本","keywords":"","body":"语义化版本 版本格式：主版本号.次版本号.修订号。如 1.0.1, 3.2.39。 主版本号：当做了不兼容的 API 修改。 次版本号：当做了向下兼容的功能性更新。 当做了向下兼容的问题修复。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-20 14:56:33 "},"Laravel/中间件.html":{"url":"Laravel/中间件.html","title":"中间件","keywords":"","body":"中间件 作用：中间件提供了一种方便的机制用于过滤进入应用程序的 HTTP 请求。 定义中间件： php artisan make:middleware 这个命令会在 app/Http/Middleware 下生成一个 类。可以将中间件想象成一些列 HTTP 请求，必须进过中间件才能访问应用层。 前置 & 后置中间件 中间件执行的前后顺序，取决于中间件本身。 这个是前置执行，注意注释和 $next($request)的位置 后置执行： 注册中间件 只需要在 app/Http/kernel.php 文件中注册就行。 如： // 在 App\\Http\\Kernel 类中... protected $routeMiddleware = [ 'auth' => \\App\\Http\\Middleware\\Authenticate::class, 'auth.basic' => \\Illuminate\\Auth\\Middleware\\AuthenticateWithBasicAuth::class, 'bindings' => \\Illuminate\\Routing\\Middleware\\SubstituteBindings::class, 'cache.headers' => \\Illuminate\\Http\\Middleware\\SetCacheHeaders::class, 'can' => \\Illuminate\\Auth\\Middleware\\Authorize::class, 'guest' => \\App\\Http\\Middleware\\RedirectIfAuthenticated::class, 'signed' => \\Illuminate\\Routing\\Middleware\\ValidateSignature::class, 'throttle' => \\Illuminate\\Routing\\Middleware\\ThrottleRequests::class, 'verified' => \\Illuminate\\Auth\\Middleware\\EnsureEmailIsVerified::class, ]; 然后就可以直接使用： # 可以分配多个 Route::get('/', function () { // })->middleware('first', 'second'); Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-20 14:13:32 "},"Laravel/Deployer-自动部署.html":{"url":"Laravel/Deployer-自动部署.html","title":"Deployer-自动部署","keywords":"","body":"Deployer-自动部署 Deployer 是一个使用 PHP 开发的轻量级的自动部署工具。 其原理就是通过 SSH 的方式登录服务器，然后执行定义好的 shell 脚本。 安装： $ composer global require deployer/deployer 通过 dep 命令查看是否安装成功 $ mkdir -p ~/Code/deploy-laravel-shop $ cd ~/Code/deploy-laravel-shop $ dep init 是否允许匿名收集信息 打开 deploy.php 文件 set('deploy_path', '~/{{application}}'); // 定义一个名为 build 的任务 task('build', function () { // 这个任务的内容是执行 cd ~/my_project && build 命令 run('cd {{release_path}} && build'); }); // 定义一个后置钩子，当 deploy:failed 任务被执行之后，Deployer 会执行 deploy:unlock 任务 after('deploy:failed', 'deploy:unlock'); // 定义一个前置钩子，在执行 deploy:symlink 任务之前先执行 artisan:migrate before('deploy:symlink', 'artisan:migrate'); 开始部署： user('root') // 使用 root 账号登录 ->identityFile('~/.ssh/laravel-shop-aliyun.pem') // 指定登录密钥文件路径 ->become('www-data') // 以 www-data 身份执行命令 ->set('deploy_path', '/var/www/laravel-shop-deployer'); //第二台 host('47.111.141.124') ->user('root') ->identityFile('~/.ssh/laravel-shop-aliyun.pem') ->become('www-data') ->set('deploy_path', '/var/www/laravel-shop'); // 第二台的部署目录与第一台不同 host('121.40.77.53') ->user('root') ->identityFile('~/.ssh/laravel-shop-aliyun.pem') ->become('www-data') ->set('deploy_path', '/var/www/laravel-shop'); // 第二台的部署目录与第一台不同 desc('Upload .env file'); task('env:upload', function () { // 将本地的 .env 文件上传到代码目录的 .env upload('.env', '{{release_path}}/.env'); }); // 定义一个前端编译的任务 desc('Yarn'); task('deploy:yarn', function () { // release_path 是 Deployer 的一个内部变量，代表当前代码目录路径 // run() 的默认超时时间是 5 分钟，而 yarn 相关的操作又比较费时，因此我们在第二个参数传入 timeout = 600，指定这个命令的超时时间是 10 分钟 run('cd {{release_path}} && SASS_BINARY_SITE=http://npm.taobao.org/mirrors/node-sass yarn && yarn production', ['timeout' => 600]); }); // 定义一个 执行 es:migrate 命令的任务 desc('Execute elasticsearch migrate'); task('es:migrate', function () { // {{bin/php}} 是 Deployer 内置的变量，是 PHP 程序的绝对路径。 run('{{bin/php}} {{release_path}}/artisan es:migrate'); })->once(); desc('Restart Horizon'); task('horizon:terminate', function() { run('{{bin/php}} {{release_path}}/artisan horizon:terminate'); }); // 定义一个后置钩子，在 deploy:shared 之后执行 env:upload 任务 after('deploy:shared', 'env:upload'); // 定义一个后置钩子，在 deploy:vendors 之后执行 deploy:yarn 任务 after('deploy:vendors', 'deploy:yarn'); // 在 deploy:vendors 之前调用 deploy:copy_dirs before('deploy:vendors', 'deploy:copy_dirs'); // 路由 after('artisan:config:cache', 'artisan:route:cache'); // 定义一个后置钩子，在 artisan:migrate 之后执行 es:migrate 任务 after('artisan:migrate', 'es:migrate'); // 在 deploy:symlink 任务之后执行 horizon:terminate 任务 after('deploy:symlink', 'horizon:terminate'); // 定义一个后置钩子，当 deploy:failed 任务被执行之后，Deployer 会执行 deploy:unlock 任务 after('deploy:failed', 'deploy:unlock'); // 定义一个前置钩子，在执行 deploy:symlink 任务之前先执行 artisan:migrate before('deploy:symlink', 'artisan:migrate'); Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Laravel/Laravel 的请求周期.html":{"url":"Laravel/Laravel 的请求周期.html","title":"Laravel 的请求周期","keywords":"","body":"Laravel 的生命周期 PHP 的生命周期 PHP 有两种运行模式，web 模式和 cli（命令行） 模式。 当我们在终端中敲 PHP 命令时使用的是命令行模式。 而当使用 nginx 或者别的服务器作为宿主时，使用的事 web 模式。 当我们请求一个 php 文件时，比如 laravel 中的 public\\index.php 文件，php 为了完成这个请求，经历了 5 个步骤： 模块初始化（MINIT），即调用 php.ini 中指明的扩展的初始化函数的初始化工作，如 mysql 扩展。 请求初始化（RINIT），即初始化执行本次脚本的所需要的变量名和变量值内容的符号表，如 $_SESSION 变量 执行该 PHP 脚本 请求处理完成（Request Shutdown）,按顺序调用各个模块的 RSHUTDOWN 方法，对每个变量调用 unset 函数，如 unset $_SESSION 变量。 关闭模块（Module Shutdown），PHP 调用每个扩展的 MSHUTDOWN 方法，这是各个模块最后一次释放内存的机会，这意味着没有下一个请求。 web 模式与 CLI 模式的不同： CLI 模式每次都会执行完整的 5 个周期，因为脚本不执行完不会有下一个请求。 WEB 为了应对并发，可能采用多线程，因此生命周期 1 5 只执行了一次，2-4 阶段重复执行，节省系统模块初始化所带来的开销。 作用： 优化 laravel 的代码，并且更加了解 singleton（单例）模式。每次请求，php 最后都会将变量 unset，所以 singleton 只会存在一个请求中，请求之间不能共享。因此，记住 PHP 是一个脚本语言，所有变量只会在一次请求中生效，下次请求时会被充值，而不像 java 静态变量拥有全局作用。 Laravel 的生命周期 Laravel 的请求周期是用处 public\\index.php 开始，从 public\\index.php 结束。 public\\index.php 总共执行了四个步骤： // 1. 执行 composer 的自动加载，自动生成 class loader，包括 composer require 的所有依赖 require __DIR__.'/../bootstrap/autoload.php'; // 2. 生成容器 Container，Applition 实例，并向容器中注册核心组件（HttpKernel，ConsoleKernel, ExceptionHandle） $app = require_once __DIR__.'/../bootstrap/app.php'; $kernel = $app->make(Illuminate\\Contracts\\Http\\Kernel::class); // 3. 处理请求，生成并发送响应。 $response = $kernel->handle( $request = Illuminate\\Http\\Request::capture() ); $response->send(); // 4. 请求结束，执行回调。（可终止中间件，就在这里回调） $kernel->terminate($request, $response); 启动 Laravel 的基础服务 第一步注册加载 composer 自动生成 class loader 就是加载初始化第三方依赖。不属于 Laravel 内核。（spl_autoload_register 注册给定函数，作为 __autoload() 的实现） 第二步生成容器 container，并向容器内注入核心组件，因为牵涉到容器 container 和合同 Contracts 后面详讲。 第三步（重点），处理请求，生成并发送响应。 首先 laravel 框架会捕获到用户发送到 public\\index.php 的请求。生成 Illuminate\\Http\\Request 实例，传递给 handle 方法。 在方法内部，将该 $request 实例绑定到第二步生成的容器中。 然后在该请求真正处理之前，调用 bootstrap 方法，进行必要的加载和注册，如环境监测，加载配置，注册 Facades (门面)，注册服务提供者，启动服务提供者等等。这是一个启动数组，具体在 Illuminate\\Foundation\\Http\\Kernel 中，包括： protected $bootstrappers = [ 'Illuminate\\Foundation\\Bootstrap\\DetectEnvironment', 'Illuminate\\Foundation\\Bootstrap\\LoadConfiguration', 'Illuminate\\Foundation\\Bootstrap\\ConfigureLogging', 'Illuminate\\Foundation\\Bootstrap\\HandleExceptions', 'Illuminate\\Foundation\\Bootstrap\\RegisterFacades', 'Illuminate\\Foundation\\Bootstrap\\RegisterProviders', 'Illuminate\\Foundation\\Bootstrap\\BootProviders', ]; laravel 是按顺序遍历执行注册这些基础服务的，注意顺序：facades 先于 ServiceProviders ，注册 facades 就是注册了 config\\app.php 中的 aliases 数组，你使用的很多类，如 Auth，Cache, DB 等等都是Facades；而 ServiceProviders 的 register 方法永远在 boot 之前，以避免 boot 方法依赖某个实例而实例未注册的现象。 将请求传递给路由 到目前为止，laravel 还没有执行到你写的主要代码（除 ServiceProviders 中以外），因为还没有将请求传递给路由 在 laravel 基础服务启动后，就要把请求传给路由。传递路由是通过 pipeline 来传递，在 pipeline 有一堵墙，在传递路由之前的请求都要经过，这个墙被定义在 app\\Http\\Kernel.php 中的 $middleware 数组中，这就是中间件。默认只有一个 CheckForMaintenanceMode 中间件，用于检测你的网站是否暂时关闭。这是一个全局中间件，所有的请求都要经过，也可以自定义全局中间件。 然后开始遍历所有注册的路由，找到第一个符合条件的路由，经过它的路由中间件，进入到控制器或者闭包函数，执行你的代码。 所以所有的请求到达你写的代码之前，都经过了重重检测，确保不符合和恶意的请求被 laravel 拒之门外。 服务容器 服务容器就是个普通的容器，用来装类的实例，然后需要用时在取出来。服务容器实现了依赖反转（Inversion of Control，缩写为IoC）。 正常情况下 A 需要用 B 要手动 new 个 B，意味着需要知道 B 的细节。比如构造函数等，但是随着项目变大，这种依赖是毁灭性的。依赖反转的意思是，将 A 主动获取 B 类的过程颠倒过来，类 A 只需要声明它需要什么，然后由容器提供。 这样做的好处是，A 不再依赖 B 的实现，一定程度上解决了耦合。 在 Laravel 中实现依赖反转有两种： 依赖注入 绑定 依赖注入 class UserController extends Controller { /** * The user repository implementation. * * @var UserRepository */ protected $users; /** * Create a new controller instance. * * @param UserRepository $users * @return void */ public function __construct(UserRepository $users) { $this->users = $users; } } 这里在 UserController 中需要一个 UserRepository 的实例，我们只需要在构造方法中声明我们需要的类型，容器在实例化 UserController 时会自动生成 UserRepository 实例。这样也就避免了了解 UserRepository 的细节和产生的依赖。 绑定 绑定操作一般在 ServiceProviders 中的 register 方法中，最基本的是 bind 方法，它接受一个类名和闭包函数来获取实例： $this->app->bind('XblogConfig', function ($app) { return new MapRepository(); }); 还有一个 singleton 方法，单例模式下绑定。 当然，也可以绑定一个已存在的对象到实例中： $this->app->instance('request',$request); 绑定之后，可以通过下面几种方式获取： app('requrest'); app()['request']; app()->make('request'); resolve('request'); bind 方法和 singleton 方法唯一的区别是，bind 方法的闭包都会每次调用以上 4 种方法之一时执行，而 singleton 方法只会执行一次。 如果想每一个类都获取不同实例，或者需要“个性化”的实例时，这个时候需要 bind 方法，以免这次使用会对下一次使用造成干扰。 如果实例化一个比较费时或者不依赖生成的上下文，可以使用 singleton 方法绑定，其好处是如果在某一次请求中多次使用了某个类，那么只实例化一次会节省空间和时间。 $app->singleton( Illuminate\\Contracts\\Http\\Kernel::class, App\\Http\\Kernel::class ); 在 laravel 生命周期的第二部，laravel 默认（在bootstrap\\app.php文件中）绑定了 Illuminate\\Contracts\\Http\\Kernel，Illuminate\\Contracts\\Console\\Kernel，Illuminate\\Contracts\\Debug\\ExceptionHandler 接口的实现类。 还有一种上下文绑定，就是相同的接口，在不同类中可以自动获取不同的实现： $this->app->when(PhotoController::class) ->needs(Filesystem::class) ->give(function () { return Storage::disk('local'); }); $this->app->when(VideoController::class) ->needs(Filesystem::class) ->give(function () { return Storage::disk('s3'); }); 在上述声明中，同样的接口 Filesystem ，使用依赖注入时，PhotoController 获取的是 local 实例而 VideoController 获取的是 S3 实例。 Contracts & Facades（合同&门面） laravel 的一个强大之处是，在配置文件中声明缓存类型(redis，memcached，file......) laravel 就会自动帮你切换成这种驱动了，而不需要修改逻辑和代码。laravel 定义了一系列的 Contracts ，本质是一些 php 接口，一系列标准，用来解耦具体需求对实现的依赖关系。 上图在不是用 Contracts 时，对于一种逻辑，只会有一种结果。如果需求变更，就需要重构代码和逻辑。 但是在使用 Contracts 之后，我们只需要按照接口写好逻辑，然后提供不同的实现，就可以再不动代码和逻辑的情况下得到更加多态的结果。 例子： 定义好接口： namespace App\\Contracts; use Closure; interface XblogCache { public function setTag($tag); public function setTime($time_in_minute); public function remember($key, Closure $entity, $tag = null); public function forget($key, $tag = null); public function clearCache($tag = null); public function clearAllCache(); } 实现具体缓存。 class Cacheable implements XblogCache { public $tag; public $cacheTime; public function setTag($tag) { $this->tag = $tag; } public function remember($key, Closure $entity, $tag = null) { return cache()->tags($tag == null ? $this->tag : $tag)->remember($key, $this->cacheTime, $entity); } public function forget($key, $tag = null) { cache()->tags($tag == null ? $this->tag : $tag)->forget($key); } public function clearCache($tag = null) { cache()->tags($tag == null ? $this->tag : $tag)->flush(); } public function clearAllCache() { cache()->flush(); } public function setTime($time_in_minute) { $this->cacheTime = $time_in_minute; } } 不使用缓存： class NoCache implements XblogCache { public function setTag($tag) { // Do Nothing } public function setTime($time_in_minute) { // Do Nothing } public function remember($key, Closure $entity, $tag = null) { /** * directly return */ return $entity(); } public function forget($key, $tag = null) { // Do Nothing } public function clearCache($tag = null) { // Do Nothing } public function clearAllCache() { // Do Nothing } } 再利用容器绑定，根据不同配置获得不同结果： public function register() { $this->app->bind('XblogCache', function ($app) { if (config('cache.enable') == 'true') { return new Cacheable(); } else { return new NoCache(); } }); } 实际上，Laravel所有的核心服务都是实现了某个 Contracts 接口（都在Illuminate\\Contracts\\文件夹下面），而不是依赖具体的实现，所以完全可以在不改动框架的前提下，使用自己的代码改变 Laravel 框架核心服务的实现方式。 Facades,在我们把类的实例绑定到容器的时候相当于给类起了个别名，然后覆盖 Facade 的静态方法 getFacadeAccessor 并返回你的别名，然后你就可以使用你自己的 Facade 的静态方法来调用你绑定类的动态方法了。其实 Facade 类利用了 __callStatic() 这个魔术方法来延迟调用容器中的对象的方法，这里不过多讲解，你只需要知道 Facade 实现了将对它调用的静态方法映射到绑定类的动态方法上，这样你就可以使用简单类名调用而不需要记住长长的类名。这也是 Facades 的中文翻译为假象的原因。 总结 Laravel 提供了强大的脚手架，如 orm，carbon 时间处理。laravel 的核心是容器和抽象解耦，实现高扩展性。 学习 Laravel 的设计模式和思想： 理解 laravel 的生命周期和请求生命周期的概念。 所有的静态变量和单例，都会在下一次请求到来时重新初始化。 将耗时且调用频繁的类用 singleton 绑定。 将变化的选项抽象为 Contracts，依赖接口而不依赖具体实现。 善于引用 laravel 提供的容器。 参考 Laravel的核心概念 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/01.初识 MySQL.html":{"url":"MySQL/01.初识 MySQL.html","title":"01.初识 MySQL","keywords":"","body":"01.初识 MySQL 1.bin 目录下的可执行文件 我的 macOS 上路径为 /usr/local/Cellar/mysql/8.0.17/bin/。 mysqld mysqld 就代表着 Mysql 的服务器程序，运行这个程序就可以直接启动一个服务器进程。但是不常用。 mysqld_safe mysqld_safe 是一个启动脚本，它会间接启动 mysqld，并启动一个监控进程，当 mysqld 挂了的时候，可以帮助重启它。mysqld_safe 还会产生一个错误日志，方便查询错误。 mysql.server mysql.server 是一个启动脚本，会间接调用 mysqld_safe。它其实是一个链接，链接到 mysql.server -> ../support-files/mysql.server。 # 启动 mysql.server mysql.server start # 关闭 mysql.server stop mysqld.multi 可以在同一台计算机上开多个服务器实例，也就是运行多个 mysql 进程。可以对每个进程的启动和停止做监控。 2.启动 mysql mysql -h主机名 -u用户名 -P端口号 -p密码 # 不要明文在 -p 后面写密码，而是通过回车来输入密码 Enter password: 3.服务器处理客户端请求 客户端请求服务器流程 连接管理 客户端进程通过 TCP/IP、Unix 嵌套字等方式来与服务器进程建立连接。每当有一个客户端进程连接到服务器时，服务器进程会创建一个线程来与客户端交互，当客户端退出时，服务器端不会立即销毁该线程，而是把他缓存起来，等下个客户端连接时，把这个缓存的线程分配给新客户端，这样避免了频繁创建和销毁线程的作用，节省了服务器资源。 这里的连接方式，和 PHP-FPM 很像，不过 PHP-FPM 是通过一个 master 进程来管理 多个 worker 进程，当服务器有请求时，master 会把请求分发给其中一个 worker 来处理，处理完之后返回给服务器。并不会销毁 worker 进程，也避免了频繁创建和销毁进程的作用，借阅服务器资源 解析与优化 执行一条 sql 语句，mysql 会通过查询缓存、语法解析、查询优化三个方面来处理。 查询缓存 即当一个客户端执行完一条 sql 语句后，另一个客户端再执行完全相同的 sql 语句时，mysql 不会重复执行，它会把第一次查询的内容存入缓存，第二次查询的时候直接从缓存中取出结果。 值得注意的事，首先只有当查询语句完全相同时（包括空格，注释，大小写），mysql 才会命中缓存。还有请求的是系统表和系统函数都不会被缓存。 虽然查询缓存有时可以提升系统性能，但也不得不因维护这块缓存⽽造成⼀些开销，⽐如每次都要去查询缓存中检索，查询请求处理完需要更新查询缓存，维护该查询缓存对应的内存区域。从 MySQL5.7.20 开始，不推荐使⽤查询缓存，并在 MySQL8.0 中删除。 语法解析 当查询缓存没有命中，mysql 服务器就会将传过来的 sql 语句进行分析、判断语法是否正确，然后从⽂本中将要查询的表、各种查询条件 都提取出来放到 MySQL 服务器内部使⽤的⼀些数据结构上来。 查询优化 语法解析之后，服务器得到了需要的信息，如查询哪张表，哪些字段，什么条件，由于我们自己写的 sql 语句执行效率可能并不高，所以 mysql 的优化程序会做一些优化，如外连接转换为内连接、表达式简化、⼦查询转为连接等。 存储引擎 mysql 把数据的存储和操作都封装到了一个叫「存储引擎」的模块中。存储引擎封装着如物理上如何表示记录，如何把数据写入物理存储器，使用什么方法读取出来等等。 最常用的存储引擎是 InnoDB，MyISAM 。 InnoDB，行锁，支持事务与分布式事务，事务回滚。不支持全文索引 FULLTEXT。写慢，清空表时是一行行删除。 MyISAM，表索，不支持事务，支持全文索引，写快。 操作 创建表指定存储引擎 create table 表名( 建表语句 )ENGINE = 存储引擎名称 修改表引擎 ALTER TABLE 表名 ENGINE = 存储引擎名称 查看引擎 SHOW CREATE TABLE 表名 4.启动选项和配置文件 在命令⾏上使⽤选项 在启动时，可以增加选项来配置参数，如启动 mysql 时的 -h 选择主机名。 在启动时，在选项名加 -- 前缀，如果选项名是多个单词可以用下划线 _ 或者横杠 - 来链接。查询有哪些选项可以通过 --help 来查询。 mysqld --skip_networking # 这两种写法等价 mysqld --skip-networking # 指定默认引擎为 MyISAM mysqld --default-storage-engine=MyISAM 选项有长形式和短形式，「--选项名」为长形式，「-字母」为短形式。 --host、-h 主机名 --user、-u 用户名 --password、-p 密码 --port、-P 端⼝ 当使用 mysqld_safe 启动服务器时，对于传的启动选项 mysqld_safe 并不会处理，会传给 mysqld 来处理 # mysqld_server 不会处理，将会把 --skip-networking 传递给 mysqld 处理 mysqld_server --skip-networking 配置文件 在类 UNIX 操作系统中， MySQL 会按照下列路径来寻找配置⽂件： 路径名 备注 /etc/my.cnf /etc/mysql/my.cnf SYSCONFDIR/my.cnf $MYSQL_HOME/my.cnf 特定于服务器的选项（仅限服务器） defaults-extra-file 命令⾏指定的额外配置⽂件路径 ~/.my.cnf ⽤户特定选项 ~/.mylogin.cnf 用户特定的登录路径选项（仅限客户端） 配置文件下面，分别有 server、mysqld、mysqld_safe、client、mysql、mysqladmin 这几个组名。而不同的启动命令，能读取的组名下的配置是不一样的。 启动命令 类别 能读取的组 mysqld 启动服务器 [mysqld]、[server] mysqld_safe 启动服务器 [mysqld]、[server]、[mysqld_safe] mysql.server 启动服务器 [mysqld]、[server]、[mysql.server] mysql 启动客户端 [mysql]、[client] mysqladmin 启动客户端 [mysqladmin]、[client] mysqldump 启动客户端 [mysqldump]、[client] 配置文件的优先级 多个配置文件设置了相同的启动选项，则以最后一个配置文件中的为准 比如在 ~/.my.cnf 和 /etc/my.cnf 都设置了 default-storage-engine ，前者是 InnoDB，后者是 MyISAM，因为~/.my.cnf比/etc/my.cnf顺序靠后，所以如果两个配置文件中出现相同的启动选项，以~/.my.cnf中的为准，所以MySQL服务器程序启动之后，default-storage-engine的值就是MyISAM。 同一个配置文件，多个组设置了相同的启动选项，则以最后一个出现的组为准 比方说例子中default-storage-engine既出现在[mysqld]组也出现在[server]组，因为[mysqld]组在[server]组后边，就以[mysqld]组中的配置项为准。 如果同一个启动选项，同时出现在配置文件和命令行中，则以命令行为准 系统变量 大部分系统变量，可以在服务器运行的过程中进行动态修改而不用重启服务器。 通过 SET [GLOBAL 全局|SESSION 会话] 系统变量名 = 值; 设置系统变量 全局变量影响整个服务器操作，会话变量影响单个会话的操作。 # 设置全局系统变量，两种写法等价 set global default_storage_engine=InnoDB; set @@global.default_storage_engine=InnoDB; # 设置会话变量，三种写法等价 SET SESSION default_storage_engine = MyISAM; SET @@SESSION.default_storage_engine = MyISAM; SET default_storage_engine = MyISAM; 通过 SHOW [GLOBAL|SESSION] VARIABLES [LIKE 匹配的模式]; 来查询。 # 查询全局系统变量中有 default 的系统变量 show global variables like \"%default%\"; # 查询全局系统变量中客户端最大连接数 show global variables like \"max_connections\"; # 查询会话系统变量中系统默认引擎 show session variables like \"default_storage_engine\"; 注意： 并不是所有系统变量都具有 GLOBAL 和 SESSION 的作用范围。 有一些只具有 gloabl 作用范围，如 max_connections 。 有一些只具有 session 作用范围，如 insert_id，表示插入时 auto_increment 的值。 有一些两个作用范围都有的，如 default_storage_engine。 有一些只读的系统变量，不能设置 如 version，查看系统版本。 状态变量 状态变量表示服务器运行状况，这些是由服务器自己设置，所以不能修改。与系统变量类似，状态变量也分为全局和会话两种。 通过 SHOW [GLOBAL|SESSION] STATUS [LIKE 匹配的模式]; 查询。 # 查询当前线程相关状态 show session status like \"thread%\"; 结果： +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 0 | | Threads_connected | 1 | | Threads_created | 1 | | Threads_running | 1 | +-------------------+-------+ 4 rows in set (0.00 sec) Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/02.字符集与比较规则.html":{"url":"MySQL/02.字符集与比较规则.html","title":"02.字符集与比较规则","keywords":"","body":"02.字符集与比较规则 1. 字符集简介 字符集就是建立字符与计算机使用的二进制之间的映射关系。 例如，采用1个字节编码一个字符的形式（一个字节有 8 位二进制），字符和字节的映射关系如下： 'a' -> 00000001 (十六进制：0x01) 'b' -> 00000010 (十六进制：0x02) 'A' -> 00000011 (十六进制：0x03) 'B' -> 00000100 (十六进制：0x04) 2. 比较规则简介 比较规则，就是如何比较两个字符的大小指定的规则。比方说字符'a'的编码为0x01，字符'b'的编码为0x02，所以'a'小于'b'。 比较规则有多种，满足不同需求。 比较规则的作用通常体现比较字符串大小的表达式以及对某个字符串列进行排序中。 一些重要的字符集 ASCII 字符集 共有 128 个字符，包括空格、标点符号、数字、大小写字母和一些不可见字符。由于总共才 128 个，所以是一个字节。 ISO 8859-1 字符集 共有 256 个字符，在 ASCII 上扩充了128个西欧常用字符(包括德法两国的字母)。也是一个字节。 GB2312字符集 收录了汉字、拉丁字母、希腊字母、平假名片假名、俄语西里尔字母。兼容 ASCII 字符集。 如果该字符在 ASCII 字符集中，则采用 1 字节 否则采用 2 字节 GBK 字符集 在 GB2312 字符集上做了扩充。 UTF8 字符集 收录了地球上所有的字符，而且还在扩充。 Mysql 中的字符集和比较规则 MySQL中的 utf8 和 utf8mb4 utf8 = utf8mb3 : 阉割过的 utf8 字符集，只使用 1-3 个字节表示字符。 utf8mb4：正宗的 utf8 字符集，使用 1-4 个字节表示字符。 字符集的查看 通过 SHOW (CHARACTER SET|CHARSET) [LIKE 匹配模式]; 来查看。 mysql> SHOW CHARSET; +----------+---------------------------------+---------------------+--------+ | Charset | Description | Default collation | Maxlen | +----------+---------------------------------+---------------------+--------+ | big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 | ... | latin1 | cp1252 West European | latin1_swedish_ci | 1 | | latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 | ... | ascii | US ASCII | ascii_general_ci | 1 | ... | gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 | ... | gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 | | latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 | ... | utf8 | UTF-8 Unicode | utf8_general_ci | 3 | | ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 | ... | latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 | | utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 | | utf16 | UTF-16 Unicode | utf16_general_ci | 4 | | utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 | ... | utf32 | UTF-32 Unicode | utf32_general_ci | 4 | | binary | Binary pseudo charset | binary | 1 | ... | gb18030 | China National Standard GB18030 | gb18030_chinese_ci | 4 | +----------+---------------------------------+---------------------+--------+ 41 rows in set (0.01 sec) mysql 总共有 41 个字符集，其中 default collation 是默认比较规则，maxlen 表示一个字节的最大字节长度。 需要记住 字符集名称 Maxlen ascii 1 latin1 1 gb2312 2 gbk 2 utf8 3 utf8mb4 4 查看比较规则 通过 SHOW COLLATION [LIKE 匹配模式]; 来查看 mysql> SHOW COLLATION LIKE 'utf8\\_%'; +--------------------------+---------+-----+---------+----------+---------+ | Collation | Charset | Id | Default | Compiled | Sortlen | +--------------------------+---------+-----+---------+----------+---------+ | utf8_general_ci | utf8 | 33 | Yes | Yes | 1 | | utf8_bin | utf8 | 83 | | Yes | 1 | | utf8_unicode_ci | utf8 | 192 | | Yes | 8 | | utf8_icelandic_ci | utf8 | 193 | | Yes | 8 | | utf8_latvian_ci | utf8 | 194 | | Yes | 8 | | utf8_romanian_ci | utf8 | 195 | | Yes | 8 | | utf8_slovenian_ci | utf8 | 196 | | Yes | 8 | | utf8_polish_ci | utf8 | 197 | | Yes | 8 | | utf8_estonian_ci | utf8 | 198 | | Yes | 8 | | utf8_spanish_ci | utf8 | 199 | | Yes | 8 | | utf8_swedish_ci | utf8 | 200 | | Yes | 8 | | utf8_turkish_ci | utf8 | 201 | | Yes | 8 | | utf8_czech_ci | utf8 | 202 | | Yes | 8 | | utf8_danish_ci | utf8 | 203 | | Yes | 8 | | utf8_lithuanian_ci | utf8 | 204 | | Yes | 8 | | utf8_slovak_ci | utf8 | 205 | | Yes | 8 | | utf8_spanish2_ci | utf8 | 206 | | Yes | 8 | | utf8_roman_ci | utf8 | 207 | | Yes | 8 | | utf8_persian_ci | utf8 | 208 | | Yes | 8 | | utf8_esperanto_ci | utf8 | 209 | | Yes | 8 | | utf8_hungarian_ci | utf8 | 210 | | Yes | 8 | | utf8_sinhala_ci | utf8 | 211 | | Yes | 8 | | utf8_german2_ci | utf8 | 212 | | Yes | 8 | | utf8_croatian_ci | utf8 | 213 | | Yes | 8 | | utf8_unicode_520_ci | utf8 | 214 | | Yes | 8 | | utf8_vietnamese_ci | utf8 | 215 | | Yes | 8 | | utf8_general_mysql500_ci | utf8 | 223 | | Yes | 1 | +--------------------------+---------+-----+---------+----------+---------+ 27 rows in set (0.00 sec) 可以看到这是 utf8 的比较规则，有几个特点 都以 utf8_ 开头 后面紧跟的是比较规则主要作用于哪种语言，比如utf8_polish_ci表示以波兰语的规则比较，utf8_spanish_ci是以西班牙语的规则比较，utf8_general_ci是一种通用的比较规则。 后缀表示是否区分语言中的重音、大小写等： 后缀 英文释义 描述 _ai accent insensitive 不区分重音 _as accent sensitive 区分重音 _ci case insensitive 不区分大小写 _cs case sensitive 区分大小写 _bin binary 以二进制方式比较 比较规则的应用 比较规则共有 4 个级别： 服务器级别 character_set_server 服务器级别的字符集 collation_server 服务器级别的比较规则 这两个值在配置文件中修改。 数据库级别 character_set_database 当前数据库的字符集 collation_database 当前数据库的比较规则 CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 表级别 CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]] ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 行级别 REATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列... ); ALTER TABLE 表名 MODIFY 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称]; 只修改字符集或比较规则： 只修改字符集，比较规则变为修改后的字符集默认的比较规则。 只修改比较规则，字符集变为修改后的比较规则对应的字符集。 字符集与比较规则的关系： 如果创建时没有显示指定字符集和比较关系，则默认使用上一级的字符集与比较规则。行->表->库->服务器。 # 客户端和服务器中的字符集 如果客户端与服务器中的编码规则不一致，则会导致“乱码”。 客户端请求服务器，服务器处理并返回给客户端，这个过程中伴随多次字符集的转换。用到了三个系统变量 系统变量 描述 character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器运行过程中使用的字符集 character_set_results 服务器向客户端返回数据时使用的字符集 服务器认为客户端传的请求是以 character_set_client 来编码的，如果不一致，则无法请求。 服务器把得到的结果通过 character_set_results 来编码回传给客户端，如果不一致则乱码。 所以，为了保证一致性，这三个系统变量通常统一设置成一个字符集。 SET character_set_client = 字符集名; SET character_set_connection = 字符集名; SET character_set_results = 字符集名; Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/03.InnoDB 页的简介.html":{"url":"MySQL/03.InnoDB 页的简介.html","title":"03.InnoDB 页的简介","keywords":"","body":"03.InnoDB 页的简介 简介 InnoDB 是一个持久化的存储引擎。数据存储在磁盘上，而真正的处理数据是在内存中进行的（内存处理速度快）。 InnoDB 会将数据划分成若干个页，以页作为磁盘和内存之间交互的「基本单位」，一般大小为 16KB。所以不管是从磁盘读取还是从内存写入磁盘，最少都会取 16KB 的内容。 InnoDB 行格式 InnoDB 把记录在磁盘上的存取方式称之为「行格式」或者「记录格式」。目前有四种「行格式」：「Compact」、「Redundant」、「Dynamic」、「Compressed」。他们大致原理相同，只有少许差别。 指定行格式的语法 CREATE TABLE 表名 （列的信息） ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 先创建一个演示表 mysql> CREATE TABLE record_format_demo ( -> c1 VARCHAR(10), -> c2 VARCHAR(10) NOT NULL, -> c3 CHAR(10), -> c4 VARCHAR(10) -> ) CHARSET=ascii ROW_FORMAT=COMPACT; Query OK, 0 rows affected (0.03 sec) # 插入数据后 mysql> SELECT * FROM record_format_demo; +------+-----+------+------+ | c1 | c2 | c3 | c4 | +------+-----+------+------+ | aaaa | bbb | cc | d | | eeee | fff | NULL | NULL | +------+-----+------+------+ 2 rows in set (0.00 sec) compact 行格式 一条记录其实被分为「记录的额外信息」和「记录的真实信息」 额外信息分为「变长字段长度列表」、「NULL 值列表」、「记录头信息」 变长字段长度列表： 如 VARCHAR(M)、VARBINARY(M)、各种 TEXT 类型，各种 BLOB 类型 都是「变长字段」，存储了多少字节的数据是不固定的，所以 mysql 要记录真实数据占用的字节数，才不会让 mysql 服务器懵，所以「变长字段长度列表」分为两部分： 真正的数据内容 占用的字节数 在 Compact 行格式中，所有「变长字段」的长度排列在一起形成一个变长字段长度列表放在记录开头，并且个字段数据占用的字节数按照 逆序 存放。注意是「逆序」。 拿上面的第一条记录当例子来看。 列名 存储内容 内容长度（十进制表示） 内容长度（十六进制表示） c1 'aaaa' 4 0x04 c2 'bbb' 3 0x03 c4 'd' 1 0x01 所以实际样子是： 由于 c1、c2、c4 列的字符串都比较短，所以用一个字节表示就行了。但是如果字符较长，也有可能采用 2 个字节表示。 首先声明下： W：一个字节最多需要使用的字节数，也就是 show charset 语句中的 maxlen 列。如 utf8 是 3，gbk 是 2，ascii 是 1。 M：这个字节最多存储 M 个「字符」。如 VARCHAR(M),这个最多能表示的字节数是 M*W。 L：实际存储锁占用的字节数。 规则是这样的： 如果 M*W 如果 M*W > 255，则分为两种情况 如果 L 如果 L>127，则用 2 个字节表示真正字符串占用的字节数。 总结一下就是说：如果该可变字段允许存储的最大字节数（M×W）超过255字节并且真实存储的字节数（L）超过127字节，则使用2个字节，否则使用1个字节。 注意：变长字段长度列表对于值为 NULL 的列长度是不存储的。 NULL 列表 顾名思义，把某些列存 NULL 值的都放在 NULL 值列表中。可以让「记录的真实数据」中存储释放空间。 处理过程： 首先统计哪些列可以存储 NULL 。如例子中可以看到 C1、C3、C4 都可以存储 NULL 值。C2 不允许。 如果表中没有允许存储 NULL 的列，那么 NULL 值列表也就不存在了。否则，每个存储 NULL 的列对应一个二进制位逆序排列。 1 表示该值是 NULL。 0 表示该值不为 NULL。 Mysql 规定 NULL 值列表必须用整数个字节的位表示，如果不够则高位补 0。下图为当前例子的示意图，加入有 9 个允许为 NULL，则用 2 个字节来表示。 那么第一条记录就是： 而第二条记录是： 所以在填充了 NULL 值列表后，示意图是这样的： 记录头信息 名称 大小（单位：bit） 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录 next_record 16 表示下一条记录的相对位置 头信息先暂时这样，等很后面遇到后再详细展开。 记录的真实信息 除了定义的列之外，Mysql 还记录了一些隐藏列（由 Mysql 生成）： 列名 是否必须 占用空间 描述 row_id 否 6字节 行ID，唯一标识一条记录 transaction_id 是 6字节 事务ID roll_pointer 是 7字节 回滚指针 实际上这几个列的真正名称其实是：DB_ROW_ID、DB_TRX_ID、DB_ROLL_PTR，我们为了美观才写成了row_id、transaction_id和roll_pointer。 InnoDB 对主键的生成策略：优先使用用户自定义的主键作为主键，如果没有定义主键，则选取一个 Unique 键作为主键，如果没有 Unique 键，则 InnoDB 会为表默认添加一个名为 row_id 的隐藏列作为主键。 Redundant 行格式 字段长度偏移列表 该列会记录所有列（包括隐藏列）的长度，都会「逆序」存储到「字段长度偏移列表」。 有「偏移」这个字眼，那么计算列值长度就没有那么直观。 如第一条的「字段长度偏移列表」是 25 24 1A 17 13 0C 06 逆序存放之后就是 25 24 1A 17 13 0C 06 其意思是： 第一列(`row_id`)的长度就是 0x06个字节，也就是6个字节。 第二列(`transaction_id`)的长度就是 (0x0C - 0x06)个字节，也就是6个字节。 第三列(`roll_pointer`)的长度就是 (0x13 - 0x0C)个字节，也就是7个字节。 第四列(`c1`)的长度就是 (0x17 - 0x13)个字节，也就是4个字节。 第五列(`c2`)的长度就是 (0x1A - 0x17)个字节，也就是3个字节。 第六列(`c3`)的长度就是 (0x24 - 0x1A)个字节，也就是10个字节。 第七列(`c4`)的长度就是 (0x25 - 0x24)个字节，也就是1个字节。 Dynamic 和 Compressed 行格式 这两种行格式类似于 COMPACT 行格式。只是在处理行溢出数据时不同，它们不会在记录的真实数据处存储字符串的前768个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址。 Compressed 换格式还会采用压缩算法对页面进行压缩。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/04.InnoDB 数据页结构.html":{"url":"MySQL/04.InnoDB 数据页结构.html","title":"04.InnoDB 数据页结构","keywords":"","body":"04.InnoDB 数据页结构 「页」是 InnoDB 管理存储空间的基本单位，大小一般是 16KB。页有很多类型，我们先来看看数据页。 名称 中文名 占用空间大小 简单描述 File Header 文件头部 38字节 页的一些通用信息 Page Header 页面头部 56字节 数据页专有的一些信息 Infimum + Supremum 最小记录和最大记录 26字节 两个虚拟的行记录 User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页面目录 不确定 页中的某些记录的相对位置 File Trailer 文件尾部 8字节 校验页是否完整 记录在页中的存储 最开始「页」是没有 User Records 的，当插入一条新数据后，都会从 Free Space 部分申请一个记录大小的空间划分到 User Records 。当 Free Space 被申请完了，那么也就代表这个页使用完了。如果还有新的记录插入，则需要申请新的页。 记录头信息的秘密 先创建一个表 mysql> CREATE TABLE page_demo( -> c1 INT, -> c2 INT, -> c3 VARCHAR(10000), -> PRIMARY KEY (c1) -> ) CHARSET=ascii ROW_FORMAT=Compact; Query OK, 0 rows affected (0.03 sec) 名称 大小（单位：bit） 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 表示当前记录拥有的记录数 heap_no 13 表示当前记录在记录堆的位置信息 record_type 3 表示当前记录的类型，0表示普通记录，1表示B+树非叶节点记录，2表示最小记录，3表示最大记录 next_record 16 表示下一条记录的相对位置 我们简化一下图： 我们插入几条数据再看看： mysql> INSERT INTO page_demo VALUES(1, 100, 'aaaa'), (2, 200, 'bbbb'), (3, 300, 'cccc'), (4, 400, 'dddd'); Query OK, 4 rows affected (0.00 sec) Records: 4 Duplicates: 0 Warnings: 0 插入了 4 条记录，但是在 InnoDB 中还自动定义了两个记录分别为最小记录和最大纪录。 这两个记录不会放在「页」的 User Records 中，而是单独放在 Infimum + Supremum。 在 next_record 中，记录了从当前记录的真实数据到下一个记录的真实数据的地址偏移量。每一行数据串联成一个链表，可以通过一个记录早到它的下一条记录，而且这几个记录是按照主键值由小到大顺序排列。 mysql> DELETE FROM page_demo WHERE c1 = 2; Query OK, 1 row affected (0.02 sec) 当我们删掉第二条记录后，示意图为： 我们可以看到变化： 第二条记录没有从存储空间中移除，而是把 delete_mask 值设置为 1. 第二条记录的 next_record 变成了 0，意味着没有下一条记录。 第一条记录的 next_record 指向了第 3 条记录。 最大记录的 n_owned 值从 5 变成了 4。 所以，无论对页怎么做增删改操作，InnoDB 始终维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。 然后我们再添加上主键值为 2 的记录。 mysql> INSERT INTO page_demo VALUES(2, 200, 'bbbb'); Query OK, 1 row affected (0.00 sec) 可以看到，InnoDB 并没有为新记录的插入而申请新的存储空间，而是直接复用了原来的被删除记录的存储空间。 Page Directory 页记录 如何查找页中某条记录： 最笨的办法当然是从最小记录开始，沿着表一直查找，总会找到的。 InnoDB 的做法是： 将所有正常的记录（包括最大和最小记录，不包括被标记为删除的记录 ）划分为几个组。 每个组的最后一条记录（组中最大的那条记录）的头信息中 n_owned 属性标记该记录用友多少条记录，也就是该组共有多少条记录。 将每个组的最后一条记录的地址偏移量单独提取出来存储到靠近「页」的尾部的地方，也就是 Page Directory ，也就是 页目录。页面目录中的地址偏移量被称之为「槽」（slot），所以这个页面目录就是由「槽」组成。 加入现在 page_demo 表中，共有 6 条记录，InnoDB 会把它们分为两组，第一组只有一个最小记录，第二组是剩余的 5 条记录： 注意看最小和最大记录的头信息 n_owned 属性 最小记录的 n_owned 值为 1，表示以最小记录结尾的分组就一条记录，也就是他本身 最大记录的 n_owned 值为 5，表示已最大记录结尾的这个分组有 5 条记录，包括最大记录本身和插入的 4 条记录。 InnoDB 规定： 最小记录所在分组只能有 1 条记录。 最大纪录所在的分组拥有的记录数只能在 1~8 条之间。 剩下的分组中记录条数范围只能是 4-8 条。 当我们在插入 12 条记录后，那么一共就有了 18 条记录 现在，我们来查找一条记录，通过「二分法」来查找主键值为 6 的记录，最低槽位 low=0,最高的槽 high=4： 计算中间槽的位置，（0+4）/2=2,所以槽 2 记录的主键值为 8，又因为 8> 6 所以设置 high=2,low 不变。 重新计算中间槽，(0+2)/2=1，所以槽 1 的主键值为 4，又因为 4 因为 high - low 值为 1，所以确定主键值为 6 的记录在槽 2 中。 遍历槽 2 中的记录，直接查出来。 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽，并且找到该槽中值最小的那条记录。 通过记录的 next_record 属性遍历该槽所在组中的各个记录。 Page Header （页面头部） 名称 占用空间大小 描述 PAGE_N_DIR_SLOTS 2字节 在页目录中的槽数量 PAGE_HEAP_TOP 2字节 还未使用的空间最小地址，也就是说从该地址之后就是Free Space PAGE_N_HEAP 2字节 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录） PAGE_FREE 2字节 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2字节 已删除记录占用的字节数 PAGE_LAST_INSERT 2字节 最后插入记录的位置 PAGE_DIRECTION 2字节 记录插入的方向 PAGE_N_DIRECTION 2字节 一个方向连续插入的记录数量 PAGE_N_RECS 2字节 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录） PAGE_MAX_TRX_ID 8字节 修改当前页的最大事务ID，该值仅在二级索引中定义 PAGE_LEVEL 2字节 当前页在B+树中所处的层级 PAGE_INDEX_ID 8字节 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10字节 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10字节 B+树非叶子段的头部信息，仅在B+树的Root页定义 File Header (文件头部) 名称 占用空间大小 描述 FIL_PAGE_SPACE_OR_CHKSUM 4字节 页的校验和（checksum值） FIL_PAGE_OFFSET 4字节 页号 FIL_PAGE_PREV 4字节 上一个页的页号 FIL_PAGE_NEXT 4字节 下一个页的页号 FIL_PAGE_LSN 8字节 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number） FIL_PAGE_TYPE 2字节 该页的类型 FIL_PAGE_FILE_FLUSH_LSN 8字节 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4字节 页属于哪个表空间 说明： FIL_PAGE_SPACE_OR_CHKSUM 校验和，就是通过算法将很长的字节变成一个较短的字节来代表这个字节串，节省比较的时间和空间。 FIL_PAGE_OFFSET 页号，用来定位页。 FIL_PAGE_PREV 和 FIL_PAGE_NEXT，表示页的上一个和下一个页号，让页之间变成双向链表。 File Trailer 用来检测页是否完整，因为数据页从内存到磁盘时，可能会出现断电等情况。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/连接的原理.html":{"url":"MySQL/连接的原理.html","title":"连接的原理","keywords":"","body":"连接的原理 简介：现有 t1，t2 两张表，连接就是将两个表的数据取出来，再依次组合起来组成一个更大的表。 这个结果集就是「笛卡尔积」。 第一个需要查询的表称之为「驱动表」。连接的表称之为「被驱动表」。连接的原理就是从「驱动表」中获取的记录拿去找「被驱动表」的记录。 内连接：「驱动表」中的记录在「被驱动表」中未找到，则不会加入到结果集中。 外连接：「驱动表」中的记录在「被驱动表」中未找到，也会加入到结果集中。 左连接：以左边的表为「驱动表」 右连接：以右边的表为「驱动表」 where 子句中的过滤条件：凡是不符合 where 子句条件的，不管内外连接，都不会加入到结果集中。 on 子句过滤条件：在内连接中，与 where 子句一样。外连接中，如果没有匹配到，则各个字段用 null 填充。 原理： 「驱动表」访问一次，「被驱动表」访问多次，访问次数取决于「驱动表」查询出来的记录条数。这种连接执行方式称为「嵌套循环连接」 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 13:55:29 "},"MySQL/使用索引.html":{"url":"MySQL/使用索引.html","title":"使用索引","keywords":"","body":"使用索引 索引的代价 空间上：每建立一个索引就是一个 B+ 树，每个树的节点都是一个页，一个页有 16KB 大小，一颗很大的树有很多页，就会占据很大的存储空间。 时间上：在 B+ 树的增删改都可能会造成：记录移位，页分裂，页回收。 索引的适用条件 假设有这样一张表 CREATE TABLE person_info( id INT NOT NULL auto_increment, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name, birthday, phone_number) ); 全值匹配 SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27' AND phone_number = '15123983239'; 这三个列的顺序不影响查询效率，mysql 的查询优化器会帮助我们优化。 匹配最左边的列 mysql 会先搜索 name 列，当 name 相同时才会搜索 birthday，甚至 phone_number 列。 所以，下面两个 sql 可以使用索引，最后一个不行。 SELECT * FROM person_info WHERE name = 'Ashburn'; SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27'; # 不能使用索引 SELECT * FROM person_info WHERE birthday = '1990-09-27'; 匹配列前缀 mysql 创建二级索引，如果这个列是字符串类型的，会有这样的特点：先按照第一个字符排序，当第一个字符相同时，再按照第二个字符排序，以此类推。 # 可以使用索引 SELECT * FROM person_info WHERE name LIKE 'As%'; # 不能使用索引 SELECT * FROM person_info WHERE name LIKE '%As%'; 匹配范围值 所有记录都是按照索引列由小到大排列好的。 SELECT * FROM person_info WHERE name > 'Asa' AND name 过程： 先查找 name 值为 Asa 的列。 再查抄 name 值为 Barlow 的列。 由于所有记录都是由链表连接起来的，所以很容易取出之间的数据。 得到这些记录的主键值，再「回表」。 但是，如果对多个列进行范围查找，只有索引最左边的那个列才会使用到 b+ 的索引： SELECT * FROM person_info WHERE name > 'Asa' AND name '1980-01-01'; 过程： 先匹配 name 值的范围，name 值的结果可能不同 由于 name 值可能不同，所以通过 birthday 进行筛选是不是用索引的。name 值只有相同的情况下才能对 birthday 进行排序。 精确匹配某一列并范围匹配另外一列 SELECT * FROM person_info WHERE name = 'Ashburn' AND birthday > '1980-01-01' AND birthday '15100000000'; 过程： 先匹配 name 列。 由于 name 值相同，所以可以使用索引再范围匹配 birthday 列。 由于 birthday 不同了，所以 phone_number 列不能使用索引。 用于排序 SELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10; mysql 在创建索引时，先按照 name 从小到大排序，再依次按照 birthday、phone_number 排序。 当排序顺序一致时： # 直接从左向右读 10 条记录 ORDER BY name, birthday LIMIT 10 # 直接从右向左读 10 条记录 ORDER BY name DESC, birthday DESC LIMIT 10 但是，如果顺序不一致，则不会使用到索引，如 SELECT * FROM person_info ORDER BY name, birthday DESC LIMIT 10; 其他情况 以下三种情况都不会使用索引： # WHERE子句中出现非排序使用到的索引列 SELECT * FROM person_info WHERE country = 'China' ORDER BY name LIMIT 10; # 排序列包含非同一个索引的列 SELECT * FROM person_info ORDER BY name, country LIMIT 10; # 排序列使用了复杂的表达式 SELECT * FROM person_info ORDER BY UPPER(name) LIMIT 10; 用于分组 SELECT name, birthday, phone_number, COUNT(*) FROM person_info GROUP BY name, birthday, phone_number 过程： 先按照 name 分组，所有 name 相同记录划分为一组。 再将 name 相同的中 birthday 相同的放到另一组。 再在 birthday 中 phone_number 相同的放到另一组。 所以整个过程看起来就是大组分小组，小组分小小组。 回表的代价 SELECT * FROM person_info WHERE name > 'Asa' AND name 以这个 sql 为例子说明，由于索引关系可以通过 name 列进行查找，由于 Asc ~ Barlow 之间的记录是相连的，可以很快从磁盘中将其取出，这种读取称之为：顺序 I/O 而我们查出了 Asc ~ Barlow 对应的主键时，需要进行「回表」操作，而由于查询出来的主键是非连续的，所以这种读取方式被称之为：随机 I/0 即： 访问二级索引是 「顺序 I/0」 访问聚簇索引是 「随机 I/0」 覆盖索引 为了彻底告别 「回表」操作，建议查询列表最好只包含索引列： SELECT name, birthday, phone_number FROM person_info WHERE name > 'Asa' AND name 因为只查询 name, birthday, phone_number 值，而二级索引的叶子页记录正好包含这三列值。所以不需要回表了。 如何挑选索引 只为用于搜索、排序或分组的列创建索引 考虑列的基数，如 2, 5, 8, 2, 5, 8, 2, 5, 8 虽然有 9 条记录，但基数是 3。基数越大，值越分散，效率才高。 索引列的类型尽量小（范围）。比如我们能使用INT就不要使用BIGINT，能使用MEDIUMINT就不要使用INT～。因为数据类型越小，CPU 查询越快，索引占用的空间越小，一条记录能塞更多记录，从来减少 I/0 带来的性能损耗。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/事务的隔离级别.html":{"url":"MySQL/事务的隔离级别.html","title":"事务的隔离级别","keywords":"","body":"事务的隔离级别 事务并发可能发生的问题 脏写（Dirty Write） 如果一个事务修改了另一个未提交事务修改过的数据，就意味着发生了「脏写」。 session A B 各开了一个事务，session a 修改了 name 为张飞，而 session b 修改成了 关羽，然后 session a 提交事务，本来应该成功，但是 session b 执行了回滚，导致数据还原成最开始的样子，对于 session a 就跟没修改过一样。 脏读（Dirty Write） 如果一个事务读取到另一个未提交事务修改过的数据，就意味着发生了「脏读」。 同样 session a b 都开启了事务，session b 将 name 修改为了 关羽但是未提交，然后 session a 就会读取到关羽这个脏数据。然后 session b 回滚了事务，导致实际上 name 未曾被修改。 不可重复读（Non-Repeatable Read） 如果一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每次对该数据进行修改，该事务都能读取到最新的值，就意味着发生了「不可重复读」。 同样 session a b,session a 每次都能读取到 session b 修改过数据的的最新值。就意味着发生了「不可重复读」 幻读（Phantom） 如果一个事务根据条件查询出一些数据,而另外一个事务又向该表插入了符合刚才查询条件的数据,导致原来的事务再次根据相同条件查询数据是,会查出刚刚另一个事务插入的数据,则意味着发生了幻读。 session a 根据条件查询出一条刘备的记录，然后 session b 往这个表中插入了曹操，当 session a 再次根据相同条件查询时，会出现 刘备和曹操 两条记录。 如果 session b 是删除某一条信息，session a 查出来会少一条信息，这个不称之为幻读。 幻读强调一个事务按照某个条件多次查询，后读取到了之前没有得记录。 问题严重排序 脏写 > 脏读 > 不可重复度 > 幻读 四种隔离级别 Read Uncommited：事务之间可以读取到彼此未提交的数据。（该级别的锁会在写操作后立即释放，而不像其他隔离级别在事务提交后释放） Read Commited：该级别将锁的释放时机放在了事务提交之后。即一个事务需要等另一个事务提交后才能读取数据。 Repeatable Read：可以重复读，事务开始时，就不允许其他事务再修改数据。 Serializble：序列化，事务必须串行执行，可以避免脏读、幻读、不可重复读。但是效率低下。 四种隔离级别都不允许脏写。 性能：RU > RC > RR > SE 切换语法： SET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [level]; 查询当前隔离级别： show variable like \"%transaction_isolation%\"; MVCC 原理 版本链 版本链：对于 InnoDB 的聚簇索引来说，有两个必要的隐藏列。 trx_id：每次一个事务对某条聚簇索引记录进行改动时（update、delete、insert）都会把该事务的 「事务 id」赋值给 「trx_id」 隐藏列。 roll_pointer：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入 undo 日志中，然后这个列作为指向修改前版本记录的指针。 如现在有这样一张表和记录，并假设它的「事务 id」是 80： mysql> SELECT * FROM hero; +--------+--------+---------+ | number | name | country | +--------+--------+---------+ | 1 | 刘备 | 蜀 | +--------+--------+---------+ 1 row in set (0.07 sec) 假设之后两个「事务id」分别为 100、200 的事务对这条记录进行UPDATE操作，操作流程如下： 每次对记录进行改动，都会产生一条 undo 日志，每条 undo 日志都有一个 roll_pointer （最初的 insert 没有这个属性，因为没有更早的记录了）属性，将这些 undo 日志连接起来，形成一个链表。这个链表称之为「版本链」，在这个版本链的头部，记录着当前记录最新的值。 ReadView 对于 RU 隔离级别来说，可以读取其他事务未提交的记录，所以直接读取最新的记录就行了。 对于 SERIALIZABLE 隔离级别来说，通过加锁形式访问。 那么对于 RC 和 RR 级别来说 通过 ReadView 来判断版本链中哪些记录对于当前事务来说可见，有 4 个重要内容： m_ids ：表示在生成 ReadView 时当前系统中活跃的读写事务的「事务 id」列表。 min_trx_id：表示在生成 ReadView 时当前系统中活跃的读写事务中最小的「事务 id」，即 m_ids 的最小值。 max_trx_id：表示生成 ReadView 时应该分配给下一个事务的 id 值（这个 max_trx_id 并不是 m_ids 的最大值，事务id是递增分配的。比方说现在有 id 为 1，2，3 这三个事务，之后 id 为 3 的事务提交了。那么一个新的读事务在生成 ReadView 时，m_ids 就包括 1 和 2，min_trx_id的值就是 1，max_trx_id的值就是 4。）。 creator_trx_id：表示生成 ReadView 的事务的 「事务 id」（只有在执行 insert update delete 才会为事务分配事务 id，否则一个只读事务的「事务 id」默认为 0）。 有了 ReadView 那么，根据下面的规则来判断某个版本是否对当前事务可见： 如果被访问的 trx_id = ReadView 中的 creator_trx_id，那么表示，当前事务在访问自己修改过的记录，可以访问 如果被访问的 trx_id 如果被访问的 trx_id > ReadView 中的 creator_trx_id，那么表示，生成该版本事务在当前事务生成 ReadView 后才开启，不可以访问。 如果被访问的版本的 trx_id 属性值在 ReadView 的 min_trx_id 和 max_trx_id 之间，就需要判断 trx_id 在不在 m_ids 中，如果在，则表示在 ReadView 创建时该版本的事务还是活跃的，该版本不可以被访问。否则，说明创建 ReadView 时该版本的事务已经被提交了，所以可以访问。 举例说明 ReadView 需要区分两种 RC 和 RR 两种隔离级别。它俩的主要区别是生成 ReadView 的时机不同。 首先，先初始化一个数据： mysql> SELECT * FROM hero; +--------+--------+---------+ | number | name | country | +--------+--------+---------+ | 1 | 刘备 | 蜀 | +--------+--------+---------+ 1 row in set (0.07 sec) RC 隔离级别 - 每次读取数据前就会生成一个 ReadView 即：只要在事务中 select 就会生成一个 ReadView。 比方说现在有两个事务 100、200。 # Transaction 100 BEGIN; UPDATE hero SET name = '关羽' WHERE number = 1; UPDATE hero SET name = '张飞' WHERE number = 1; # Transaction 200 BEGIN; # 更新了一些别的表的记录（只有做修改操作才会单独分配一个「事务 id」） ... 此时的版本链是： 好了，此时如果有一个 RC 的事务开始执行： # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200未提交 SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备' 这个 SELECT1 过程： 在执行 SELECT 时，会先生成一个 ReadView，其 m_ids 列表为「100，200」,min_trx_id=100,max_trx_id=201，creator_trx_id=0 然后在版本链中挑选出可见记录，最新的是张飞，版本号为 100，在 m_ids 中，不符合可见要求，然后 roll_pointer 会跳到下一个版本。 第二个版本是关于，版本号也是 100，跟张飞一样，所以也不符合。 下个版本是刘备，版本号是 80，小于 min_trx_id 的 100，所以符合要求。所以最终返回给用户的 name 是刘备。 然后我们提交 select1。 # Transaction 100 BEGIN; UPDATE hero SET name = '关羽' WHERE number = 1; UPDATE hero SET name = '张飞' WHERE number = 1; COMMIT; 然后再到 事务id 为 200 的事务中更新一下表 hero 中 number 为 1 的记录： # Transaction 200 BEGIN; # 更新了一些别的表的记录 ... UPDATE hero SET name = '赵云' WHERE number = 1; UPDATE hero SET name = '诸葛亮' WHERE number = 1; 现在版本链就变成这样了： 我们在回到刚刚 RC 级别事务中继续查找 # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200均未提交 SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备' # SELECT2：Transaction 100提交，Transaction 200未提交 SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'张飞' 这个 select2 的执行过程： 由于是 RC 级别，每次执行 SELECT 时都会重新生成一个 ReadView，该 ReadView 的 m_ids=「200」（事务id 100 已经提交了），min_trx_id 为 200，max_trx_id 为 201，creator_trx_id 为 0。 从版本链第一条开始查询，最新是诸葛亮，trx_id=200，在 m_ids 中，所以不符合可见性要求，移动到下一个。 下一个版本是 赵云，同上，继续移动到下一个。 下一个是 张飞，trx_id=100，小于 min_trx_id ，所以符合要求，返回给用户。 总结：在 RC 隔离级别的事务，每次查询开始时都会生成一个独立的 ReadView。 RR 隔离级别 - 在第一次读取数据时生成一个 ReadView 即：在同一个事务里，只会在第一次使用 select 时 生成 ReadView，后面的都不会在生成了。 比方说现在有两个事务 100、200。 # Transaction 100 BEGIN; UPDATE hero SET name = '关羽' WHERE number = 1; UPDATE hero SET name = '张飞' WHERE number = 1; # Transaction 200 BEGIN; # 更新了一些别的表的记录（只有做修改操作才会单独分配一个「事务 id」） ... 此时的版本链是： 好了，此时如果有一个 RC 的事务开始执行： # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200未提交 SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备' 这个 SELECT1 过程： 在执行 SELECT 时，会先生成一个 ReadView，其 m_ids 列表为「100，200」,min_trx_id=100,max_trx_id=201，creator_trx_id=0 然后在版本链中挑选出可见记录，最新的是张飞，版本号为 100，在 m_ids 中，不符合可见要求，然后 roll_pointer 会跳到下一个版本。 第二个版本是关于，版本号也是 100，跟张飞一样，所以也不符合。 下个版本是刘备，版本号是 80，小于 min_trx_id 的 100，所以符合要求。所以最终返回给用户的 name 是刘备。 然后我们提交 select1。 # Transaction 100 BEGIN; UPDATE hero SET name = '关羽' WHERE number = 1; UPDATE hero SET name = '张飞' WHERE number = 1; COMMIT; 然后再到 事务id 为 200 的事务中更新一下表 hero 中 number 为 1 的记录： # Transaction 200 BEGIN; # 更新了一些别的表的记录 ... UPDATE hero SET name = '赵云' WHERE number = 1; UPDATE hero SET name = '诸葛亮' WHERE number = 1; 现在版本链就变成这样了： 我们在回到刚刚 RC 级别事务中继续查找 # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200均未提交 SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备' # SELECT2：Transaction 100提交，Transaction 200未提交 SELECT * FROM hero WHERE number = 1; # 得到的列name的值为'张飞' 看到这里发现与 RC 隔离级别没什么区别。但是从 SELECT2 这个开始，就不一样了。 # SELECT2 的执行过程： 因为当前事务级别是 RR，而之前 SELECT1 已经生成过一次 ReadView 了，所以直接拿来复用。之前的 ReadView 的 m_ids 列表的内容就是 「100, 200」 ，min_trx_id 为 100，max_trx_id 为 201，creator_trx_id 为 0。 从版本链开始查找第一条数据是 诸葛亮，trx_id=200，在 m_ids 中，不符合可见性要求。 下一个是，赵云，trx_id=200,同上。 下一个是，张飞，trx_id=100，也在 m_ids 中，不符合可见性要求。 下一个是，关于，trx_id=100，同上。 下一个是，刘备，trx_id=80，小于 min_trx_id。所以符合可见性要求，所以返回给用户。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/索引描述.html":{"url":"MySQL/索引描述.html","title":"索引描述","keywords":"","body":"索引描述 索引是存储的表中一种特殊的数据结构。 聚簇索引 页内记录是根据主键大小顺序排列的单向链表。 各个存放用户记录的页是也是根据页内的用户数据的主键大小顺序存放的双向链表。 存放目录项的页可以分不同层次，每层次的目录项的页也是根据页中目录项记录的主键大小顺序存放的双向链表。 B+ 树的叶子节点存放的用户的完整数据。 二级索引 假如以 C2 列创建一个二级索引。 页内记录是按 C2 列大小顺序排列的单向链表。 各个存放用户记录的页也是根据页内记录的 C2 列大小顺序排列的双向链表。 存放目录项的页可以分不同层次，每层次的目录项的页也是根据存放目录项记录 C2 列大小顺序存放的双向链表。 叶子页存储的不是用户完整记录而是，C2 列和主键。 目录项记录的也不是 主键+页号，而是 C2 列+页号。 如何根据二级索引查找一个记录 通过二分法找到记录存放在哪个页。 然后再在页中根据二分法查找出记录以及对应的主键。 最后通过主键进行「回表」，在聚簇索引中通过主键查找出完整的记录信息。 内节点中记录保持唯一性 c1 c2 c3 1 1 'u' 3 1 'd' 5 1 'y' 7 1 'a' 如果我们想新插入一行记录，其中c1、c2、c3的值分别是：9、1、'c'，这个时候会导致 MYSQL 无法判断如何插入。所以需要保证在同一层的目录项记录除了页号之外是唯一的。所以增加一个主键来使其唯一。 所以现在目录项记录页共有三个值：索引列、主键、页号。 联合索引 假如以 C2 和 C3 列创建一个联合索引，具体做法和二级索引一样。不同的是： 页内记录会先按照 C2 列书序排列 在记录的 C2 列相同的情况下，再按照 C3 列进行排序。 每个目录项内部存储的是 C2 和 C3 和页号。 叶子页存储的是 C2 和 C3 和 主键。 总结 每个索引都对应一个 B+ 树，最下面的是叶子页，其余的是内节点。叶子页存放着「用户记录」，内节点存放着「目录项记录」。 InnoDB 会自动创建聚簇索引，叶子页存放着用户完整记录。 我们可以自己创建二级索引，叶子页存放的是索引列+主键，如果用二级索引查找用户完整数据，需要先通过二级索引查找数据的主键，再进行「回表」操作查询。 B+ 数的每层节点都是通过主键从小到大顺序排序组成的一个双向链表。每个页内都是通过主键从小到大排序形成的一个单向链表。如果是联合索引，首先按照联合索引前面的列排序，如果列值相同，再通过联合索引后面的列进行排序。 通过索引查找记录是由根节点开始，一层一层往下搜索，由于每个页面都是按照索引列的值建立了 Page Directory（页目录），所以操作非常快。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/锁.html":{"url":"MySQL/锁.html","title":"锁","keywords":"","body":"锁 如何避免「脏读」「不可重复读」「幻读」 使用 MVCC 方案 MVCC 中，会通过生成 ReadView ，通过 ReadView 找到符合版本的记录。查询语句只会读到已经提交过的事务的更改，没提交过的是看不到的。而写操作是肯定是针对最新版本的记录，采用 MVCC 时，读-写并不冲突。 RC 下，一个事务每次 SELECT 时都会生成一次 ReadView，ReadView 确保不会读到其他事务没提交的数据，也就避免了脏读。 RR 下，一个事务只会在第一次 SELECT 时生成 ReadView，之后 SELECT 都会复用这个 ReadView 可以避免不可重复度和幻读。 对读写进行加锁 读-写跟写-写一样，进行排队 脏读：脏读产生的原因是一个事务读取到另一个事务未提交的数据，而当对数据进行加锁时，另一个事务在当前事务未提交之前必须等待事务提交才能访问数据，所以避免了脏读。 不可重复读：不可重复读产生的原因是，当前事务读取了数据，另一个事务对数据修改提交之后，当前事务再次读取数据会不一致。当对这个数据进行加锁后，另一个事务无法修改数据，则避免了不可重复读。 幻读：幻读是当前事务读取一个范围内的数据，另一个事务在满足当前事务查询条件下插入了一条记录，导致当前事务再次查询数据时，会得到多的这条记录。当当前事务对这个范围内的数据都加锁时，另一个事务是无法进行插入数据的，必须等当前事务提交后，才能执行。所以避免了幻读。 一致性读 事务利用 MVCC 进行的读取操作称之为 一致性读，或者一致性无所读。 锁定读 共享锁（Shared Locks）：S 锁，即事务读取一条记录前，先要获取该记录的 S 锁。 排它锁（Exclusive Locks）：X 锁，即事务修改一条数据前，先要获取该记录的 X 锁。 兼容性如下图： X 锁与任何锁都不兼容，S-S 兼容的。 锁定读的语句： # 加 S 锁 SELECT ... LOCK IN SHARE MODE; # 加 X 锁 SELECT ... FOR UPDATE; 写操作无非三种： DELETE：对一条记录 DELETE，需要先在 B+ 树中，找到这条记录，然后获取 X 锁，然后执行删除操作。 INSERT：一般情况下，插入操作不加锁。 UPDATE：其过程是先 DELETE，再 INSERT。 多粒度锁： 意向共享锁，英文名：Intention Shared Lock，简称 IS 锁，即当事务想对某条记录加 S 锁时，需要先在「表」上加 IS 锁。 意向独占锁，英文名：Intention Exclusive Lock，简称 IX 锁。即当事务相对某个记录加 X 锁时，需要先在「表」上加 IX 锁。 兼容性： 意向锁之间是兼容的，IX - IX， S-S 是兼容的，其余的都不兼容。 行锁和表锁 表级别的 X 锁和 S 锁 对某个表进行 ALTER TABLE 和 DROP TABEL 这类 DDL 时，会加表锁。 表级别的 IS 和 IS 锁。 表级别的 AUTO-INC 锁，听名字就跟主键自增有关，AUTO_INCREMENT。每次执行插入操作时，都会在表级别加个 AUTO-INC 锁，确保主键是连续的。 行级别的锁 正常的 X 和 S 锁。 GAP LOCKS：间隙锁（开区间），可以防止幻读。比如这个，在 number 值为 8 之间加了间隙锁，意味着事务不允许其他事务在 number (3, 8) 进行插入操作。只有当前事务提交后，才可以 如果把间隙锁加在 20 上，那么后面所有区间（20，+∞）都不允许其他事务进行插入操作。 Next-Key Locks：可以说是 GAP LOCKS 的升级版（闭区间），可以为 (3,8] 之间进行加锁。 死锁 死锁：指多个事务相互竞争同一资源相互占用，并且请求对方占用资源，导致的恶心循环。 如何解决： 以固定顺序访问，避免交叉等待锁的情况。 大事务拆小。业务允许情况下。 在同一事务中，缩小锁定行数。减少锁定行数，就减小了死锁概率。 降低隔离级别。从 RR 降低到 RC，避免很多因为 GAP 锁造成的死锁。 为表添加合理的索引。 如果不走索引则会对每一行进行加锁。 参考 浅谈数据库并发控制 - 锁和 MVCC 『浅入浅出』MySQL 和 InnoDB 数据库两个神器【索引和锁】 - Java知识点大全 - SegmentFault 思否 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/主从复制.html":{"url":"MySQL/主从复制.html","title":"主从复制","keywords":"","body":"主从复制 什么是主从复制 建立一个与主数据库完全相同的环境，称为从数据库。 主从复制的作用 做数据热备，后备数据库，当主数据库挂掉后，从数据库可以接替主数据库。避免数据丢失。 架构扩展，提高 I/0 效率。 读写分离，使数据库支持更大的并发。 主从复制原理 数据库中有个 binlog 日志，记录了所有的 sql 语句。 把主数据库的 binlog 日志复制到从数据库中。 让其在从数据库中再执行一次这些 sql 语句即可同步。 需要三个线程来执行： binlog 输出线程：每当有从库连接主库时，主库都会创建一个线程然后发送 binlog 内容到从库。 从库会建立两个线程： 从库 I/O 线程，当 START SLAVE 在从库开始执行之后，从库会创建 I/0 线程，该线程会连接到主库，并请求主库发送 binlog 里面的更新记录到从库上。从库的 I/O 线程会读取主库的 binlog 输出线程发送的更新并拷贝这些更新到本地。 从库 SQL 线程，从库创建一个 SQL 线程，这个线程会读取从库 I/0 线程写到的 relay log 的更新并执行。 所以我们知道，每一个主从复制链接都有三个线程，拥有多个从库的主库为每一个连接到主库的从库创建一个 binlog 输出线程，每一个从库中都有一个从库 I/0 线程和从库 SQL 线程。 步骤： 主库的更新操作(update,insert,delete)被写入 binlog 中。 从库连接主库 主库创建 binlog 输出线程，连接从库，并将 binlog 更新发送给从库 从库创建 I/0 线程，读取从库传过来的 binlog 并写入到 relay log。 从库创建 SQL 线程，从 relay log 中读取内容，将更新内容写入从库。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"MySQL/redo 和 undo.html":{"url":"MySQL/redo 和 undo.html","title":"redo 和 undo","keywords":"","body":"redo 和 undo redo 日志 redo 日志的作用是把在事务过程中的所有修改操作全部记录下来，在之后系统崩溃重启后，把事务的所做的修改全部恢复。 组成： 内存中的重做缓冲（redo log buffer），易丢失，在内存中。 重做日志（redo log file），是持久的，保存在磁盘中。 步骤 ： 将原始数据从磁盘中取出 修改数据的内存拷贝 生成一条重做日志，并写入 redo log buffer ，记录的数据是被修改的值。 当事务 commit 时，将 redo log buffer 中的内容刷新到 redo log file，采用追加写的方式。 定期将内存中修改的数据刷新到磁盘中。 在 InnoDB 中，redo log 都是以 512 个字节的块的形式进行存储的，同时块的大小和磁盘扇区的大小一致，所以保证了重做日志的原子性，不会因为机器断电导致重做日志只写了一半。 mini-transaction 上图为重做日志的过程，每个 mini—transaction （mtr）对应一条 DML 语句。对数据修改后，产生一条 redo1，首先将其写入 mtr 私有 buffer 中，当 DML 语句结束后，将 redo1 的私有 buffer 拷贝到共有的 log buffer 中。当外部整个事务提交时，再将 redo log buffer 刷入 redo log file 中。 undo 日志 为了保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚。在 mysql 中，回滚操作是由 undo 日志实现的，所有事务的修改都会记录在 undo log 中，在发生错误时进行回滚。 undo 日志只能「逻辑地」将数据恢复成之前的样子，其实它做的是与修改相反的工作，如一个 insert 修改，那么 undo 日志就会生成一个 delete ，一个 update 生成一条相反的 update。 作用： 事务回滚 MVCC（多版本并发控制） 写入时机： DML 操作修改聚簇索引之前，记录 undo 日志。 二级索引记录的修改，不记录 undo 日志。 x 为了保证原子性，必须将数据在事务钱写到磁盘中，只要事务成功，数据必然持久化。 undo log 必须先于数据持久化到磁盘中，可以用来回滚事务。 先回滚 redo 再回滚 undo。 redo 日志从前往后恢复。 undo 日志从后往前恢复。 参考 浅析MySQL事务中的redo与undo - 简书 『浅入深出』MySQL 中事务的实现 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 00:58:03 "},"PHP-基础知识/01.类型.html":{"url":"PHP-基础知识/01.类型.html","title":"01.类型","keywords":"","body":"01.类型 php 开启 '' 短标签：在 php.ini 中找到 short_open_tag 改为 true。（不建议使用） 如果文本为纯 PHP 代码，则最好删除 PHP 结束标记，这样可以防止意外加入的空格和换行符，会导致 PHP 输出本无意输出的空格。 php 的 ?> 结尾自带一个分号，如 合法。 1、类型 PHP 支持 9 种原始数据类型 基本类型，标量 整形 integer 字符串 string 浮点型 float 布尔型 boolean复合类型 对象 object 数组 array 可调用 callback其他 资源 resource NULL 其类型判断可通过：is_integer , is_string() , is_float() , is_bool() , is_array() , is_object() , is_resource() , is_null() , is_numeric() , is_scalar() 判断是否是标量 1.1 Boolean 布尔类型 $bool = True; // 不区分大小写 其他类型转换 当其他类型转换为 Boolean 时，以下 7 个会被认为是 FALSE： 布尔 FALSE 本身 整型 0 浮点型 0.0 空字符串 '' 以及 '0' 不包含任何元素的空数组 [] 特殊类型 NULL 从空标记生成的 SimpleXML 对象。 1.2 Integer 整型 32 位系统中范围为 -2^31-1 ~ 2^31-1 64 位系统中范围为 -2*63-1 ~ 2^63-1 123 -123 0123 #八进制 0x123 #十六进制 当给定的数超过范围时，php 会自动将其变为 浮点型（Float） 。同样通过计算的超过范围也会返回 浮点型（Float） 。做除法运算时，除不尽的也会变成 浮点型（Float） 。 PHP 7.0.0 起，NaN 和 Infinity 在转换成 integer 时，不再是 undefined 或者依赖于平台，而是都会变成零。 其他类型转换 资源类型 Resource 将返回 php 运行时 resource 分配的唯一资源号 布尔型 Boolean false=0，true=1 浮点型 Float 通常情况下是向下取整，但是由于精度问题可能造成无法预估的情况 echo (int) ( (0.1+0.7) * 10 ); // 显示 7! 字符串 String 如果字符串中没有包含 ','，'E'或'e'，该字符串将被当成 integer 来取值。其它所有情况下都被作为 float 来取值。该字符串的开始部分决定了它的值。如果该字符串以合法的数值开始，则使用该数值。否则其值为 0（零）。合法数值由可选的正负号，后面跟着一个或多个数字（可能有小数点），再跟着可选的指数部分。指数部分由 'e' 或 'E' 后面跟着一个或多个数字构成。 1.3 Float 浮点型 浮点数精度有限，取决于系统。永远不要相信浮点数最后一位。也不要直接比较两个浮点数，如需要请使用精度函数 bc。 字符串转换为数值 1.4 String 字符串 字符串有 4 种表示方法 单引号，单引号中变量不被解析。如果要表达单引号本身需要加反斜线\\。 $a = 'abc'; $b = '$a';//$a $c = \"$a\"; //abc 双引号，双引号中的变量以及特殊字符会被解析。\\n 换行（ASCII 字符集中的 LF 或 0x0A (10)） \\r 回车（ASCII 字符集中的 CR 或 0x0D (13)） \\t 水平制表符（ASCII 字符集中的 HT 或 0x09 (9)） \\\\ 反斜线 \\$ 美元标记 \\\" 双引号 heredoc 结构是一种提供一个开始标记和一个结束标记。方便书写大段的字符串。结束标记必须顶格写。类似双引号。单引号不转义。变量解析 foo = 'Foo'; $this->bar = array('Bar1', 'Bar2', 'Bar3'); } } $foo = new foo(); $name = 'MyName'; echo foo. Now, I am printing some {$foo->bar[1]}. This should print a capital 'A': \\x41 EOT; // 输出 My name is \"MyName\". I am printing some Foo. Now, I am printing some Bar2. This should print a capital 'A': A Nowdoc 结构是类似于单引号字符串的。Nowdoc 结构很象 heredoc 结构，但是 nowdoc 中不进行解析操作 nowdoc前需要加上单引号。 string中的字符可以通过一个从 0 开始的下标，用类似 Array 结构中的方括号包含对应的数字来访问和修改，也可以通过花括号{}。 $a = 'start'; echo $a[0]; // 输出 s echo $a{0}; // 输出 s 字符串的连接使用 . 来完成。 $a = 'a'; $b = 'b'; echo $a.$b; // ab 复杂（花括号）语法，可以在字符串中使用复杂表达式。其方式为在 string 中将表达式通过 {}（花括号）括起来。变量$符号必须紧挨{（左花括号）。 $great = 'fantastic'; echo \"This is {$great}\"; // 有效 echo \"This is { $great}\"; // 无效 echo \"This is ${great}\"; // 有效 echo \"This square is {$square->width}00 centimeters broad.\"; // 有效 echo \"This works: {$arr['key']}\";// 有效，只有通过花括号语法才能正确解析带引号的键名 echo \"This works: {$arr[4][3]}\"; // 有效 echo \"This works: {$arr['foo'][3]}\"; // 有效，当在字符串中使用多重数组时，一定要用括号将它括起来 // 有效 echo \"This works: \" . $arr['foo'][3]; echo \"This works too: {$obj->values[3]->name}\"; echo \"This is the value of the var named $name: {${$name}}\"; echo \"This is the value of the var named by the return value of getName(): {${getName()}}\"; echo \"This is the value of the var named by the return value of \\$object->getName(): {${$object->getName()}}\"; 转换为字符串 boolean 的 true 会被转换为 string \"1\"，而 false 会被转换为 string ‘’。 整数和浮点数就直接被装换为字面的 string。 array 类型会被转换为 string “Array”。 object 类型会被转换为 string “Object”。 资源 resource 总会被转变成 \"Resource id #1\" 这种结构的字符串。 NULL 总是被转变成空字符串。 1.5 Array 数组 PHP 中的数组是一个有序映射。映射是一种把 values 映射到 keys 的类型。 $a = array(1,2,3); // 索引数组 $b = [ 'a'=>'abc', 'd'=>'def', ]; // 简写 php5.4+，关联数组 键名 包含有合法整型值的字符串会被转换为整型。例如键名 \"8\" 实际会被储存为 8。但是 \"08\" 则不会强制转换，因为其不是一个合法的十进制数值。 浮点数也会被转换为整型，意味着其小数部分会被舍去。例如键名 8.7 实际会被储存为 8。 布尔值也会被转换成整型。即键名 true 实际会被储存为 1 而键名 false 会被储存为 0。 Null 会被转换为空字符串，即键名 null 实际会被储存为 \"\"。 数组和对象不能被用为键名。坚持这么做会导致警告：Illegal offset type。 1.6 Object 对象 通过 new 来实例化对象 class a{} $obj = new a(); 1.7 Resource 资源 资源类型是保持外部的一个引用。如数据库的链接，文件的句柄等。 $fp = fopen(\"./a.log\");//resource 1.8 Null Null 表示一个变量没有值。Null 类型唯一的值就是 Null。 3 种情况下会被认为是 Null 变量没有赋值 变量赋值 Null 变量被 unset();$a = null // null $b;//var_dump($b); null $c = 1; unset($c);//var_dump($c); null 1.9 Callback 类型 一些函数如 call_user_func() 或 usort() 可以接受用户自定义的回调函数作为参数。回调函数不止可以是简单函数，还可以是对象的方法，包括静态类方法。 // An example callback function function my_callback_function() { echo 'hello world!'; } // An example callback method class MyClass { static function myCallbackMethod() { echo 'Hello World!'; } } // Type 1: Simple callback call_user_func('my_callback_function'); // Type 2: Static class method call call_user_func(array('MyClass', 'myCallbackMethod')); // Type 3: Object method call $obj = new MyClass(); call_user_func(array($obj, 'myCallbackMethod')); // Type 4: Static class method call (As of PHP 5.2.3) call_user_func('MyClass::myCallbackMethod'); Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:55 "},"PHP-基础知识/02.变量与常量.html":{"url":"PHP-基础知识/02.变量与常量.html","title":"02.变量与常量","keywords":"","body":"02.变量与常量 php 变量通常以美元 $ 符号开头，以字母、下划线开头，后面跟任意字母、数字、下滑线。 php 的变量大小写敏感，通常以小驼峰或单词以下划线隔开来命名。如 $productSku 或 $product_sku $this 是一个特殊的变量，不能定位成 this 变量。 $a; // 合法 $_a; // 合法 $2a; // 非法 1. 变量的引用赋值 通过给变量钱加 & 来赋值。只能引用有值的变量。 $a = 4; $b = &$a; $b = 5; echo $a; // 5 echo $b; // 5 2. 预定义变量 超全局变量：在全部作用域下始终可用的内置变量 超全局变量 解释 $GLOBALS 引用全局作用域中所有的全部变量 $_GET HTTP GET 变量 $_POST HTTP POST 变量 $_REQUREST HTTP Request 变量 $_SERVER 服务器和执行环境信息 $_FIELS 文件上传变量 $_SESSION Session 变量 $_COOKIE Cookie 变量 $_ENV 环境变量 预定义变量 |预定义变量|解释| |:--:|:--:| |$php_errormsg | 前一个错误信息| |$HTTP_RAW_POST_DATA | 原生POST数据| |$http_response_header | HTTP 响应头| |$argc | 传递给脚本的参数数目| |$argv | 传递给脚本的参数数组| 3. 可变变量 变量名称本身是个变量的变量叫可变变量1 超全局变量和 $this 不可用作可变变量。 $a = 'b'; $b = '4'; echo $$a; // 4 4. 变量的作用域 在文件最外层定义的变量是全局变量。 函数内部的变量是局部变量，局部变量只能在函数内访问。 $a = 123; function test(){ echo $a; } echo $a; // 123; test(); // 无输出 在函数内通过 static 修饰的变量，仅在局部函数内存在，当程序作用完之后离开时，静态变量不会消失，等全部程序执行完成后才会销毁。 function test(){ static $a = 0; // 正确 static $a = 1; // 会报错，static 不能重复通过 static 定义。 $a++; echo $a; } test(); // 1 test(); // 2 在函数内部通过 global 修饰函数外部全局变量，可以使全局变量能够作用于局部函数内部 5. 常量 常量是定义之后就不能更改的变量。通常由 define() 函数来定义。常量通常大写，其他命名规则与变量类似。 define('IS_DEBUG',0); 在常量中还有一些系统预定义好的常量。如 PHP_EOL 换行。 8 个魔术常量 常量 解释 LINE 文件中当前行号 FILE 返回包含当前文件名的绝对路径 DIR 返回当前文件所在文件夹的绝对路径（结尾不包含反斜杠/） FUNCTION 返回当前函数的名称 CLASS 返回当前类的名称 TRAIT 返回 trait 的名字 METHOD 返回类的方法名 NAMESPACE 返回命名空间 namespace App; class test{ public function a(){ echo __CLASS__.PHP_EOL; echo __METHOD__.PHP_EOL; echo __NAMESPACE__.PHP_EOL; echo __FUNCTION__.PHP_EOL; } } $obj = new test(); $obj->a(); // 输出 App\\test // App\\test::a // App // a Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:55 "},"PHP-基础知识/03.运算符.html":{"url":"PHP-基础知识/03.运算符.html","title":"03.运算符","keywords":"","body":"03.运算符 1.运算符优先级 结合方向 运算符 附加信息 无 clone new clone 和 new 左 [ array() 右 \\ 算术运算符 右 ++ -- ~ (int) (float) (string) (array) (object) (bool) @ 类型和递增／递减 无 instanceof 类型 右 ! 逻辑运算符 左 ** / %* 算术运算符 左 + - . 算术运算符和字符串运算符 左 >> 位运算符 无 > >= 比较运算符 无 == != === !== <> 比较运算符 左 & 位运算符和引用 左 ^ 位运算符 左 ｜ 位运算符 左 && 逻辑运算符 左 ｜｜ 逻辑运算符 左 ?? 比较运算符 左 ? : ternary right = += -= =* *= /= .= %= &= ｜= ^= >>=* 赋值运算符 左 and 逻辑运算符 左 xor 逻辑运算符 左 or 逻辑运算符 $a = 5, $b = 5 ?> 2. 算数运算符 $a = 5; $b = 2; echo $a+$b; // 加法 结果：7 echo $a-$b; // 减法 结果：3 echo $a*$b; // 乘法 结果：10 echo $a/$b; // 除法取商 结果：2 echo $a%$b; // 取模 结果：1 echo $a**$b; // 指数乘积 结果：25 // 取模例子 取模正负只看前一个值得正负 echo (5 % 3); // 结果：2 echo (-5 % 3); // 结果：-2 echo (5 % -3); // 结果：2 echo (-5 % -3); // 结果：-2 3.赋值运算符 $b = 4; $a = $b +=5; // $a = $b = ($b+5) a b 都等于 9 $c = 'hello'; $c .= 'world'; // hello world $d = 4; $e = &d; // 引用赋值 $e = 5; // d e 都等于 5 4.位运算符 $a \\&\\ $b And（按位与） 将把 $a 和 $b 中都为 1 的位设为 1。 $a \\｜$b Or（按位或） 将把 $a 和 $b 中任何一个为 1 的位设为 1。 $a ^ $b Xor（按位异或） 将把 $a 和 $b 中一个为 1 另一个为 0 的位设为 1。 ~ $a Not（按位取反） 将 $a 中为 0 的位设为 1，反之亦然。 $a Shift left（左移） 将 $a 中的位向左移动 $b 次（每一次移动都表示“乘以 2”）。 $a >> $b Shift right（右移） 将 $a 中的位向右移动 $b 次（每一次移动都表示“除以 2”）。 5.比较运算符 当比较有字符串时，会把字符串转换为数字来进行比较 $a == $b 等于 TRUE，如果类型转换后 $a 等于 $b。 $a === $b 全等 TRUE，如果 $a 等于 $b，并且它们的类型也相同。 $a != $b 不等 TRUE，如果类型转换后 $a 不等于 $b。 $a <> $b 不等 TRUE，如果类型转换后 $a 不等于 $b。 $a !== $b 不全等 TRUE，如果 $a 不等于 $b，或者它们的类型不同。 $a 小与 TRUE，如果 $a 严格小于 $b。 $a > $b 大于 TRUE，如果 $a 严格大于 $b。 $a 小于等于 TRUE，如果 $a 小于或者等于 $b。 $a >= $b 大于等于 TRUE，如果 $a 大于或者等于 $b。 $a $b 太空船运算符（组合比较符） 当$a小于、等于、大于$b时 分别返回一个小于、等于、大于0的integer 值。 PHP7开始提供. $a ?? $b ?? $c NULL 合并操作符 从左往右第一个存在且不为 NULL 的操作数。如果都没有定义且不为 NULL，则返回 NULL。PHP7开始提供。 echo 1 1; // 0 echo 2 1; // 1 echo 1 2; // -1 echo 'a' 'b'; // -1 echo 'a' 'a'; // 0 echo 'b' 'a'; // 1 echo \"a\" \"aa\"; // -1 echo \"zz\" \"aa\"; // 1 echo [] []; // 0 echo [1, 2, 3] [1, 2, 3]; // 0 echo [1, 2, 3] []; // 1 echo [1, 2, 3] [1, 2, 1]; // 1 echo [1, 2, 3] [1, 2, 4]; // -1 多种类型比较 运算数 1 运算数 2 结果 null 或 string string null 转换为'',再比较 bool 或 null 任何其他类型 null 转换为 false，再比较 object object 除非内部结构相同，否则无法比较。 == 属性和值一样则相等。 === 判断是否为同一对象 string，resource 或 number string，resource 或 number 都转换为数字比较 array array 较少成员的较小。如果运算数 1 中的键不存在于运算数 2 中则数组无法比较，否则挨个值比较 array 任何其它类型 array 总是更大 object 任何其它类型 object 总是更大 6.错误控制运算符 PHP 支持在表达式前加 @ 。表示忽略这个表达式的可能产生的任何错误 7.执行运算符 PHP 支持一个执行运算符：反引号(`)。PHP 将其作为 shell 命令进行执行，并返回结果。 反引号不能在双引号中执行。 反引号不能再激活了安全模式或者关闭了 shell_exec() 时执行。 ```php $a =ls -al`; echo $a; // 结果 -rw-r--r--@ 1 jupiter.k staff 241684 Mar 18 23:30 a.txt #### 8.递增/递减运算符 ```php $a = 5; $a++; // 先返回5，$a 再加 1 ++$a; // $a 先加 1，再返回 6 9.逻辑运算符 $a and $b And（逻辑与） TRUE，如果 $a 和 $b 都为 TRUE。 $a or $b Or（逻辑或） TRUE，如果 $a 或 $b 任一为 TRUE。 $a xor $b Xor（逻辑异或） TRUE，如果 $a 或 $b 任一为 TRUE，但不同时是。 ! $a Not（逻辑非） TRUE，如果 $a 不为 TRUE。 $a && $b And（逻辑与） TRUE，如果 $a 和 $b 都为 TRUE。 $a ｜｜ $b Or（逻辑或） TRUE，如果 $a 或 $b 任一为 TRUE。 10.字符串连接符 字符串连接符号 点(.) $a = 1; $b = 222 echo $a . $b; //1222 11.类型运算符 instanceof 用于确定一个 PHP 变量是否属于某一类 $c = new C(); $c instanceof C; Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:55 "},"PHP-基础知识/04.流程控制.html":{"url":"PHP-基础知识/04.流程控制.html","title":"04.流程控制","keywords":"","body":"04.流程控制 1. if、else if($a == 1) { echo '1'; }else if($a == 2) { echo '2'; }else{ echo 'other'; } #简写 不推荐 if ($a == 1): echo \"1\"; else: echo \"else\"; endif; 2.switch switch 类似于一系列的 if else。每个 case 下都有一个 break。如果没有则继续往下执行。 if($a == 1) { echo '1'; }else if($a == 2) { echo '2'; }else{ echo 'other'; } # swtich switch ($a) { case 1: echo '1'; break; case 2: echo '2'; break; default: echo 'other'; break; } // 其结果只能是其中一个。 switch ($a) { case 1: echo '1'; case 2: echo '2'; default: echo 'other'; break; } // 若 $a=1 , 则值为 12other 3.while、do while while 和 do while都是满足条件后才循环，不满足则跳出。 do while 相比 while 是先执行一次再判断。 $a = 10; while($a>1) { echo $a--;//1098765432 } #do while $b = 1; do{ echo $b; //1 }while($b>1) 4.for foreach for 循环是 PHP 中最复杂的循环结构。它的行为和 C 语言的相似 for (expr1; expr2; expr3){ statement } 第一个表达式（expr1）在循环开始前无条件求值（并执行）一次。 expr2 在每次循环开始前求值。如果值为 TRUE，则继续循环，执行嵌套的循环语句。如果值为 FALSE，则终止循环。 expr3 在每次循环之后被求值（并执行）。 $people = Array( Array('name' => 'Kalle', 'salt' => 856412), Array('name' => 'Pierre', 'salt' => 215863) ); for($i = 0; $i foreach 语法结构提供了遍历数组的简单方式。foreach 仅能够应用于数组和对象 引用修改后，一定要 unset 将 $value 销毁 foreach($array as $key=>$value) { } # 简写 foreach($array as $key=>$value): endforeach; #引用修改 数组最后一个元素的 $value 引用在 foreach 循环之后仍会保留。建议使用 unset() 来将其销毁。 foreach (array(1, 2, 3, 4) as &$value) { $value = $value * 2; } unset($value); 5.break 、continue break 结束当前 for，foreach，while，do-while 或者 switch 结构的执行。break后面可以跟一个数字。表示跳出几重循环 for($i = 0; $i continue 在循环结构用用来跳过本次循环中剩余的代码并在条件求值为真时开始执行下一次循环 continue 接受一个可选的数字参数来决定跳过几重循环到循环结尾。默认值是 1，即跳到当前循环末尾 for($i = 0; $i 6.declare declare 结构用来设定一段代码的执行指令。declare 的语法和其它流程控制结构相似 目前只认识两个指令：ticks encoding Tick（时钟周期）是一个在 declare 代码段中解释器每执行 N 条可计时的低级语句就会发生的事件。N 的值是在 declare 中的 directive 部分用ticks=N 来指定的 encoding 指令来对每段脚本指定其编码方式。 declare(ticks=1); // A function called on each tick event function tick_handler() { echo \"tick_handler() called\\n\"; } register_tick_function('tick_handler'); $a = 1; if ($a > 0) { $a += 2; print($a); } declare(encoding='ISO-8859-1'); 7.return 在函数中使用return 将结束函数的执行。 return 是语言结构而不是函数，因此其参数没有必要用括号将其括起来。通常都不用括号，实际上也应该不用，这样可以降低 PHP 的负担。 function f(){ return 1; echo '11';//不会执行 } #a.php #b.php // ba 8.include 、require include(path) 会按照给定的参数 进行查找，如果没有找到就到include_path中查找。如果还没有找到，那么就会抛出一个警告。 如果定义了路径——不管是绝对路径（在 Windows 下以盘符或者 \\ 开头，在 Unix/Linux 下以 / 开头）还是当前目录的相对路径。include_path就会被忽略。 require 和include查找文件基本上一致。只是require会抛出一个error错误终止代码的执行。 require 和include是一个语言构造器而不是一个函数 include 'a.php'; echo \"hello\"; # 会输出hello require 'a.php'; echo \"hello\"; # 抛出一个error hello不会输出。 9.include_once 、require_once include_once 、require_once 语句在脚本执行期间包含并运行指定文件.如果该文件中已经被包含过，则不会再次包含 include_once 会抛出warning 。require_once 会抛出error 10.goto goto操作符并不常用。用来跳转程序的一个位置。目标位置只能位于同一个文件和作用域 无法跳出一个函数或类方法，也无法跳入到另一个函数。也无法跳入到任何循环或者 switch 结构中 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:55 "},"PHP-基础知识/05.函数.html":{"url":"PHP-基础知识/05.函数.html","title":"05.函数","keywords":"","body":"05.函数 函数名命名规则与变量名命名规则一直，以字母或下划线打头，后面跟任意字母、数字、下划线。 要避免递归函数／方法调用超过 100-200 层，因为可能会使堆栈崩溃从而使当前脚本终止 PHP 中的所有函数和类都具有全局作用域。PHP 不支持函数重载，也不可能取消定义或者重定义已声明的函数。 1.自定义函数 function 函数名(参数 1,参数 2....参数 n){ // 函数体 } 2.函数参数 可以通过参数将函数外部信息传到内部 函数可以有默认值，顺序先非默认值参数，后有默认值参数 函数的参数可以指定类型，指定的类型可以是，类名、接口名、self、array、callback。php7.0 之后支持 bool、float、int、string。 function demo(ClassName $class_name,String $arg2 = 'apple'){ } 引用参数，默认情况下传入函数的参数值是不会改变的，当需要使传入值改变时，可以使用 & 引用。 4.严格类型 默认情况下，如果能做到的话，PHP将会强迫错误类型的值转为函数期望的标量类型。 在严格类型下，则不会。并且会报错，唯一的例外时将 integer 类型传给期望是 float 类型的函数。 declare(strict_types=1); function fn2(float $a){ echo $a; } $c = 1; fn2($c); // 输出 1 function fn(int $a){ echo $a; } $c = '1';//string fn($c);// 报错 5.可变参数的参数列表 PHP 在用户自定义函数中支持可变数量的参数列表。在 PHP 5.6 及以上的版本中，由 ... 语法实现 6.返回值类型 函数的返回值可以通过 return 返回。返回任意类型，返回后立即终止函数的运行，并且将控制权返还给调用函数行。 function fn(){ return \"hello\"; } 在 php7.0 之后，可以限制返回值的类型。 当覆盖一个父类方法时，子类方法的返回值类型声明必须与父类一致。如果父类方法没有定义返回类型，那么子类方法可以定义任意的返回值类型声明。 declare(strict_types=1); function($a):float { return 1.1; } 7.可变函数 php 支持可变函数概念，即在变量后加括号()，则会调用变量值同名的函数。不能用于系统函数如 echo，print，unset()，isset()，empty()，include，require 。 function foo(){ echo 1; } $a = foo; $a(); // 输出 1 8.匿名函数 匿名函数也叫闭包函数。 闭包可以从父作用域中继承变量。 任何此类变量都应该用 use 语言结构传递进去 (function (){ echo '匿名函数'; })(); #传递参数 $a = function ($arg) { echo $arg; } $arg = 'hello'; $a($arg);//hello; # 传递外部作用域变量 $arg = 'arg'; $f = function() use($arg){ echo $arg; } $f(); Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:55 "},"PHP-基础知识/06.类与对象1.html":{"url":"PHP-基础知识/06.类与对象1.html","title":"06.类与对象1","keywords":"","body":"06.类与对象1 类：类是对象的类型，将现实世界的有统一特征的个体抽象出来的结果。如人类、动物类、商品。 对象：对象是类的实例。有统一特征的抽象结果的实例，单独其中一个个体，如动物类的实例可以是猫，狗。 属性：是对象的一个特征，如猫的毛色，种类。 方法：是对象能够执行的动作。如猫可以走、跑、跳、睡觉。 1.类的定义 类名的定义规则与变量、函数的命名方式相同。以字母或下划线开头，跟任意字母、数字、下划线。最后一堆花括号。 class demo{ // 声明属性 public $var = 'a default value'; // 声明方法 public function displayVar() { echo $this->var; // 表示当前对象 } } $obj = new demo(); // 实例化对象 $obj->displayVar(); // 输出 a default value // ----------------------------------------------------------------------------------------- class A { function foo() { if (isset($this)) { echo '$this is defined ('; echo get_class($this); echo \")\\n\"; } else { echo \"\\$this is not defined.\\n\"; } } } class B { function bar() { A::foo(); } } $a = new A(); $a->foo(); // $this is defined (A) A::foo(); // $this is not defined. $b = new B(); $b->bar(); // $this is defined (B) B::bar(); // $this is not defined. 2.对象的实例化 通过 new 关键字来实例化一个对象，如果该类有命名空间则需要补全命名空间或者在文件开头用 use 引用。 use App; $obj = new className(); // 也可以这样做 $a = 'className'; $obj = new $a(); 对象赋值 $instance = new SimpleClass(); $assigned = $instance; $reference = &$instance; $instance->var = '$assigned will have this value'; $instance = null; // $instance and $reference become null var_dump($instance); // null var_dump($reference); // null var_dump($assigned); // object(SimpleClass)#1 (1) 3.类的继承 一个类在声明时使用 extends 关键字来继承另一个类的属性和方法。一个类只能继承一个类，不支持多继承。 displayVar(); // 输出 Extending class // a default value ?> 4.::class 自 PHP 5.5 起，关键词 class 也可用于类名的解析。使用 ClassName::class 可以获得类名，如果该类声明了命名空间，则会包括命名空间完整显示。 5.类的属性 类的变量叫类的属性。类的声明由 public 、protected 、 private 开头，然后跟上普通变量名。 在类的成员方法中，可以通过 ->(对象运算符) 来获取，如果是静态方法，则通过 self:: 来获取。 class demo{ public $a=1; public static $b=2; public function foo(){ echo $this->$a; // 1 echo self::$b; // 2 } } $obj = new demo(); $obj->foo(); 6.类常量 在类中始终保持不变的值定义为常量。在定义和使用常量的时候不需要使用 $ 符号 类常量是一个定值。类常量使用 const 定义。访问的时候使用 self:: 访问类常量 接口（interface）中也可以定义常量。更多示例见文档中的接口部分。 class demo{ const PI=3.14; } echo demo::PI; // 3.14 7.类的自动加载 类的自动加载可以不用手动 include 各个类文件。 自动加载不可用于 PHP CLI 模式 spl_autoload_register(function ($class_name) { require_once $class_name . '.php'; }); $obj = new demo1(); 8.构造函数和析构函数 构造函数 __construct()：当实例化一个对象时，立即执行的函数，非常适合用于初始化对象。 析构函数 __destruct()：析构函数在到某个对象的所有引用都被删除或者当对象被显式销毁时执行。 如果子类中定义了构造函数则不会隐式调用其父类的构造函数。要执行父类的构造函数，需要在子类的构造函数中调用parent::__construct() 试图在析构函数（在脚本终止时被调用）中抛出一个异常会导致致命错误。 name = \"MyDestructableClass\"; } function __destruct() { print \"Destroying \" . $this->name . \"\\n\"; } } $obj = new MyDestructableClass(); // 输出 In constructor // Destroying MyDestructableClass class P{ public function __construct(){ echo \"construct\"; } public function __destruct(){ echo \"destruct\"; } } $p = new P();// construct; unset($p);//destruct; ?> 9.访问控制（可见性） 对属性和方法的访问控制，可以通过，public（公有）,protected（受保护的），private（私有的）三种方式来实现。 public （默认）: 任何成员都可以访问。 protected :只能其自身或子类访问。 private : 只能自身访问。 class A{ public $name = 'a'; protected $age = 10; private $money = 100; } class B extends A{ public function test(){ echo $this->age;//a } public function testPrivate(){ echo $this->money; } } $b = new B(); echo $b->name;//a echo $b->test();//10 # 不可访问 echo $b->age;//error; #子类不能访问 echo $b->testPrivate();//error 10.范围解析操作符（::） 范围解析操作符（也可称作 Paamayim Nekudotayim）或者更简单地说是一对冒号，可以用于访问静态成员，类常量。还可以用于覆盖类中的属性和方法。 self，parent 和 static 这三个特殊的关键字是用于在类定义的内部对其属性或方法进行访问的 当一个子类覆盖其父类中的方法时，PHP 不会调用父类中已被覆盖的方法。是否调用父类的方法取决于子类。使用self调用父类，使用$this 调用本类。 class A{ public $name = 'a'; protected $age = 10; private $money = 100; } class B extends A{ public static $s = 's'; const PI = 111; public function test(){ echo parent::age;// 10 } public static function testStatic(){ echo self::$s; } public function testConst(){ echo self::PI; } public function testPrivate(){ echo $this->money; } } # self 和 $this class ParentClass { function test() { self::who(); // will output 'parent' $this->who(); // will output 'child' } function who() { echo 'parent'; } } class ChildClass extends ParentClass { function who() { echo 'child'; } } $obj = new ChildClass(); $obj->test();// 11.static 静态关键字 声明类属性或方法为静态，就可以不实例化类而直接访问。静态属性不能通过一个类已实例化的对象来访问（但静态方法可以） 静态属性不可以由对象通过 -> 操作符来访问。静态属性只能被初始化为文字或常量。静态属性不随着对象的销毁而销毁。 class P{ $a = \"world\"; public static function test(){ echo \"hello\".self::$a; } } p::test(); Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:55 "},"PHP-基础知识/07.类与对象2.html":{"url":"PHP-基础知识/07.类与对象2.html","title":"07.类与对象2","keywords":"","body":"07.类与对象2 1.抽象类 PHP 5 支持抽象类和抽象方法。定义为抽象的类不能被实例化。当一个类中有一个方法是抽象方法则，这个类就是抽象类 继承一个抽象类后，子类必须实现抽象类中所有抽象方法。 某个抽象方法被声明为受保护的，那么子类中实现的方法就应该声明为受保护的或者公有的，而不能定义为私有的。 此外方法的调用方式必须匹配，即类型和所需参数数量必须一致。 2.对象接口 使用接口（interface），可以指定某个类必须实现哪些方法，但不需要定义这些方法的具体内容。 使用接口（interface）定义的类，和定义一个标准类一样，不过所有的方法都必须是空的。（不可定义属性） 接口的所有方法都必须是公有的。 使用 implements 关键字来实现一个接口。可以一次性实现多个接口。实现多个接口时，方法不能重名。 接口可以继承，使用 extends 关键字。 类要实现接口，必须使用和接口中所定义的方法完全一致的方式。否则会导致致命错误。 foo1(); $obj->foo2(); $obj->foo3(); 3.trait 自 PHP 5.4.0 起，PHP 实现了一种代码复用的方法，称为 trait。 trait 可以让单继承的语言准备进行代码复用，避免传统多继承带来的问题。 sayHello(); echo 'World'; } } $obj = new world(); $obj->sayHelloWorld(); // Hello World 从基类继承的成员会被 trait 插入的成员所覆盖。优先顺序是来自当前类的成员覆盖了 trait 的方法，而 trait 则覆盖了被继承的方法。 即：当前类 > trait > 被继承的类 sayHello(); ?> 多个 trait sayHello(); $o->sayWorld(); $o->sayExclamationMark(); ?> 如果多个trait中。都有同名的方法，则会产生冲突，冲突会产生一个致命的错误。 可以使用 insteadof 来指明当产生冲突时，使用哪一个。 as 操作符可以 为某个方法引入别名 smallTalk(); // b $obj->bigTalk(); // A $obj->talk(); // B 使用 as 语法还可以用来调整方法的访问控制。 smallTalk(); // 报错 $obj->bigTalk(); $obj->talk(); 使用 trait 来组成 trait sayHello(); $obj->sayWorld(); trait 的抽象方法、静态成员、属性 hello; } } class MyHelloWorld{ use HelloWorld; } $obj = new MyHelloWorld(); $obj->sayHello(); $obj->sayWorld(); 4.匿名类 php 7 之后支持匿名类，匿名类很有用，可以创建一次性的简单对象。 $a = new class{ public function say(){ echo 'Hello'; } }; $a->say(); // Hello 匿名类被嵌套进普通 Class 后，不能访问这个外部类（Outer class）的 private（私有）、protected（受保护）方法或者属性。 为了访问外部类（Outer class）protected 属性或方法，匿名类可以 extend（扩展）此外部类。 为了使用外部类（Outer class）的 private 属性，必须通过构造器传进来： class Outer { private $prop = 1; protected $prop2 = 2; protected function func1() { return 3; } public function func2() { return new class($this->prop) extends Outer { private $prop3; public function __construct($prop) { $this->prop3 = $prop; } public function func3() { return $this->prop2 + $this->prop3 + $this->func1(); // 2+1+3 } }; } } echo (new Outer)->func2()->func3(); // 6 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 01:06:22 "},"PHP-基础知识/08.类与对象3.html":{"url":"PHP-基础知识/08.类与对象3.html","title":"08.类与对象3","keywords":"","body":"08.类与对象3 1.重载与重写 重写是指覆盖原方法，一般用于子类继承父类重写父类方法。 PHP 提供的“重载”是动态的“创建”类的属性和方法。通过使用魔术方法来实现。 当调用当前环境下未定义的属性和方法是，则会调用魔术方法。 在给不可访问属性赋值时，__set() 会被调用。 读取不可访问属性的值时，__get() 会被调用。 当对不可访问属性调用 isset() 或 empty() 时，__isset() 会被调用。 当对不可访问属性调用 unset() 时，__unset() 会被调用。 在对象中调用一个不可访问方法时，__call() 会被调用。 在静态上下文中调用一个不可访问方法时，__callStatic() 会被调用。 $name; } public function __set($name,$value){ $this->$name=$value; } public function __isset($name){ echo \"isset $name\".PHP_EOL; } public function __unset($name){ echo \"unset $name\".PHP_EOL; } public function __call($name,$args){ $args = implode(',', $args); echo \"call $name ,args is {$args}\".PHP_EOL; } public static function __callStatic($name,$args){ $args = implode(',', $args); echo \"call static $name ,args is {$args}\".PHP_EOL; } } $obj = new demo(); $obj->a = 'b'; echo $obj->a.PHP_EOL; // b isset($obj->a); // isset a unset($obj->a); // unset a $obj->say('Hello','World'); // call say ,args is Hello,World demo::staticSay('World','World'); // call static staticSay ,args is World,World 2.多态 统一操作，作用于不同对象，得到不同的结果。如数据库操作，$db->query(); 可以是 mysql 的操作，也可以是redis 的 query(); } run(new mysql()); // query mysql run(new redis()); // query redis 3.遍历对象 遍历对象可以使用foreach遍历可见属性。或者实现iterator接口 $v) { echo $key.\"=>\".$v.PHP_EOL; } // var1=>value 1 // var2=>value 2 // var3=>value 3 4.魔术方法 __construct 初始化调用 __desturct 对象销毁时调用 __call 访问一个不存在的方法的时候调用 __callStatic 访问一个不存在的静态方法调用 __get() 访问一个不存在的属性调用 __set() 修改一个不存在的属性调用 __isset() 使用 isset 判断一个高属性的时候调用 __toString() 当一个对象以一个字符串返回时候触发调用 __invoke() 当把一个对象当函数去调用的时候触发 __sleep() 当使用 serialize() 时调用 __wakeup() 当使用 unserialize() 时调用 server]; } public function __wakeup(){ return $this->server; } } $c = new MyClass(); $c(5); // int(5) var_dump($d = serialize($c)); // string(44) \"O:7:\"MyClass\":1:{s:6:\"server\";s:6:\"server\";}\" var_dump(unserialize($d)); //object(MyClass)#2 (1) { // [\"server\"]=> // string(6) \"server\" // } 5.Final 关键字 被 final 修饰的类不能被继承。 如果一个类中某个方法被 final ，这个类不能被重写。 final 不能修饰属性。 6.对象的复制和比较 对象的复制通过 clone 关键字来完成。这将会调用 clone() 方法。对象中的 clone() 方法不能被直接调用 当对象被复制后，PHP 5 会对对象的所有属性执行一个浅复制（shallow copy）。所有的引用属性 仍然会是一个指向原来的变量的引用。 当复制完成时，如果定义了 clone() 方法，则新创建的对象（复制生成的对象）中的 clone() 方法会被调用，可用于修改属性的值（如果有必要的话）。 a = 'b'; } public function say(){ echo $this->a.PHP_EOL; } } $a = new MyClass(); $b = clone $a; $a->say(); // a $b->say(); // b 当使用比较运算符（==）比较两个对象变量时，比较的原则是：如果两个对象的属性和属性值 都相等，而且两个对象是同一个类的实例，那么这两个对象变量相等。 而如果使用全等运算符（===），这两个对象变量一定要指向某个类的同一个实例（即同一个对象） 7.类型约束 函数的参数可以指定必须为对象（在函数原型里面指定类的名字），接口，数组 query(); } } $obj = new demo(); $obj->run(new mysql()); // query mysql $obj->run(new redis()); // query redis 8.静态绑定 self 调用的方法是所定义的类所定义的方法。 static 调用的方法是调用类的方法。 9.对象和引用 PHP 的引用是别名，就是两个不同的变量名字指向相同的内容。在 PHP 5，一个对象变量已经不再保存整个对象的值。只是保存一个标识符来访问真正的对象内容。 当对象作为参数传递，作为结果返回，或者赋值给另外一个变量，另外一个变量跟原来的不是引用的关系，只是他们都保存着同一个标识符的拷贝，这个标识符指向同一个对象的真正内容。 $b->foo = 2; echo $a->foo.\"\\n\"; Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 23:12:59 "},"PHP-基础知识/09.命名空间.html":{"url":"PHP-基础知识/09.命名空间.html","title":"09.命名空间","keywords":"","body":"09.命名空间 命名空间简单来说就是为了解决命名重复而造成的系统冲突。其思想来源于文件路径，\"同一路径下不能存在相同名称的文件\"。 如 www/a、www/b 可以同时存在，www/a/c、www/b/c 也可以同时存在。 1.定义命名空间 命名空间由关键字 namespace 定义。 其必须在程序脚本的第一条语句，否则会报致命错误。除 declare 之外。 子命名空间,中间由 \\ 隔开 2.同一文件定义多个命名空间 3.命名空间基础：命名空间与路径类似，分为非限定名称、限定名称、完全限定名称。 非限定名称，不加前缀的名称，如 $a = new foo(), foo::staticmethod()。如果当前命名空间是 currentnamespace，则 foo 就会被解析成 currentnamespace\\foo。 限定名称，包含前缀的名称，如 $a = new subnamespace\\foo(),或 subnamespace\\foo::staticmethod()。如果当前命名空间为 currentnamespace，则会被解析为 currentnamespace\\subnamespace\\foo。 完全限定名称，如 $a = new currentnamespace\\subnamespace\\foo(), currentnamespace\\subnamespace\\foo::staticmethod()。 // file1 // file2 // file3 4.namespace 与 NAMESPACE 常量 NAMESPACE 表示当前的命名空间，如果在全局中，则返回一个空字符串。 关键字 namespace 可用来显式访问当前命名空间或子命名空间中的元素。它等价于类中的 self 操作符。 5.别名与导入 使用 use 关键字来导入命名空间 使用 as 关键字来给命名空间取别名，可以给类、接口、命名空间使用别名。 6.全局命名空间 如果没有定义任何命名空间，则为全局空间，调用时在前面加斜杠 (\\) 7.名称规则解析 类名称总是会解析到当前命名空间中，如果不存在则需要使用完全限定命名空间。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:55 "},"PHP-基础知识/10.PHP-FPM 与 Nginx.html":{"url":"PHP-基础知识/10.PHP-FPM 与 Nginx.html","title":"10.PHP-FPM 与 Nginx","keywords":"","body":"10.PHP-FPM 与 Nginx 1.基础 静态数据 在整个网站架构中，web server 只是一个内容分发者。如客户端请求的是 index.html 时，web server 会在文件系统中找到这个 index.html 文件，发送给浏览器，这里分发的事静态数据。 动态数据 如果请求的是 index.php 文件，那么 web server 会将客户端的请求数据，通过 CGI 协议转发给 PHP 解析器来处理。 当 web server 收到 index.php 请求后，会启动相应的 CGI 程序，CGI 程序会找到 PHP 的配置文件 php.ini 进行程序的初始化，然后处理请求，再以规定的 CGI 的规定转换为相应的格式返回给 web server，最后返回给客户端。这就是一个完整的访问流程。 2.几个概念： Web Application：指 Apache 、Nginx Web Server：指 PHP、Java 等。 CGI：通用网关接口（Common Gateway Interface），是 Web Server 和 Web Application 之间的一种数据交换协议。 CGI 就相当于两个不同国家的人用来交流的翻译器。Web Server 将数据（url，查询字符串，POST 数据，HTTP header 等）通过标准的输入传递给 Web Application ， Web Application 根据配置文件初始化后(如 PHP 解析器会先根据 php.ini 文件进行初始化)，再处理数据，通过标准的输出返回给 Web Server。 但是 CGI 有个蛋疼的地方，就是每一次web请求都会有启动和退出过程，也就是最为人诟病的 fork-and-execute 模式，这样一在大规模并发下，就死翘翘了。 FastCGI：同 CGI，是一种通信协议，在 CGI 的效率上做了优化。 做了哪些优化呢，“PHP 解析器会先根据 php.ini 文件进行初始化” 这里。每次执行程序都会执行一次这个操作，随着 web 的兴起，高并发是常态，这样低效且浪费资源的方式，导致处理每个请求都耗费很长时间。 FastCGI 会先启动一个 master，解析配置文件，初始化执行环境，然后在启动多个 worker。当有新请求过来时，通过 master 会传递给 worker 然后立即执行下一给请求，这样就避免了重复劳动，提高了效率。 当 worker 不够用可以根据配置提前启动几个 worker 等着。当 worker 多余时，也会停掉一些，这样就提高了性能，节约了资源。 FastCGI 是一个常驻性版本的 CGI，它可以一直执行，不用每次都花费时间去 fork。 FastCGI 接口方式采用 C/S 的架构，可以将 Web Server 和 Web Application 分开，同时在服务器上启动一个或多个守护进程。当有请求过来时，直接发送给 FastCGI 进程处理，将结果返回给浏览器。这种方式可以让 Web Server 专心处理静态数据或者将动态数据传给 Web Application ,很大程度上提高了性能。 PHP-CGI：是 PHP（Web Application）对 Web Server 提供的 CGI 的接口程序。 官方出品的 CGI 程序。但是性能太差。 在修改了 php.ini 文件后，必须重启 PHP-CGI 才能让新 php.ini 生效。无法平滑重启 直接杀死 PHP-CGI 后程序将无法执行。 PHP-FPM：是 PHP（Web Application）对 Web Server 提供的 FastCGI 的接口程序。还提供了相对智能的任务管理。 PHP-FPM 是 FastCGI 的具体实现，负责管理一个进程池，处理 Web Server 的请求。 是一个 PHP 进程管理器，包含 master 和 worker 两种进程。master 进程只有一个，负责监听端口，接收来自服务器的请求，而 worker 根据配置可以有多个，每个进程内部嵌入了一个 php 解释器，是 PHP 代码真正执行的地方。 具体步骤如下： FPM 的 master 接受请求 master 根据配置指定给 worker 进行请求处理，如没有可用 worker 则返回错误，nginx 报 502。 worker 请求超时，返回错误，nginx 报 504。 请求处理结束，返回结果。 当修改了 php.ini 之后，PHP-CGI 程序是无法重启的。所以 PHP-FPM 的做法是当旧的进程处理完旧请求后，杀死。通过启动新的 worker 进程来启用新的配置，这样实现平滑重启。 3.nginx Nginx 不仅仅是一个 Web 服务器，也是一个功能强大的 Proxy 服务器。提供了很多协议，如 HTTP，当然也有与 FPM 相关的 FastCGI 协议。Nginx 提供了 FastCGI 模块来将 HTTP 请求映射为对应的 FastCGI 请求。 Nginx 的 FastCGI 模块提供了 fastcgi_param 指令，主要处理参数的映射关系。这些都是基本不变的，会单独放在一个文件中。 $ cat /usr/local/nginx/conf/fastcgi.conf fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; fastcgi_param REQUEST_METHOD $request_method; fastcgi_param CONTENT_TYPE $content_type; fastcgi_param CONTENT_LENGTH $content_length; fastcgi_param SCRIPT_NAME $fastcgi_script_name; fastcgi_param REQUEST_URI $request_uri; fastcgi_param DOCUMENT_URI $document_uri; fastcgi_param DOCUMENT_ROOT $document_root; fastcgi_param SERVER_PROTOCOL $server_protocol; fastcgi_param REQUEST_SCHEME $scheme; fastcgi_param HTTPS $https if_not_empty; fastcgi_param GATEWAY_INTERFACE CGI/1.1; fastcgi_param SERVER_SOFTWARE nginx/$nginx_version; fastcgi_param REMOTE_ADDR $remote_addr; fastcgi_param REMOTE_PORT $remote_port; fastcgi_param SERVER_ADDR $server_addr; fastcgi_param SERVER_PORT $server_port; fastcgi_param SERVER_NAME $server_name; 在 Nginx 的服务器配置中，最重要的 fastcgi_pass 指令，这个指令用于监听 FPM 进程地址，Nginx 会把所有的 php 请求 翻译成 FastCGI 请求再发送给这个地址。下面一个服务器配置实例。 server { listen 80; server_name test.me; root /usr/local/web/myproject/public; index index.php index.html index.htm; access_log /usr/local/nginx/logs/test-access.log; error_log /usr/local/nginx/logs/test-error.log; location / { try_files $uri $uri/ /index.php?$query_string; } location ~\\.php$ { include fastcgi_params; fastcgi_param SCRIPT_FILENAME /usr/local/web/myproject/public/$fastcgi_script_name; fastcgi_pass unix:/usr/local/php/var/run/php-fpm.sock; fastcgi_index index.php; } } 在这个配置文件中，我们新建了一个虚拟主机，监听端口 80，项目根目录为 /usr/local/web/myproject/public。然后通过 location 指令，将所有以 .php 结尾的请求都交给 FastCGI 模块处理。从而把所有的PHP请求都交给了FPM处理，从而完成Nginx到FPM的闭环。 一次完整的请求过程 在浏览器输入网之后，请求会发送给某个 ip 地址下的 80 端口。 负责监听 80 端口的 nginx 会接收到这个请求，由于这是一个 HTTP 请求，需要交给对应的模块处理。 经过初始化和解析请求，确定请求是在 .php 文件中。说明这是一个 PHP 请求，需要 PHP 模块来解析。但是 PHP 和 nginx 不能直接通信，他们之间需要 fastcgi 协议来交换数据。 而 php-fpm 正是 fastcgi 的实现，所以请求转到了 php-fpm。php-fpm 有一个 master 主进程，主进程负责监听端口和接受来自服务器的请求。主进程下包含若干个 worker，每个 worker 下都有一个 php 解释器。php 解释器处理完请求后，返回给 php-fpm 再交给 nginx 发送给浏览器，完成了一次请求过程。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"PHP-基础知识/垃圾回收机制.html":{"url":"PHP-基础知识/垃圾回收机制.html","title":"垃圾回收机制","keywords":"","body":"垃圾回收机制 PHP 的内部变量中包含两个字节： 一个是 is_ref ,布尔值，用来标识该变量是否是引用。通过这个字节将普通变量和引用变量区分开。 第二个是 refcount,表示指向这个变量的变量有多少个。 当发生 unset 时，会减少 refcount 结果为： a: (refcount=3, is_ref=0)='new string' a: (refcount=1, is_ref=0)='new string' 在数组中： 'life', 'number' => 42 ); $a['life'] = $a['meaning']; xdebug_debug_zval( 'a' ); ?> a: (refcount=1, is_ref=0)=array ( 'meaning' => (refcount=2, is_ref=0)='life', 'number' => (refcount=1, is_ref=0)=42, 'life' => (refcount=2, is_ref=0)='life' ) 回收周期 清理垃圾有两个准则： 如果一个变量 value 的 refcount 减少到 0，则 value 会被释放掉，不属于垃圾。 如果一个变量 value 的 refcount 减小后仍大于 0，则此 zval 还不能被释放掉，但可能是一个垃圾。 垃圾回收 以这个为例子： 在 unset 之前，step1 会根据算法计算，对这个数组中所有元素进行 refcount -1 的操作，由于索引 1 对应的是 zval_a ，所以 -1 后 refcount 变为了 1。说明不是垃圾。 当执行 unset 后（step2），继续进行算法计算，由于环形引用，会得到上面的垃圾，zval_a 的 refcounnt 是 1，当对所有元素进行 -1 时，zval_a 的值会变为 0，则就会被认为是垃圾。 参考： 一看就懂系列之 由浅入深聊一聊php的垃圾回收机制 官方文档 PHP的垃圾回收机制 垃圾回收 深入理解PHP7内核之zval Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"PHP-基础知识/写时复制.html":{"url":"PHP-基础知识/写时复制.html","title":"写时复制","keywords":"","body":"写时复制（COPY-ON-WRITE） 当 $a = 1 时，内部变量 refcount=1。当 $b = $a，由于他们俩的值相同，PHP 会采用直接复制的方法，让 $a $b 指向同一个 1，这时 refcount=2。表示这个值被两个变量引用了。 当 $b = 2 时，这个时候 $b 会复制一份 1 出来，再将 1 改为 2，并且 $a $b 的 refcount 都改为 1；这叫写时复制。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-27 15:37:05 "},"Redis/01.基础数据结构.html":{"url":"Redis/01.基础数据结构.html","title":"01.基础数据结构","keywords":"","body":"01.基础数据结构 Redis 有五种数据结构，分别是 String（字符串）、List（列表〉、Hash （字典〉 、 Set（集合）和 Zset（有序集合〉。 1.String 字符串 Redis 的字符串是动态字符串，是可以修改的字符串，采用预分配冗余空间的形式来减少内存的频繁分配。 当字符串小于 1M 时，按原来的的空间加倍来扩容。如当前空间 16kb，扩容后是 32kb。 当字符串大于 1M 时，只会比当前空间多加 1M 来扩容。如当前空间 2M，扩容后是 3M。 Redis 的 String 类型最大长度是 512MB。 String 的用途 缓存用户信息，通过对 json 序列化，将用户所有信息存储到 String。同样以 json 反序列化来取出用户数据。 # 设置键值对， set [key] [value] > set name codehole OK # 获取某个键， get [key] > get name \"codehole\" # 查询某个键是否存在， exists [key] > exists name (integer) 1 # 删除某个键 ，del [key] > del name (integer) 1 > get name # 再次查询则已经被删除 (nil) 批量设置键值对，可以节省网络耗时开销 # 批量赋值， mset [key1] [value1] [key2] [value2] .... > mset name1 boy name2 girl name3 unknown OK # 批量查询，mget [key1] [key2] ... > mget name1 name2 name3 1) \"boy\" 2) \"girl\" 3) \"unknown\" 过期和 set 的扩展命令 # 给某个键添加过期时间， expire [key] [secend] > set name codehole > get name ” codehole \" > expire name 5 ... # wait for 5s > get name # 已经被删除了 (nil) # 当对一个有 ttl 的 key 进行重新赋值时，会删除原来的 ttl # 设置键值对并且设置过期时间， setex [key] [expire] [value] > setex name 5 codehole # 5s 后过期，等价于 set+expire > get name \"codehole\" ... # wait for 5s > get name (nil) # 设置键值对，若存在则不创建，否则创建。 setnx [key] [value] > setnx name codehole # 如果 name 不存在就执行 set 创建 (integer) 1 > get name \"codehole\" > setnx name holycoder (integer) 0 # 因为 name 已经存在，所以 set 创建不成功 > get name \"codehole\" # 没有改变 计数 > set age 30 OK # 通过 incr [key], incrby [key] [increment] 来做加法 # 通过 decr [key], decrby [key] [decrement] 来做减法 > incr age (integer) 31 > incrby age 5 (integer) 36 > incrby age -5 (integer) 31 > decrby age 5 (integer) 26 # 如果 value 是一个整数，那么他的范围是在 signed long 的最大和最小值之间。超过范围则 Redis 报错。 > set codehole 9223372036854775807 # Long.Max OK > incr codehole (error) ERR increment or decrement would overflow 2.List 列表 Redis 的列表是个链表，对于插入和删除操作的时间复杂度是 O(1)。但是索引定位 index 会很慢，时间复杂度为 O(n)。 当列表弹出最后一个元素后，则该数据结构被自动删除，内存释放。 #从 左/右 边压入，lpush/rpush [key] [value1] [value2] [value3] #从 左/右 边弹出，lpop/rphp [key] 队列：右进左出，先进先出 常用于消息队列和异步逻辑处理，确保元素的访问顺序性> rpush books python java golang (integer) 3 > llen books (integer) 3 > lpop books \"python\" > lpop books \"java\" > lpop books \"golang\" > lpop books (nil) 栈：右进右出 > rpush books python java golang (integer) 3 > rpop books \"golang\" > rpop books \"java\" > rpop books \"python\" > rpop books (nil) 慢操作 > rpush books python java golang (integer) 3 # 查寻元素，从 0 开始，index 可以为负数，-1 表示最后一个。lindex [key] [index] > lindex books 1 # O(n) 慎用 \"java\" > lrange books 0 -1 # 获取所有元素，O(n) 慎用 1) \"python\" 2) \"java\" 3) \"golang\" # 截取其中一部分，ltirm [key] [start] [stop] > ltrim books 1 -1 # O(n) 慎用 OK > lrange books 0 -1 1) \"java\" 2) \"golang\" > ltrim books 1 0 # 这其实是清空了整个列表，因为区间范围长度为负 OK # 查看长度， llen [key] > llen books (integer) 0 3.Hash 哈希 Hash 在移除最后一个元素后，该数据结构自动被删除，内存被回收。 常用于存储用户信息，它可以单独设置或取出用户数据，与 String 一次性取出用户所有数据相比，节省了网络流量。但是 Hash 结构的存储消耗要高于单个 String。所以要根据实际情况进行权衡。 # 向键中插入单个列，hset [key] [field] [value] > hset books java \"think in java\" # 命令行的字符串如果包含空格，要用引号括起来 (integer) 1 > hset books golang \"concurrency in go\" (integer) 1 > hset books python \"python cookbook\" (integer) 1 # 获取该键所有列，hgetall [key] > hgetall books # entries()，key 和 value 间隔出现 1) \"java\" 2) \"think in java\" 3) \"golang\" 4) \"concurrency in go\" 5) \"python\" 6) \"python cookbook\" # 查看该键长度，hlen [key] > hlen books (integer) 3 # 查看该键中某列的值，hget [key] [field] > hget books java \"think in java\" > hset books golang \"learning go programming\" # 因为是更新操作，所以返回 0，但是更新成功 (integer) 0 > hget books golang \"learning go programming\" # 批量设置， hmset [key] [field1] [value1] [field2] [value2] ... > hmset books java \"effective java\" python \"learning python\" golang \"modern golang programming\" # 批量 set OK 对单个 key 进行操作，与 String 的 incrby 用法一样。 # 老钱又老了一岁 > hincrby user-laoqian age 1 (integer) 30 4.Set 集合 Redis 中的 set 是一个无需集合，其内部是「唯一」且「无序」的。 当集合中最后一个元素被移除之后， 数据结构被自动删除， 内存被回收。 可以用作存储活动中奖用户的 id。因为有去重功能，可以保证一个用户不会中奖两次。 # 添加一个或多个元素到集合，sadd [key] [value1] [value2]... > sadd books python (integer) 1 > sadd books python # 重复，则不添加 (integer) 0 > sadd books java golang (integer) 2 # 查看一个集合，smembers [key] > smembers books # 注意顺序，和插入的并不一致，因为 set 是无序的 1) \"java\" 2) \"python\" 3) \"golang\" # 判断一个元素是否是集合的元素，sismember [key] [value] > sismember books java # 查询某个 value 是否存在，相当于 contains(o) (integer) 1 > sismember books rust (integer) 0 # 查询一个集合有多少个元素， scard [key] > scard books # 获取长度相当于 count() (integer) 3 # 弹出一个集合元素， spop [key] > spop books # 弹出一个 \"java\" 5.Zset 有序集合 同样是一个集合，但集合中每个元素多了一个 score，代表这个元素在集合中的权重。 Zset 可以用来记录粉丝/点赞列表，value 为 用户 id，score 为关注/点赞时间。 Zset 也可以存储学生成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次。 # 向有序集合中添加一个元素 zadd [key] [score] [value] > zadd books 9.0 \"think in java\" (integer) 1 > zadd books 8.9 \"java concurrency\" (integer) 1 > zadd books 8.6 \"java cookbook\" (integer) 1 # 顺序查询，zrange [key] [start] [stop] > zrange books 0 -1 # 按 score 排序列出，参数区间为排名范围 1) \"java cookbook\" 2) \"java concurrency\" 3) \"think in java\" # 逆序查询，z revrange [key] [start] [stop] > zrevrange books 0 -1 # 按 score 逆序列出，参数区间为排名范围 5) \"think in java\" 6) \"java concurrency\" 7) \"java cookbook\" # 查看集合中元素个数，zcard [key] > zcard books # 相当于 count() (integer) 3 > zscore books \"java concurrency\" # 获取指定 value 的 score \"8.9000000000000004\" # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题 # 查询某个元素在集合中的排名，zrank [key] [value] > zrank books \"java concurrency\" # 排名 (integer) 1 # 查询某个 score 段的元素，zrangebyscore [key] [range_start] [range_end] > zrangebyscore books 0 8.91 # 根据分值区间遍历 zset 1) \"java cookbook\" 2) \"java concurrency\" > zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。 1) \"java cookbook\" 2) \"8.5999999999999996\" 3) \"java concurrency\" 4) \"8.9000000000000004\" # 删除某个元素，zrem [key] [value] > zrem books \"java concurrency\" # 删除 value (integer) 1 > zrange books 0 -1 1) \"java cookbook\" 2) \"think in java\" 6.容器型数据结构的通用规则 list/set/hash/zset 这四种数据结构是容器型数据结构，它们共享下面两条通用规则： create if not exists 如果容器不存在，则创建一个，再进行操作。如 hset 时没有 hash 容器，先创建一个容器，再进行插入。 drop if no elements 如果一个容器没有任何元素了，则删除容器。如 lpop 弹出最后一个元素，则删除该队列。 7.过期时间 Redis 所有的数据结构都可以设置过期时间，时间到了，Redis 会自动删除相应的对象。需要注意的是过期是以对象为单位，比如一个 hash 结构的过期是整个 hash 对象的过期，而不是其中的某个子 key。 如果一个字符串设置了时间，然后用 set 方法修改了它，那么它的过期时间则会消失。 127.0.0.1:6379> set codehole yoyo OK # 所有容器都支持这个语法 ， expire [key] [secend] 127.0.0.1:6379> expire codehole 600 (integer) 1 127.0.0.1:6379> ttl codehole (integer) 597 127.0.0.1:6379> set codehole yoyo OK 127.0.0.1:6379> ttl codehole (integer) -1 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/02.分布式锁.html":{"url":"Redis/02.分布式锁.html","title":"02.分布式锁","keywords":"","body":"02.分布式锁 原子性：是运行过程中的最小单位。 原子操作：指在不会被线程调度机制打断的操作。这种操作一旦开始，会一直运行到结束，不会被打断。 分布式锁：很常见的例子是，通过某个接口查询数据库，由于访问量大，一般都会加一层缓存，并且加上过期时间。但是这里有个问题是，当缓存过期的瞬间，会有大量的请求穿透去数据库查询，导致宕机。而分布式锁就可以解决这个问题。 分布式锁至少需要满足几个条件： 互斥，在任何时刻，同一个锁只能由一个客户端用户锁定。 不会死锁，就算持有锁的客户端在持有期间崩溃了，也不影响其他客户端用户加锁。 谁加锁谁解锁。就是解锁要验证客户端身份，不能被其他客户端解锁。 错误的方式 1.通常会以这一的方式来加锁 >setnx lock-key true OK ... do something ... >del lock-key OK 这里会有个问题，如果在加锁后，执行 del 之前服务器挂了，那么锁就不会被删除，会导致死锁。 好，那么加一个过期时间怎么样。看下面这个例子 2.加过期时间 在上个例子中，加上过期时间。 >setnx lock-key true OK # 加上过期时间 expire lock-key 5 ... do something ... >del lock-key OK 加上过期时间后应该就不会死锁了吧。但是，如果在 setnx 和 expire 之间，服务器挂了呢，同样也会造成死锁这个问题。 发现问题了吧，原因是 setnx 和 expire 这两个操作是两个命令而不是一个原子操作。不过好在 Redis 官方给了个解决方案，使用 set 命令。 3.使用 set 命令 从 Redis 2.6.12 版本开始， SET 命令的行为可以通过一系列参数来修改： set 命令：SET key value [EX|PX] [NX|XX] EX|PX ：过期时间，单位秒/毫秒。 NX：只有键不存在，才对键进行操作。 SET key value NX 效果等同于 SETNX key value 。 XX：只有键存在，才对键进行操作。 那么我们使用 set 来修改上面的列子。 >set lock-key true EX 5 NX OK ... do something ... >del lock-key OK 4.超时问题 在上面例子中，我们解决了加锁问题，但是还有一些问题没有解决。如果客户端 A 持有锁过期了，但是它的临界区的逻辑没有执行完，客户端 B 提前持有了锁，导致代码代码无法严格串行执行下去。 这里产生了两个问题： 验证锁的所有者 删除锁 很遗憾，Redis 中没有「验证锁」同时「删除锁」的原子性操作。不过可以使用 lua 脚本来实现。 if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end PHP 实现 set($key, $requireId, self::MILLISECONDS_EXPIRE_TIME, $expireTime, self::IF_NOT_EXIST); return (String)$result === self::LOCK_SUCCESS; } /** * @param \\Predis\\Client $redis redis客户端 * @param String $key 锁 * @param String $requireId 请求 id * @return bool */ public static function releaseLock(\\Predis\\Client $redis, String $key, String $requireId) { // lua 脚本 $lua = eval($lua, 1, $key, $requireId); return self::RELEASE_SUCCESS === $result; } } 可重入性 可重入性：是指线程在持有锁的情况下再次请求加锁， 如果一个锁支持同一个线程的多次加锁，那么这个锁就是可重入的。 暂时没体会到有什么作用，等以后了解用法后再更新。 应用 在 VRM 中，修改仓库设置，用到了 redis 分布式锁。防止多人同时修改仓库设置。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 16:04:40 "},"Redis/03.延时队列.html":{"url":"Redis/03.延时队列.html","title":"03.延时队列","keywords":"","body":"03.延时队列 一般常用的消息队列有 RabbitMQ, KAFKA 这样强大的中间件，可以再应用之间进行异步地消息传递功能。它们功能强大并专业，特性诸多，但是使用起来也比较繁琐。 有了 redis，可以从中解脱出来，不过如果对消息的可靠性有较高的要求，那么 redis 就不适合了，还是得用 RabbitMQ, KAFKA 专业的中间件来。 异步消息队列 使用 redis 的 List 数据结构来作为消息队列，通过 lpush,rpop 或者 rpush,lpop 进行插入和消费。 这里会存在一个问题，如果队列空了会怎么办。 我们可以 sleep 一会，降低 CPU 占用率和 QPS。但是这也会导致消息队列产生延迟问题。 所以，可以通过 blpop/brpop 替代前面的 lpop/rpop。通过阻塞读的方式来降低 CPU 和 QPS，并且延迟几乎为零。（b 代表 blocking 阻塞读：在队列没有数据的时候，会立即进入休眠状态，一旦数据来了便立即醒来。） 但是，上面的方案还是有问题，因为如果线程一旦阻塞在那里，Redis 的客户端连接就成了闲置连接，闲置过久，服务器便会主动断开，减少闲置资源占用。这个时候 blpop/brpop 就会抛出异常。 所以，消费的时候一定要注意，捕获异常并重试 锁冲突问题 如果客户端加锁没加成功该怎么办呢？ 直接抛出异常，通知用户重试。 这种本质上是放弃当前请求，让用户决定是否重新尝试。 sleep 一会再重试。 sleep 会阻塞当前线程，并且导致队列后续消息产生延迟。如果因为个别死锁 key 导致加锁不成功，线程便会彻底堵死，后续任务永远无法处理。 将请求转入延时队列，过会再重试。 这种方式比较适合异步消息处理，将当前冲突的请求移到另一个队列延后处理以避开冲突。 延迟队列实现 一般通过 Redis 的 zset（有序集合） 数据结构来实现。将消息序列化为一个字符串作为 zset 的 value，到期时间作为 score，然后采用多线程轮询的方式来进行处理。多线程为了保障可用性，万一挂了一个线程，其他线程也能正常处理。但是有多线程，还要解决并发争抢任务，防止一个任务被执行多次。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/04.位图.html":{"url":"Redis/04.位图.html","title":"04.位图","keywords":"","body":"04.位图 Redis 可以做位运算。如同正常的位运算一样。 用处： 用户签到：月签，年签，利用 Redis 的位图可以轻松统计出用户每月/年的签到次数。想想如果有上亿用户，签到需要的空间是非常惊人的。 用户在线人数统计：用户 id 主键为比特位，在线为 1，不在线为 0，通过位图轻松记录有多少人在线，或者确定谁在线。1亿用户只需要 10M 的大小就能做到（100000000/1024(mb)/1024(kb)/8（bit））。 基本用法 如 'h' 这个字符，通过 Python 的 bin(ord('h')) 可以将 h 转换为 ASCII 码「104」，再转换为二进制即 「01101000」。 比特位 0 1 2 3 4 5 6 7 比特值 0 1 1 0 1 0 0 0 通过 set 方法创建一个 h，然后通过 getbit 来获取转化成二进制的字符在指定比特位的值。 获取指定位置的比特位的值 getbit [key] [offset] 127.0.0.1:6379> set demo h # 设置一个键为 h，其二进制是 01101000 OK 127.0.0.1:6379> getbit demo 0 # 比特位第一位 0 (integer) 0 127.0.0.1:6379> getbit demo 1 # 比特位第一位 1 (integer) 1 127.0.0.1:6379> getbit demo 2 (integer) 1 127.0.0.1:6379> getbit demo 3 (integer) 0 127.0.0.1:6379> getbit demo 4 (integer) 1 127.0.0.1:6379> getbit demo 5 (integer) 0 127.0.0.1:6379> getbit demo 6 (integer) 0 127.0.0.1:6379> getbit demo 7 (integer) 0 第二个例子，我们通过设置比特位「01101000」创建一个 h 字符。 设置指定位置的比特位的值 setbit [key] [offset] [value] 127.0.0.1:6379> setbit demo2 0 0 (integer) 0 127.0.0.1:6379> setbit demo2 1 1 (integer) 0 127.0.0.1:6379> setbit demo2 2 1 (integer) 0 127.0.0.1:6379> setbit demo2 3 0 (integer) 0 127.0.0.1:6379> setbit demo2 4 1 (integer) 0 127.0.0.1:6379> setbit demo2 5 0 (integer) 0 127.0.0.1:6379> setbit demo2 6 0 (integer) 0 127.0.0.1:6379> setbit demo2 7 0 (integer) 0 127.0.0.1:6379> get demo2 \"h\" 当对应的字节不可打印时，则 Redis 会返回一个十六进制的值 127.0.0.1:6379> setbit x 0 1 (integer) 0 127.0.0.1:6379> setbit x 1 1 (integer) 0 127.0.0.1:6379> get x \"\\xc0\" 统计与查找 统计一个键从第 start 个「字符」到第 end 个「字符」的「比特位」有多少个「1」，bitcount [key] [start] [end] 注意这里的 start 是字符，如 hello 的第一个字符是 h。 127.0.0.1:6379> set w hello OK 127.0.0.1:6379> bitcount w # hello 转换成二进制后有 21 个 1 (integer) 21 127.0.0.1:6379> bitcount w 0 0 # 统计第一个字符 h 中有多少个 1。 (integer) 3 127.0.0.1:6379> bitcount w 0 4 # 统计从第一个字符开始到第五个字符有多少个 1 (integer) 21 127.0.0.1:6379> bitcount w 1 4 # 统计从第二个字符开始到第五个有多少个 1 (integer) 18 返回从第 start 个字符到第 end 个字符之间，第一个比特值为 1 的比特位的位置，bitpos [key] [start] [end] 127.0.0.1:6379> set w hello OK 127.0.0.1:6379> bitpos w 1 1 1 # 从第二个字节开始，第一次出现的 1 的比特位是 9。 (integer) 9 127.0.0.1:6379> bitpos w 1 2 2 # 从第三个字节开始，第一次出现的 1 的比特位是 17。 (integer) 17 运算 对一个或多个 key 进行位运算。 bitop and destkey key [key...] 对一个或者多个 key 求逻辑并，并将结果保存到 destkey bitop or destkey key [key...]对一个或者多个 key 求逻辑或，并将结果保存到 destkey bitop xor destkey key [key...]对一个或者多个 key 求逻辑异或，并将结果保存到 destkey bitop not destkey key 对给定 key 求逻辑非，并将结果保存到 destkey。注意这里只能是一个 key 示例： 127.0.0.1:6379> set a a OK 127.0.0.1:6379> set c c OK 127.0.0.1:6379> bitop and result a c (integer) 1 127.0.0.1:6379> get result \"a\" 127.0.0.1:6379> bitop or result2 a c (integer) 1 127.0.0.1:6379> get result2 \"c\" 127.0.0.1:6379> bitop xor result3 a c (integer) 1 127.0.0.1:6379> get result3 \"\\x02\" 127.0.0.1:6379> bitop not result4 a c # 注意这里取否只能是一个 key，两个会报错 (error) ERR BITOP NOT must be called with a single source key. 127.0.0.1:6379> bitop not result4 a (integer) 1 127.0.0.1:6379> get result4 \"\\x9e\" 魔术指令 bitfield 我们设置 (setbit）和获取（getbit）指定位的值都是单个位的， 如果要一次 操作多个位，就必须使用管道来处理。 在 Redis3.2 版本之后，可以使用 bitfield 来操作多个位。 其中 bitfield 有三个子指令 (get set incrby)。它们都可以对指定位片段进行 读写，但是最多只能处理 64 个连续的位，如果超过 64 位，就得使用多个子指令。 返回指定的二进制范围, get [u|i][offset] [start] u 为无符号数，即没有符号位，获取到的位数组全部都是值。 i 为有符号位，即第一位是符号位，剩下的都是值。 有符号数最多可以获取 64 位，无符号数只能获取 63 位（因为 Redis 协议中的 integer 是有符号数，最大 64 位，不能传递 64 位无符号值〉。如果超出限制，则 Redis 会报错。 127.0.0.1:6379> setbit w 0 1 (integer) 0 127.0.0.1:6379> setbit w 1 1 (integer) 0 127.0.0.1:6379> setbit w 2 1 (integer) 0 127.0.0.1:6379> setbit w 3 1 (integer) 0 127.0.0.1:6379> get w # 把 w 设置成 1111 \"\\xf0\" 127.0.0.1:6379> bitfield w get u4 0 #从 0 开始取 4 位无符号数。 (integer) 15 127.0.0.1:6379> bitfield w get i4 0 #从 0 开始取 4 位有符号数。 (integer) -1 127.0.0.1:6379> bitfield w get u3 2 # 从第 3 位开始取 3 个无符号数。二进制 110 ，十进制 6 (integer) 6 127.0.0.1:6379> bitfield w get i2 3 # 从第 4 位开始取 2 个有符号数。二进制为 10，有符号转十进制要进行补码（即反码后+1），则 10 反码为 01 再 +1，为 10，符号位负，所以十进制为 -2。 (integer) -2 对指定的二进制范围进行设置，并返回它的旧值 set [u|i][offset] [start] 127.0.0.1:6379> set w hello OK 129.0.0.1:6379> bitfield w set u8 8 97 ＃从第 9 个位开始，将接下来的 8 个位用无符号数 97 替换 (integer) 101 127.0.0.1:6379> get w \"hallo\" 对指定的二进制范围执行加法操作，并返回它的旧值。可以通过 increment 参数传入负值来进行减法操作。 [overflow] [warp|fail|sat] incrby [u|i][offset] [increment] Redis 提供了溢出策略的子命令： warp (默认)：使用回绕的方法来处理有符号整数和无符号整数的溢出情况 无符号：回绕就像使用数值本身与能够被储存的最大无符号整数执行取模计算，这也是C语言的标准行为。 有符号：上溢将导致数字重新从最小的负数开始计算,而下溢将导致数字重新从最大的正数开始计算。比如：127 的i8执行加一操作 那么得到的结果是-128 sat：使用饱和计算方法处理溢出,也就是说,下溢计算的结果为最小的整数值,而上溢计算的结果为最大的整数值 例子：如果对一个值为120的i8整数执行加10计算,那么命令的结果将i8所能存储的最大整数值为127。相反,如果针对i8值计算造成了下溢,那么这个i8值将被设置为-127。 fail：在这一模式下,命令将拒绝执行那些会导致上溢或者下溢情况出现的计算,并向用户返回空值表示计算未被执行。 warp 例子： 127.0.0.1:6379> set w hello OK 127.0.0.1:6379> bitfield w incrby u4 2 1 (integer) 11 127.0.0.1:6379> bitfield w incrby u4 2 1 (integer) 12 127.0.0.1:6379> bitfield w incrby u4 2 1 (integer) 13 127.0.0.1:6379> bitfield w incrby u4 2 1 (integer) 14 127.0.0.1:6379> bitfield w incrby u4 2 1 (integer) 15 127.0.0.1:6379> bitfield w incrby u4 2 1 (integer) 0 127.0.0.1:6379> set w hello OK 127.0.0.1:6379> bitfield w incrby i4 2 1 (integer) -5 127.0.0.1:6379> bitfield w incrby i4 2 1 (integer) -4 127.0.0.1:6379> bitfield w incrby i4 2 1 (integer) -3 127.0.0.1:6379> bitfield w incrby i4 2 1 (integer) -2 127.0.0.1:6379> bitfield w incrby i4 2 1 (integer) -1 sat 例子： 127.0.0.1:6379> set w hello OK 127.0.0.1:6379> bitfield w overflow sat incrby u4 2 1 (integer) 11 127.0.0.1:6379> bitfield w overflow sat incrby u4 2 1 (integer) 12 127.0.0.1:6379> bitfield w overflow sat incrby u4 2 1 (integer) 13 127.0.0.1:6379> bitfield w overflow sat incrby u4 2 1 (integer) 14 127.0.0.1:6379> bitfield w overflow sat incrby u4 2 1 (integer) 15 127.0.0.1:6379> bitfield w overflow sat incrby u4 2 1 (integer) 15 fail 例子： 127.0.0.1:6379> set w hello OK 127.0.0.1:6379> bitfield w overflow fail incrby u4 2 1 (integer) 11 127.0.0.1:6379> bitfield w overflow fail incrby u4 2 1 (integer) 12 127.0.0.1:6379> bitfield w overflow fail incrby u4 2 1 (integer) 13 127.0.0.1:6379> bitfield w overflow fail incrby u4 2 1 (integer) 14 127.0.0.1:6379> bitfield w overflow fail incrby u4 2 1 (integer) 15 127.0.0.1:6379> bitfield w overflow fail incrby u4 2 1 ＃不执行 (nil) Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 16:05:44 "},"Redis/05.HyperLogLog 统计.html":{"url":"Redis/05.HyperLogLog 统计.html","title":"05.HyperLogLog 统计","keywords":"","body":"05.HyperLogLog 统计 为了很好说明这个数据结构的作用，举个例子：通常会有这样的需求，统计网站上每个网页的 PV 和 UV。 PV：采用 Redis 的 String 来计数，每个网页分配一个独立计数器就行。来一个请求就执行 incr 一次。 UV：UV 与 PV 不一样，它要去重，同一个用户一天之内的多次访问请求只能计数一次。每个网页都需要带上用户的 id 来记录。通常想到的是使用 Set 数据结构。但当 UV 特别大的时候，Set 就特别浪费空间。那么就需要更好的方案来解决。 使用 Redis 自带的 HyperLogLog 数据结构来统计。它是用来做基数统计的算法，HyperLogLog 的 优点：在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 什么是基数? 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 HyperLogLog 有三个指令： pfadd key element [element ...] 添加指定元素到 HyperLogLog 中。 PFCOUNT key [key ...] 统计指定元素的基数估计值。 PFMERGE destkey sourcekey [sourcekey ...] 合并多个基数值成为一个新的基数值。 127.0.0.1:6379> pfadd pv user1 user2 user3 user4 (integer) 1 127.0.0.1:6379> pfcount pv (integer) 4 127.0.0.1:6379> pfadd pv user1 user2 user3 user4 (integer) 0 127.0.0.1:6379> pfcount pv (integer) 4 127.0.0.1:6379> pfadd uv user5 user6 user7 user8 user9 user10 (integer) 1 127.0.0.1:6379> pfcount uv (integer) 6 127.0.0.1:6379> pfmerge mergeV pv uv # 合并 OK 127.0.0.1:6379> pfcount mergeV # 发现得到 10 (integer) 10 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 16:05:59 "},"Redis/06.布隆过滤器.html":{"url":"Redis/06.布隆过滤器.html","title":"06.布隆过滤器","keywords":"","body":"06.布隆过滤器 当处理某一个值是不是已经存在 HyperLogLog 中时，它无能为力，它没有提供这样的方法。 场景：当我们使用新闻 APP 来看新闻时，它会不停的给我们推荐新闻，并且过滤到已经看过的重复新闻。怎么实现呢？ 如果将已收看过的新闻数据存在关系型数据库如 mysql 中，当推送系统进行推送时，可以从每个用户的历史记录里进行筛选，以过滤到看过的新闻，但是如果用户量很大并且看过的新闻很多的时候，服务器性能恐怕是跟不上的。 如果采用缓存，那么又得浪费巨大的空间，这个数据一般是线性增长，坚持的了一个月但是没法坚持更长时间。 高级数据结构布隆过滤器（Bloom Filter）闪亮登场了， 它就是专门用来解决这种去重问题的。它在起到去重作用的同时，在空间上还能节省 90% 以上，只是稍微有那么点不精确， 也就是有一定的误判概率。 可以将布隆过滤器理解为不怎么精确的 set 结构。当布隆过滤器说某个值存在时，这个值可能不存在；当它说某个值不存在时，那么就一定不存在。 安装 # 下载包 git clone https://github.com/RedisBloom/RedisBloom.git # 编译，会得到 redisbloom.so make 记录 redisbloom.so 的位置，然后将它加载到 Redis 中，这里有两个方式。 启动时加入# /path/to 改为自己的路径 redis-server --loadmodule /path/to/redisbloom.so 放入配置文件中，我这边配置文件路径是 /usr/local/etc/redis.conf 添加下面代码后保存，同样 /path/to 改为自己的路径。loadmodule /path/to/redisbloom.so 然后重启 brew services restart redis，之后再打开 redis-cli 就可以使用布隆过滤器了 基本命令 添加一个元素，bf.add [key] [value] 判断一个元素是否存在， bf.exists [key] [value] 添加多个元素，bf.madd [key] [value1] [value2]... 判断多个元素是否存在，bf.mexists [key] [value1] [value2]... 127.0.0.1:6379> bf.add demo user1 (integer) 1 127.0.0.1:6379> bf.add demo user2 (integer) 1 127.0.0.1:6379> bf.add demo user3 (integer) 1 127.0.0.1:6379> bf.add demo user4 (integer) 1 127.0.0.1:6379> bf.add demo user5 (integer) 1 127.0.0.1:6379> bf.exists demo user1 (integer) 1 127.0.0.1:6379> bf.exists demo user10 (integer) 0 127.0.0.1:6379> bf.madd demo user6 user7 user8 user9 user10 1) (integer) 1 2) (integer) 1 3) (integer) 1 4) (integer) 1 5) (integer) 1 127.0.0.1:6379> bf.mexists demo user11 user10 user9 1) (integer) 0 2) (integer) 1 3) (integer) 1 控制布隆控制器的误判率，bf.reserve [key] [error_rate] [initial_size] error_rate：错误率，错误率越低，需要的空间越大。 initial_size：预计需要可能放入的数量。 如果不是用 bf.reserve ,默认 error_rate 是 0.01，initial_size 是 100。 127.0.0.1:6379> bf.reserve demo 0.9 10 OK 127.0.0.1:6379> bf.madd demo user6 user7 user8 user9 user10 1) (integer) 1 2) (integer) 1 3) (integer) 0 4) (integer) 0 5) (integer) 0 127.0.0.1:6379> bf.mexists demo user11 user10 user1 1) (integer) 1 2) (integer) 1 3) (integer) 1 当 initial_size 设置过大时，会浪费存储空间，设置过小会影响准确率。所以一定要尽可能估计好元素个数，避免浪费空间。 同样，error_rate 越小，需要的存储空间就越大。对于不需要那么精确的场合，error_rate 稍大一点，无伤大雅。比如新闻 APP 去重新闻一样，误判只有小部分文章不适合被推送。 布隆过滤器的原理 每个布隆过滤器对应到 Redis 的数据结构就是一个大型的 位数组 和几个不一样的无偏 hash 函数。所谓无偏 hash 函数就是能够把 hash 值算的比较均匀，让元素被 hash 映射到位数组中比较均匀。 如图，baidu 这个词被 hash 函数算出值后取模得到得到一个位置，每个函数都会算出一个位置。 当往布隆过滤器中添加 key 时，会使用多个 hash 函数对 key 进行 hash，算出一个整数索引值，然后对位数组长度取模运算得到一个位置，每个函数都会得到一个位置。再把这些位置都设置为 1，就完成了 add 操作。 向布隆过滤器询问一个 key 是否存在时，与 add 同样的操作，看看这几个值是否为 1，只要有一个为 0 则表示不存在。都为 1 ，并不能说明这个 key 一定存在，而是极有可能存在。因为这个 1 很有可能是别的 key 存在导致的。所以如果这个位数组比较稀疏，判断正确的概率就会很大，否则就小。 空间占用估计 布隆过滤器有两个参数，第一个是预计元素的数量 n，第二个是错误率 f。公式 根据这两个输入得到两个输出，第一个输出是位数组的长度 l，也就是需要的存储空 间大小（ bit ），第二个输出是 hash 函数的最佳数量 k。 hash 函数的数量也会直接影 响到错误率，最佳的数量会有最低的错误率。 k=0.7*(l/n) ＃约等于 f=0.6185^(l/n) ＃ ^表示次方计算，也就是 math.pow 为了省去麻烦，现在有很多现成的在线布隆计算器。 当实际元素超出时，误判率会有怎样变化 f=(1-0.5^t)^k ＃极限近似，k 是 hash 函数的最佳数量。 应用 新闻 app 推送新闻过滤已读新闻。 爬虫系统，过滤已经爬过的 URL。 邮件的垃圾过滤系统，会有某些正常邮件也被放入垃圾邮件目录中，这个就是误判导致，概率很低。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/07.限流.html":{"url":"Redis/07.限流.html","title":"07.限流","keywords":"","body":"07.限流 在高并发场景下有三把利器保护系统：缓存、降级、和限流。 缓存的目的是提升系统的访问你速度和增大系统能处理的容量。 降级是当服务出问题或影响到核心流程的性能则需要暂时屏蔽掉。 限流一般作用于高并发下，除了控制流量外，还有一个应用目的是控制用户行为，避免垃圾请求。如论坛里，控制用户的发帖，回复和点赞等功能。还有商城里如秒杀、抢购、评论、恶意爬虫等。 使用 reids 来实现简单的限流策略 限流算法 常见的限流算法有：计数器，漏桶、令牌桶。 计数器 计数器：记录一段时间窗口内，用户行为总数，判断是否超过限制。 下面是 PHP 的实现方式 isActionAllowed($redis, \"110\", \"reply\", 60 * 1000, 5)); //执行可以发现只有前5次是通过的 } } /** * @param \\Predis\\Client $redis * @param String $userId (用户 id) * @param String $actionName (操作名) * @param Int $period 时间窗口(毫秒) * @param Int $maxCount (最大限制个数) * @return bool * @throws \\Exception */ public function isActionAllowed(\\Predis\\Client $redis, String $userId, String $actionName, Int $period, Int $maxCount) { // 设置键名 $actionKey = sprintf('current-limiting.%s.%s', $actionName, $userId); list($msec, $sec) = explode(' ', microtime()); // 毫秒时间戳 $now = intval(($sec + $msec) * 1000); // 管道 $replies = $redis->pipeline() // value 和 score 都用毫秒时间戳 ->zadd($actionKey, $now, $now) // 移除时间窗口之前的行为记录，剩下的都是时间窗口内的 ->zremrangebyscore($actionKey, 0, $now - $period) // 统计现在个数 ->zcard($actionKey) // 多加一秒过期时间 ->expire($actionKey, $period + 1) // 执行 ->execute(); return $replies[2] 可以发现，这几个 redis 操作都是针对同一个 key，所以使用管道 pipeline 会显著提升效率。但是这个方案也有一个缺陷，就是当量很大时，会占用很大空间，就不适合做限流了（如限定 60 秒，操作不超过 100 万次）。 漏斗限流 顾名思义，算法灵感来自于漏斗。 漏斗容量有限，上面有水龙头灌水，漏斗下面漏水。当灌水速率大于漏水，则漏斗会满，无法再如新水。反之则漏斗永远装不满水。 PHP 实现漏斗法 capacity = $capacity; $this->leakingRate = $leakingRate; $this->leftCapacity = $capacity; $this->lastLeakedTime = time(); } public function makeSpace() { $now = time(); // 距离上一次漏水过去了多久 $deltaTime = $now - $this->lastLeakedTime; // 计算已经腾出了多少空间 $deltaSpace = $deltaTime * $this->leakingRate; // 腾出空间最小单位是 1,太小就忽略 if ($deltaSpace leftCapacity += $deltaSpace; // 记录漏水时间 $this->lastLeakedTime = time(); // 如果剩余容量大于了容器容量,则剩余容量为容器容量 $this->leftCapacity = ($this->leftCapacity > $this->capacity) ? $this->capacity : $this->leftCapacity; } public function watering(float $quota) { //漏水操作 $this->makeSpace(); // 当还有空间时,则减少容器剩余空间 if ($this->leftCapacity >= $quota) { $this->leftCapacity -= $quota; return true; } return false; } } 调用过程 watering(1); } public function demo() { for ($i=0; $iisActionAllowed(\"110\", \"reply\", 15, 0.5)); //执行可以发现只有前15次是通过的 } } 解析代码的关键是 makeSpace() 方法，每次调用前都会触发漏水，腾出多少空间。能腾出多少空间取决于 漏水时间*漏水速率。 分布式的漏斗算法 我们可以把 Funnel 对象存放到 Redis 的 hash 数据结构中。灌水的时候取出进行逻辑计算，再将新值存回到 hash 中。 但是这里有一个问题，无法保证整个过程的原子性。从 hash 中取出，逻辑计算后再存回，这几个过程都无法原子化。意味着需要进行适当的加锁，而一旦加锁，就有失败的可能，加锁失败就需要选择重试或者放弃。重试导致性能下降，放弃就降低用户体验。 于是，Redis-Cell 来了。 Redis 在 4.0 之后提供一个 Redis-Cell 模块，该模块基于漏斗算法，并提供了原子的限流指令。 安装 Redis-Cell 官网：GitHub - brandur/redis-cell: A Redis module that provides rate limiting in Redis as a single command. 在这里下载对应的版本。Releases · brandur/redis-cell · GitHub 我电脑是 mac，下载好后，解压得到 .dylib 文件，如果是 linux 会得到 .so 文件。然后,找到 redis 的配置文件 /usr/local/etc/redis.conf，添加下面一段代码。 loadmodule /usr/local/etc/libredis_cell.dylib 重启 redis,安装就完成了 brew services restart redis Redis-Cell 指令 该模块只有一个指令 cl.throttle。其意思是允许\"某个行为\"的频率是每 60 秒最多 30 次（漏水速率）。漏斗的最大容量为 15。 CL.THROTTLE user123 15 30 60 1 ▲ ▲ ▲ ▲ ▲ | | | | └───── apply 1 token (default if omitted) 可选参数，默认值为 1 | | └──┴─────── 30 tokens / 60 seconds 速率 | └───────────── 15 最大容量 └─────────────────── key \"user123\" 其返回值为： 127.0.0.1:6379> CL.THROTTLE user123 15 30 60 1) (integer) 0 # 0 表示允许，1 表示拒绝 2) (integer) 15 # 漏斗容量 capacity 3) (integer) 14 # 当前容量 leftCapacity 4) (integer) -1 # 如果被拒绝了，需要多少时间后再试（漏斗有空间了，单位秒） 5) (integer) 2 # 多长时间后，漏斗完全空出来（leftCapacity=capacity，单位秒） 在执行指令时，如果被拒绝了，则需要丢弃或重试。重试的话，指令已经算好了时间，直接 sleep 就行，不过会阻塞线程，否则异步定时任务来执行。 漏斗算法的弊端 无法应对短时间的突发流量。 令牌桶算法 令牌桶算法算是漏斗算法的改进，漏斗算法能够限制流出速率，而令牌桶算法能 在限制调用平均速率的同时还允许一定程度上的突发调用 算法描述： 有一个固定容量的桶，按照固定的速率往里面添加令牌。 如果桶满了，新添加的令牌将丢弃。 当请求来时，必须从桶中拿出一个令牌才能继续处理，否则拒绝请求，或者暂存到某个缓冲区等待先从桶中获取令牌再执行请求。 php 实现令牌桶算法 _redis = $redis; $this->_key = $key; $this->_maxCount = $maxCount; } /** * 添加令牌 * @param Int $num * @return Int */ public function add(Int $num = 0) { // 当前剩余令牌数 $leftTokenCount = $this->_redis->llen($this->_key); // 计算最大可加入的令牌数量，不能超过最大令牌数 $num = ($leftTokenCount + $num > $this->_maxCount) ? 0 : $num; if ($num > 0) { // 填充 token，为了简化步骤，这里的 token 值为 1。 $token = array_fill(0, $num, 1); $this->_redis->lpush($this->_key, $token); return $num; } return 0; } /** * 重置令牌桶 */ public function reset() { $this->_redis->del([$this->_key]); $this->add($this->_maxCount); } /** * 获取令牌 * @return string */ public function getToken() { return $this->_redis->rpop($this->_key); } } 执行代码 reset(); for ($i = 0; $i getToken()); } dump($service->add(3)); for ($i = 0; $i getToken()); } 结果 \"1\" \"1\" \"1\" \"1\" \"1\" null null null 3 \"1\" \"1\" \"1\" null null Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/08.GeoHash 算法.html":{"url":"Redis/08.GeoHash 算法.html","title":"08.GeoHash 算法","keywords":"","body":"08.GeoHash 算法 如今移动互联网时代 LBS（Location Based Service 基于位置服务） 应用越来越多，如交友 app 查找附近的人，外卖 app 查找附近的餐馆，地图 app 查找附近的地点等等。那么这些究竟是如何实现的呢？ 我们都知道，地球的位置是使用二维经纬度来表示的，经度 [-180, 180]， 维度 [-90， 90]。只要给出一个地点的经纬度，我们就知道它在地球上的那个位置。 通过 sql 来计算 “附近的人” 比如我们要找“附近的人”，我们的坐标是 (x0,y0)，搜索半径为 r。那么使用如下 SQL 即可： select id from user where x0-r 但是会有什么问题呢： 当并发量大的时候，会搞垮数据库。 计算的是一个矩形的位置，而不是以我为中心 r 公里为半径的圆形方位。 精准度比较低。我们知道地球不是平面坐标系，而是一个圆球，这种矩形计算在长距离计算时会有很大误差 使用 Redis 的 GeoHash GeoHash 算法是将二维经纬度数据映射到一维的整数上，把所有的元素都挂载到了一条数轴上。在数轴上找到我们的点，然后就可以获取附近的点了。 将地球看成一个二维平面 然后划分为一系列的正方形方格，好比围棋棋盘。 将地图坐标都表如所在方格中。方格越小，精度越高。 然后进行编码 编码原理：设想一个正方形的蛋糕摆在你面前，二刀下去均分分成四块小正方形，这四个小正方形可以分别标记为 00,01,10,11 四个二进制整数。然后对每一个小正方形继续用二刀法切割一下，这时每个小小正方形就可以使用 4bit 的二进制整数予以表示。然后继续切下去，正方形就会越来越小，二进制整数也会越来越长，精确度就会越来越高。 原理 主要分为三步 将三维的地球变为二维的坐标 在将二维的坐标转换为一维的点块 最后将一维的点块转换为二进制在通过 base32 编码 Redis 的 Geohash 基于 zset。 指令 添加一个或多个，geoadd [key] [lon1] [lat1] [member1] [lon2] [lat2] [member2] ... （lon：经度，lat：维度） 127.0.0.1:6379> geoadd city 120.20000 30.26667 hangzhou 116.41667 39.91667 beijing 121.47 31.23 shanghai (integer) 3 geohash 没有删除，但是 geohash 基于 zset 数据结构，所以直接使用 zrem 就行了 127.0.0.1:6379> zrem city hangzhou (integer) 1 计算两个元素之间的距离，geogist [key] [member1] [member2] [unit] 127.0.0.1:6379> geodist city beijing shanghai km \"1068.3890\" 获取一个或多个元素位置，geopos [key] [member1] [member2]... 127.0.0.1:6379> geopos city beijing shanghai 1) 1) \"116.41667157411575317\" 2) \"39.91667095273589183\" 2) 1) \"121.47000163793563843\" 2) \"31.22999903975783553\" 获取一个或多个元素 hash 值，geohash [key] [member1].... 127.0.0.1:6379> geohash city beijing shanghai 1) \"wx4g14s53n0\" 2) \"wtw3sj5zbj0\" 获取附近的元素，georadiusbymember [key] [member1] [num] [unit] [withcoord|withdist|withhash] [count] [num] [asc|desc] 不会排除自己。 127.0.0.1:6379> geoadd company 116.48105 39.996794 juejin 116.514203 39.905409 ireader 116.489033 40.007669 meituan 116.562108 39.787602 jd 116.334255 40.027400 xiaomi (integer) 5 # 查找 ireader 附近 20KM 以内，最多 3 个公司，按顺序排列，它不会排除自己 127.0.0.1:6379> georadiusbymember company ireader 20 km count 3 asc 1) \"ireader\" 2) \"juejin\" 3) \"meituan\" # 查找 ireader 附近 20KM 以内，最多 3 个公司，按逆序排列，它不会排除自己 127.0.0.1:6379> georadiusbymember company ireader 20 km count 3 desc 1) \"jd\" 2) \"meituan\" 3) \"juejin\" # 三个可选参数 withcoord（坐标）、 withdist（距离）、 withhash（hash 值）。 127.0.0.1:6379> georadiusbymember company ireader 20 km withcoord withdist withhash count 3 asc 1) 1) \"ireader\" 2) \"0.0000\" 3) (integer) 4069886008361398 4) 1) \"116.5142020583152771\" 2) \"39.90540918662494363\" 2) 1) \"juejin\" 2) \"10.5501\" 3) (integer) 4069887154388167 4) 1) \"116.48104995489120483\" 2) \"39.99679348858259686\" 3) 1) \"meituan\" 2) \"11.5748\" 3) (integer) 4069887179083478 4) 1) \"116.48903220891952515\" 2) \"40.00766997707732031\" 根据坐标查找附近元素，georadius [key] [lon] [lat] [num] [unit] [withcoord|withdist|withhash] [count] [num] [asc|desc] 127.0.0.1:6379> georadius company 116.514202 39.905409 20 km withdist count 3 asc 1) 1) \"ireader\" 2) \"0.0000\" 2) 1) \"juejin\" 2) \"10.5501\" 3) 1) \"meituan\" 2) \"11.5748\" 注意事项 在一个地图应用中，车的数据，餐馆的数据，人的数据可能有几千万条，如果使用 redis 的 geo 数据结构，它们将被全部放在一个 zset 集合中。 在 Redis 集群环境中，集合可能从一个节点迁移到另一个节点，那么单个 key 的数据过大，会导致集群迁移工作造成较大影响。 在集群环境中，单个 key 的大小不要超过 1MB，否则会导致集群迁移出现卡顿现象，影响线上服务正常运行。 所以，一般建议要对 geo 数据使用单独的 redis 实力部署，不使用集群。 如果数据量过亿，甚至更大，就可以对 geo 数据进行拆分，按国家、按省份、按市、按区域拆分，显著降低单个 zset 集合大小。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/09.大海捞针-scan.html":{"url":"Redis/09.大海捞针-scan.html","title":"09.大海捞针-scan","keywords":"","body":"09.大海捞针-scan 在平时 Redis 维护过程中，可能需要从成千上万个 key 中找到特定前缀的 key 列表来手动处理数据。Redis 给了几种方法来处理： keys 指令 指令：keys [正则] 127.0.0.1:6379> keys * 1) \"company\" 2) \"city\" 3) \"demo\" 127.0.0.1:6379> keys c* 1) \"company\" 2) \"city\" 这个指令虽然方便，但是有几个缺点： 没有 limit offset 参数，一次性吐出所有满足条件的 key，如果有上万个那么结果将刷屏。 keys 算法是遍历算法，时间复杂度是 O(n)。如果有上千万条 key，执行这个指令的时候服务器会卡顿。由于 Redis 是单线程程序，顺序执行所有指令，其他指令必须等 keys 指令执行完成后才可继续执行。 为了解决这个问题，Redis 又推出了 scan 指令 scan 指令 scan 指令的特点： 时间复杂度是 O(n)，但是它是通过游标分步进行的，不会阻塞线程。 提供 limit 参数，可以控制返回结果的最大数量，但是 limit 只是一个 hint（提示）,返回结果可多可少。 同 keys 一样，提供匹配功能。 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数。 返回的结果中可能有重复，需要客户端去重，很重要！！！。 遍历的过程中如果有数据修改，不确定是否能遍历到。 单次返回的结果为空不代表遍历结束，而是看返回的游标值是否为零。 指令：scan [cursor] match [匹配] [count]。cursor=游标 事先造 10 个 key。 127.0.0.1:6379> scan 0 match key* count 3 1) \"14\" 2) 1) \"key6\" 2) \"key8\" 3) \"key20\" 127.0.0.1:6379> scan 14 match key* count 3 1) \"1\" 2) 1) \"key7\" 2) \"key5\" 127.0.0.1:6379> scan 1 match key* count 3 1) \"11\" 2) 1) \"key9\" 2) \"key4\" 127.0.0.1:6379> scan 11 match key* count 3 1) \"15\" 2) 1) \"key1\" 2) \"key3\" 127.0.0.1:6379> scan 15 match key* count 3 1) \"0\" 2) 1) \"key2\" 返回第一个结果是游标，作为第二次执行指令。可以理解为第二页的页码，当游标的值为 0 时，则代表遍历结束，也就是平时我们所说的尾页。 除了 scan，其他容器也有类似指令，如 zscan 遍历 zset 集合，hscan 遍历 hash 字典的集合，sscan 遍历 set 集合。 大 key 扫描 为了定位大 key，又想避免线上 Redis 卡顿，官方为我们带来了一个指令 --bigkeys。 redis-cli -h127.0.0.1 -p 7001 --bigkeys 如果担心这个指令会大幅提升线上 Redis 的 ops。可以加一个休眠参数 redis-cli -h127.0.0.1 -p 7001 --bigkeys -i 0.1 这个参数代表每隔 100 条就会休眠 0.1s。 结果 ~ redis-cli --bigkeys -i 0.1 # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest string found so far 'key6' with 1 bytes [00.00%] Biggest zset found so far 'city' with 2 members [00.00%] Biggest zset found so far 'company' with 5 members -------- summary ------- Sampled 13 keys in the keyspace! Total key length in bytes is 56 (avg len 4.31) Biggest string found 'key6' has 1 bytes Biggest zset found 'company' has 5 members 0 lists with 0 items (00.00% of keys, avg size 0.00) 0 hashs with 0 fields (00.00% of keys, avg size 0.00) 10 strings with 10 bytes (76.92% of keys, avg size 1.00) 0 streams with 0 entries (00.00% of keys, avg size 0.00) 0 sets with 0 members (00.00% of keys, avg size 0.00) 1 MBbloom--s with 0 ? (07.69% of keys, avg size 0.00) 2 zsets with 7 members (15.38% of keys, avg size 3.50) Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-18 16:16:54 "},"Redis/10.内存回收机制.html":{"url":"Redis/10.内存回收机制.html","title":"10.内存回收机制","keywords":"","body":"10.内存回收机制 面试题: redis删除很多key, 内存不减少的原因? 答：因为操作系统回收内存是以「页」为单位，如果这个页中只要有一个 key 还在使用，那么这个页就不会被回收。假如 redis 内存有 10 个 G，现在删除 1 个 G 的 key，但是这些 key 是分散在各个内存页中，这就导致内存不会立刻被回收。 如果执行了 flushdb（注：删除所有 key），然后再观察内存，则发现内存被回收。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:44 "},"Redis/11.主从同步.html":{"url":"Redis/11.主从同步.html","title":"11.主从同步","keywords":"","body":"11.主从同步 CAP 原理 所谓 CAP 即： C - Consistent ，一致性 A - Availability ，可用性 P - Partition tolerance ，分区容忍性 分布式系统节点往往分布在不同机器上，当机器间网络断开时，则称之为「网络分区」。 一旦网络分区，将会导致主从系统之间无法同步数据，导致「一致性」无法满足。除非牺牲「可用性」，暂停分布式节点服务，在网络分区时不在提供修改数据功能，知道网络恢复为止。 s 一句话概括 CAP 原理就是——网络分区发生时，一致性和可用性两难全。 最终一致性 Redis 的主从同步是异步的，所以分布式的 Redis 不满足「一致性」要求。但是 Redis保证「最终一致性」。即使网络分区时，主节点依旧可以对外提供正常服务保证「可用性」。当网络恢复时，从节点会努力追赶主节点，最终达到一致。 主从同步 Redis 同步支持主从同步和从从同步，从从同步是后续版本增加的。 增量同步 AOF Redis 同步是指令流，主节点会将对自己产生修改性的指令记录在本地内存的 buffer 中，然后异步的同步给从节点。从节点一边执行同步指令，一边向主节点反馈自己同步到哪了。 但是由于内存的 buffer 是有限的，Redis 的复制内存 buffer 是一个环形数组，如果该数组满了，则会从头覆盖前面的指令。 如因为网络分区，导致从节点无法与主节点进行同步，当网络恢复时，主节点中没有同步的指令在 buffer 中被后续指令给覆盖了。 快照同步 RDB 快照同步是一种非常耗资源的操作，其过程是在主节点上进行一次 bgsave，将当前内存的数据全部快照到磁盘文件中，然后将快照文件传给从节点，从节点接收完毕后，立即执行全量加载，加载之前会将当前内存的数据清空。加载完毕后通知主节点进行增量同步。 整个快照同步过程中，主节点的复制 buffer 还是在一直向前一定，如果快照同步时间过长或者复制 buffer 过小，也会导致增量指令在 buffer 中被覆盖，导致增量同步无法完成，又会再一次发起快照同步，如此可能造成死循环。 所以，需要设置一个合适的复制 buffer 大小，避免快照复制死循环。 增加从节点 当从节点加入到集群时，先执行一次快照同步，再进行增量同步。 无盘复制 主节点进行快照同步时，会进行很重的 io 操作。对于非 ssd 磁盘存储时，会对系统造成较大的负载。特别是当系统正在进行 AOF 的 fsync 操作时如果发生快照，fsync 将会被推迟执行，这就会严重影响主节点的服务效率。 所以从 redis 的 2.8.18 版开始支持无盘复制。所谓无盘复制是指，主节点直接通过套接字（socket）连接从节点，生成快照是一个遍历的过程，主节点会一边遍历内存，一边将序列化的内容发送给从节点，从节点还是跟之前一样，接收完毕后，再一次性加载。 wait Redis 的复制是异步的，wait 指令可以使其变成同步复制，却表系统的一致性（不严格）。 > set key value OK > wait 1 0 (integer) 1 wait 提供两个参数，第一个参数是从库的数量 N，第二个参数是时间 T（毫秒）。它表示 等待 wait 之前的所有写操作同步到 N 个库 (也就是确保 N 个从库的同步没有滞后)，最多等待 T 秒。如果 T=0 ，则表示无限等待直到 N 个从库同步完成达成一致。 如果出现网络分区，wait 指令的第二个参数 T=0，那么主从同步将无法执行，wait 命令将永远阻塞，Redis 服务器将丧失可用性。 Redis主从同步： 增量追赶，写指令流同步 增量太慢了，全量追赶，快照同步 无盘复制，生产快照是一个遍历的过程 主从主要是为了保障Redis的高可用性，同时也能兼顾提升性能 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/12.Redis 集群-sentinel.html":{"url":"Redis/12.Redis 集群-sentinel.html","title":"12.Redis 集群-sentinel","keywords":"","body":"12.Redis 集群-sentinel sentinel 之前只说到主从方案，最终一致性。那么现在有个问题，如果当主节点发生故障时，就必须人为来操作修改主节点等等，这样效率太低，Redis 官方提供了一个方案-sentinel（哨兵）。 如图所示，sentinel 可以看做是集群高可用的心脏，负责监视主从节点的健康状态，当主节点发生故障时，会在从节点中选取最优节点作为主节点。 客户端连接集群时，会首先连接 sentinel ，sentinel 会查询主节点的地址，并告诉客户端，客户端再去连接主节点进行数据交互。 如下图，当主节点发生故障时，客户端会重新向 sentinel 要新主节点的地址。原来的主从连接将断开，客户端和原来的主节点也会断开。如此应用程序无需重启即可完成主从节点切换。 当主节点挂掉后，原来的主从复制也断开。客户端和原主节点的连接也断开了。从节点被提升为主节点后，其他从节点和新主节点建立复制关系。客户端会通过新的主节点继续进行交互。 sentinel 会持续监视已经挂掉的主节点，待其恢复后，会变成新的从节点，集群将会调整为下图。原来的主节点变成了从节点，与新的主节点建立复制关系。 消息丢失 Redis 的复制关系是异步的，当主节点挂掉的时候，可能还没有将所有消息同步给从节点，那么这部分消息可能丢失。如果延迟特别大，丢的消息也就会越多。 sentinel 无法保证消息完全不丢失，但是肯依尽可能的保证少丢失。有下面两个选项 min-slaves-to-write 1 min-slaves-max-lag 10 第一个参数表示，至少有一个从节点在进行正常复制，否则停止对外服务，丧失可用性。 那何为正常复制，何为异常复制呢？这个是由第二个参数控制，它单位是秒，表示 10s 没收到从节点反馈，就意味从节点同步不正常，要么网络分区，要么一直没反馈。 机制 sentinel 的作用 master 节点的状态监测 如果 master 节点异常，会进行 master-slave 切换，选举一个 slave 作为新 master。将之前的 master 作为 slave。 master-slave 切换后，会修改 master 和 slave 的 conf 配置。新 master 的 conf 中会多出 slaveof 的配置。sentinel.conf 的配置也会改变。 sentinel 的工作方式（每个实例都执行的定时任务） 每个 sentinel 每 1 秒都会对已知的 master，slave 发送 ping 命令来确认其状态。 如果有一个实例距离最后一次有效回复 ping 的时间超过了配置中的 own-after-milliseconds 的值，那么这个实例将会被 sentinel 标记为「主观下线」。 如果一个实例被标记为「主观下线」，那么监视这个 master 的所有 sentinel 都要以每 1 秒的频率确认 master 的确进入了「主观下线」状态。 当有足够多（配置的数量）的 sentinel 在指定时间范围内标记了 master 为「主观下线」，则 master 会被标记为「客观下线」。 一般情况下，每个 sentinel 都会每 10 秒对已知的 master、slave 发送 info 命令。 当 master 被 sentinel 标记为 「客观下线」时，则 sentinel 从每 10 秒改为每 1 秒对下线的 master 的所有 slave 进行发送 info 命令。 如果没有足够数量的 sentinel 同意 master 下线，那么 master 的 「客观下线」将会被接触，如果 master 重新向 sentinel 的 ping 命令进行回复，那么「主观下线」将会被接触 每个 sentinel 都有三个定时任务 每 10 秒对 master 和 slave 进行 info 命令。其目的为： a. 发现 master 节点。 b. 确认主从关系。 每 2 秒每个 sentinel 通过 master 节点的 channel 交换信息（pub/sub）。 master 节点上有一个发布订阅的频道(sentinel:hello)。sentinel 节点通过 sentinel:hello 频道进行信息交换(对节点的\"看法\"和自身的信息)，达成共识。 每 1 秒对其他 sentinel 和 redis 节点进行 ping 操作来相互监控。其实这是一个心跳检测，是失败的判断依据。 主观下线（Subjectively Down， 简称 SDOWN）：单个 sentinel 对节点做出的判断。 客观下线（Objectively Down， 简称 ODOWN）：多个的 sentinel 对同一个节点做出 SDOWN 判断。 sentinel.conf 中有 2 个配置说明： sentinel monitor [masterName] [ip] [port] [quorum] a. masterName： 顾名思义，master 节点的名字或者别名。ip 和 port 就是 master 节点的 ip 和端口号。 b. quorum 就是「客观下线」的依据，表示至少有 quorum 个 sentinel 认为 master 故障，才会对这个 master 进行下线并且执行「故障转移」。 sentinel down-after-milliseconds [masterName] [timeout] a. timeout 为毫秒值，表示如果 sentinel 在 timeout 毫秒后仍然没有连通 master 或者 slave，将会主观认为该节点下线。 b. 只有 master 需要「客观下线」。因为 slave 下线不需要「故障转移」。 选举领头 sentinel（领导者选举） 当 master 被判断为「客观下线」时，需要多个 sentinel 进行协商，选举一个领头来对这个节点进行故障转移操作。 其规则是： 所有的 sentinel 都有资格被选举为领头。 所有 sentinel 都有且只有一次机会将某一个 sentinel 选作领头。一旦选定不能修改。 设置 sentinel 领头是先到先得，一旦当前选定，以后要求设置 sentienl 领头将会被拒绝。 每个发现节点「客观下线」的 sentinel，都会要求其他 sentinel 将自己作为领头。 当一个 sentinel（源 sentinel）向另一个sentinel（目 sentinel）发送 is-master-down-by-addr ip port current_epoch runid 命令的时候，runid 参数是 sentinel 运行id，就表示源 sentinel 要求目标 sentinel 选举其为领头。 源 sentinel 会检查目标sentinel对其要求设置成领头的回复，如果回复的 leader_runid 和 leader_epoch 为源 sentinel，表示目标 sentinel 同意将源sentinel设置成领头。 如果某个 sentinel 被半数以上的 sentinel 设置为领头，则选举成功。 如果在限定时间内没有选出领头，则过一段时间再尝试。 选举领头的作用： 因为只能有一个 sentinel 节点去完成「故障转移」。 sentinel is-master-down-by-add 这个命令有两个作用： 确认下线判断。 领头选举。 主从切换方案 sentinel 主要负责三个任务： 监控（Monitoring），监控主从节点是否运行正常。 提醒（Notification），如果被监控的节点发生问题，可以通过 api 向管理员或者其他应用程序发送通知。 自动故障转移（Automatic failover），当 master 发生故障时，sentinel 需要进行「故障转移」，将其中一个 slave 作为新 master，旧 master 作为其 slave。当客户端试图连接失效的 master 时，sentinel 会返回新的 master 地址给客户端。 「故障转移」分为三个步骤： 从下线的 master 的 slave 中选取一个 slave 作为新 master。 领头sentinel从剩下的从列表中选择优先级高的，如果优先级一样，选择偏移量最大的（偏移量大说明复制的数据比较新），如果偏移量一样，选择运行 id 最小的从服务。 对下线的 master 的 slave 对新的 master 进行复制关系。 当下线的 master 设置为新 master 的 slave。 Sentinel 状态的持久化 Sentinel 的状态会被持久化在 Sentinel 配置文件里面。每当 Sentinel 接收到一个新的配置， 或者当领头 Sentinel 为主服务器创建一个新的配置时， 这个配置会与配置纪元一起被保存到磁盘里面。这意味着停止和重启 Sentinel 进程都是安全的。 配置 基于 homestead 环境，Ubuntu 18.04 主从配置 # 跳转到 redis 配置目录 cd /etc/redis # 复制两个从节点配置 sudo cp redis.conf redis-6381.conf sudo cp redis.conf redis-6382.conf 打开其中一个从节点配置文件，修改。 #修改 port 6381 #修改 pidfile /var/run/redis/redis-server-6381.pid #修改 logfile /var/log/redis/redis-server-6381.log # 增加一行 slaveof 127.0.0.1 6379 保存之后，启动两个从节点实例 sudo redis-server /etc/redis/redis-6381.conf sudo redis-server /etc/redis/redis-6382.conf 然后通过 redis-cli 进入主节点，使用命令 info 查看。 127.0.0.1:6379> info . . . # Replication role:master connected_slaves:2 slave0:ip=127.0.0.1,port=6381,state=online,offset=784,lag=1 slave1:ip=127.0.0.1,port=6382,state=online,offset=784,lag=1 master_replid:125d233d1f1ddf1f52300d8ca4720a979b01283f master_replid2:0000000000000000000000000000000000000000 master_repl_offset:798 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:798 省略了一部分结果，我们可以看到， role:master #本机是『主节点』 connected_slaves:2 #有2个『从节点』 slave0:ip=127.0.0.1,port=6381,state=online,offset=784,lag=1 slave1:ip=127.0.0.1,port=6382,state=online,offset=784,lag=1 \"从\"服务器iP地址和端口是 6381 和 6382 接下来我们试验下,首先在主节点设置一个 key。 127.0.0.1:6379> set name demo1 OK 127.0.0.1:6379> get name \"demo1\" # 切换为从节点 vagrant@homestead:/etc/redis$ redis-cli -p 6381 127.0.0.1:6381> get name \"demo1\" 可以看到从节点已经有值，表示主从节点已经有复制关系。 然后尝试在从节点进行写操作 127.0.0.1:6381> set name lisi (error) READONLY You can't write against a read only slave. 报错，说明从节点只能读取，这就是读写分离。 在 laravel 中配置主从 在 config/database.php 中修改、 [ 'client' => env('REDIS_CLIENT', 'predis'), 'cluster' => false, 'default'=>[ 'tcp://127.0.0.1:6379', 'tcp://127.0.0.1:6381', 'tcp://127.0.0.1:6382', ], ], ] sentinel 由于当前环境没有安装 redis-sentinel，所以先安装 sudo apt-get install redis-sentinel 安装完成后，/etc/redis 目录下才会有 sentinel.conf 文件,打开文件并修改 port 26379 pidfile \"/var/run/redis-sentinel-26379.pid\" logfile \"/var/log/redis/redis-sentinel-26379.log\" #主节点别名为mymaster，后面是ip和端口，2代表判断主节点失败至少需要2个sentinel节点同意 sentinel monitor mymaster 127.0.0.1 6379 2 # 设置密码 # sentinel auth-pass mymaster 123456 #主节点故障30秒后启用新的主节点 sentinel down-after-milliseconds mymaster 30000 #故障转移时最多可以有1个从节点同时对主节点进行数据同步，数字越大，用时越短，存在网络和 IO 开销 sentinel parallel-syncs mymaster 1 #故障转移超时时间180s：a 如果转移超时失败，下次转移时时间为之前的2倍；b 从节点变主节点时，从节点执行 slaveof no one 命令一直失败的话，当时间超过180S时，则故障转移失败；c 从节点复制新主节点时间超过180S转移失败 sentinel failover-timeout mymaster 180000 然后复制两份 sentinel.conf vagrant@homestead:/etc/redis$ sudo cp sentinel.conf sentinel-26381.conf vagrant@homestead:/etc/redis$ sudo cp sentinel.conf sentinel-26382.conf 分别打开修改 pidfile \"/var/run/sentinel/redis-sentinel-26381.pid\" logfile \"/var/log/redis/redis-sentinel-26381.log\" port 26381 # myid 为当前 sentinel 的 id，随意设置一个就行，只要不重复，最开始我就是没修改这个，导致始终无法建立多个 sentinel sentinel myid 8f2969662f6bcd3553661608c213a3d637ab9987 上面配置完成后，启动 vagrant@homestead:/etc/redis$ sudo redis-sentinel sentinel.conf vagrant@homestead:/etc/redis$ sudo redis-sentinel sentinel-26381.conf vagrant@homestead:/etc/redis$ sudo redis-sentinel sentinel-26382.conf 当启动后，查看日志发现 /var/log/redis/redis-sentinel.log 29329:X 25 Sep 17:15:33.445 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 29329:X 25 Sep 17:15:33.445 # Redis version=4.0.9, bits=64, commit=00000000, modified=0, pid=29329, just started 29329:X 25 Sep 17:15:33.445 # Configuration loaded 29330:X 25 Sep 17:15:33.448 * Increased maximum number of open files to 10032 (it was originally set to 1024). # 启动模式为 sentinel， 端口为 26379 29330:X 25 Sep 17:15:33.450 * Running mode=sentinel, port=26379. # 它的 id 为 8f2969662f6bcd3553661608c213a3d637cd2331 29330:X 25 Sep 17:15:33.451 # Sentinel ID is 8f2969662f6bcd3553661608c213a3d637cd2331 # redis 集群主节点为 端口号 6379，当主节点发生故障时，需要至少两个 quorum 投票确定新主节点 29330:X 25 Sep 17:15:33.451 # +monitor master mymaster 127.0.0.1 6379 quorum 2 # 发现了两个新从节点 29330:X 25 Sep 17:15:33.455 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6379 29330:X 25 Sep 17:15:33.464 * +slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6379 # 发现了两个新 sentinel 29330:X 25 Sep 17:15:47.382 * +sentinel sentinel 8f2969662f6bcd3553661608c213a3d637ab9987 127.0.0.1 26381 @ mymaster 127.0.0.1 6382 29330:X 25 Sep 17:15:49.633 * +sentinel sentinel 8f2969662f6efg3553661608c213a3d637cd0000 127.0.0.1 26382 @ mymaster 127.0.0.1 6382 这个时候再打开 sentinel.conf 会发现多了几行，表示已知的从节点和 sentinel。 sentinel known-slave mymaster 127.0.0.1 6381 sentinel known-slave mymaster 127.0.0.1 6382 sentinel known-sentinel mymaster 127.0.0.1 26381 8f2969662f6bcd3553661608c213a3d637ab9987 sentinel known-sentinel mymaster 127.0.0.1 26382 8f2969662f6efg3553661608c213a3d637cd0000 sentinel current-epoch 0 当杀死 master 时，sentinel 将会从新选举新的 master 我们进入 6381 查看状态,6381 已经是 master了 并且拥有一个 6382 的slave 127.0.0.1:6381> info Replication # Replication role:master connected_slaves:1 slave0:ip=127.0.0.1,port=6382,state=online,offset=405135,lag=0 master_replid:2fb1ec7df33d87f7489d451e2a4b1ba379c16038 master_replid2:f2291a4914b906e8ce6b88203d755ed9b539ab0a master_repl_offset:405135 second_repl_offset:58660 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:4800 repl_backlog_histlen:400336 重新启动 6379 后,并且将 6379 设置为 6381 的 slave。 29330:X 25 Sep 17:32:11.190 * +slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381 29330:X 25 Sep 17:32:41.251 # +sdown slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381 29330:X 25 Sep 17:32:41.500 # -sdown slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381 sentinel 在 laravel 中使用 'sentinel', 'service' => 'mymaster']; $redis = new \\Predis\\Client($sentinels, $options); $redis->set('name','jack'); dd($redis->info()); sentinel 几个常用命令 SENTINEL masters #查看主节点信息 SENTINEL sentinels # 查看其他 sentinel 信息 SENTINEL slaves #查看对应集群的从节点信息 SENTINEL failover #进行故障转移 SENTINEL get-master-addr-by-name #查看当前的主节点地址 参考： Redis哨兵模式（sentinel）学习总结及部署记录（主从复制、读写分离、主从切换） - 散尽浮华 - 博客园 Redis 哨兵使用以及在 Laravel 中的配置 | Laravel China 社区 Redis Sentinel服务配置 - 漫天雪_昆仑巅 - CSDN博客 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/13.Redis 集群-Cluster.html":{"url":"Redis/13.Redis 集群-Cluster.html","title":"13.Redis 集群-Cluster","keywords":"","body":"13.Redis 集群-Cluster 介绍 Cluster 是官方出的 Redis 集群化解决方案。 它相对于 Codis 不同，它是去中心化的。如图所示，该集群分为三个节点，三个节点负责存储一部分数据，每个节点数据可能各不相同，三个节点互相连接构成一个对等的集群。他们之间通过特殊的二进制协议相互交互集群信息。 Redis Cluster 将所有数据划分为 16384 的 slots（槽），它比 Codis 的 1024 个槽分的更加精细。每个节点负责其中一部分槽位。槽位信息存储在每个节点中，它不像 Codis 需要另外的分布式存储来存储节点槽位信息。 当 Redis Cluster 客户端连接集群时，它会得到一部分集群的槽位配置信息，这样当查询某个 key 时，就可以直接定位目标节点。 Codis 需要通过 proxy 来定位节点。而 Redis Cluster 是直接定位。客户端为了能直接定位某个具体 key 所在的节点，就在客户端缓存了槽位相关信息。同时又因为客户端槽位信息可能和服务器不一致，所以还有纠正机制。 Redis Cluster 的每个节点会将集群信息持久化到配置文件中，所以必须保证配置文件可写，配置文件尽量不依靠人为修改。 比较 节点主从 优点：读写分离，增加 salve 可以增加并发读能力。 缺点：master 写能力有瓶颈。slave 虽然没瓶颈但是有维护成本。 hash slot 优点：写操作分散到多个节点，提高写的并发。扩容简单。 缺点：每个节点相互监听，高并发读写，任务繁重。当一个节点挂掉，影响系统稳定。 cluster 使用 hash 分逻辑节点，每个在节点内部进行主从部署。这样保证读写分离，又可扩展，也不怕节点挂掉，即高可用。 槽位算法 Cluster 会将 key 值使用 crc16 算法进行 hash 得到一个整数值，然后对这个值进行 16384 取模来得到槽位。 Cluster 还允许用户强制某个 key 挂在特定槽位上，通过在 key 上嵌入 tag 标记，就可以强制 key 所挂的槽位等于 tag 所在的槽位。 跳转 当客户端向一个错误的节点发出指令，该节点发现指令的 key 不属于它管理，这时它会向客户端发送一个特殊的跳转指令，告诉客户端去连接这个节点。 GET x -MOVED 3999 127.0.0.1:6381 MOVED 后面的 3999 表示对应的 key 的槽位编号，后面的地址表示目标节点地址。MOVED 前面有个减号，表示这时一条错误信息。 客户端收到 MOVED 指令后，会立即纠正本地槽位映射表，后续 key 都将使用新映射表。 迁移 Redis Cluster 提供了工具 redis-trib 让维护人员可以手动调整槽位的分配情况。 Redis 迁移的单位是槽，Redis 一个槽一个槽的迁移。当一个槽正在迁移时，这个槽就出于中间过渡状态。此时这个槽在原节点状态为 「migrating」，在目标节点的状态是 「importing」，表示数据正在流向目标。 迁移工具 redis-trib 步骤： 首先会在原节点和目标节点设置好中间状态 然后一次性获取原节点槽位的所有 key 列表（也可以通过 keysinslot 部分获取），再挨个进行迁移。 每个迁移过程是原节点做为目标节点的「客户端」，原节点对 key 执行 dump 命令得到序列化内容 然后原节点向目标节点发送 restore 携带序列化的内容作为参数，目标节点反序列化回复到目标节点中 最后标节点对原节点「客户端」返回 OK，原节点再删除 key。 这就是整个迁移过程。 从源节点获取内容 => 存到目标节点 => 从源节点删除内容。 注意： 迁移过程是同步的，知道删除原 key 之前，原节点是阻塞的。 如果迁移过程中发生网络故障，这时两个节点依旧处于中间状态，待下次迁移工具重新连接上时，会提示用户继续进行迁移。 迁移过程中，key 一般很小，所以迁移很快，不会影响到客户端的正常访问。当 key 很大时，迁移指令会阻塞原节点和目标节点，影响集群的稳定性。所以在日常中要避免较大 key 的产生。 迁移过程中，客户端访问流程会发生变化，客户端先尝试连接原节点，如果原节点有数据，就直接取出。如果原节点的数据已经消失，则存在两种可能，要么在新节点，要么根本不存在。旧节点不知道是哪种情况，所以它会向客户端返回一个-ASK targetNodeAddr的重定向指令。客户端收到这个重定向指令后，先去目标节点执行一个不带任何参数的asking指令，然后在目标节点再重新执行原先的操作指令。因为在迁移没有完成之前，按理说这个槽位还是不归新节点管理的，如果这个时候向目标节点发送该槽位的指令，节点是不认的，它会向客户端返回一个-MOVED重定向指令告诉它去源节点去执行。如此就会形成 重定向循环。asking指令的目标就是打开目标节点的选项，告诉它下一条指令不能不理，而要当成自己的槽位来处理。 容错 Redis Cluster 可以为每个主节点设置若干个从节点，当主节点出现故障时，集群会自动的将某个从节点提升为主节点。 如果主节点没有从节点，那么当发生故障时，集群将不可用。可以通过 cluster-require-full-coverage 参数来允许部分节点故障，其他节点正常访问。 网络抖动 网络抖动，即突然部分连接不能正常访问了，但是很快有恢复正常。 为了解决这个问题，Redis Cluster 提供了一个参数 cluster-node-timeout。表示某个节点持续 timeout 的时间后，才可认定为故障，需要进行主从切换。 如果没有这个选项，那么由于网络抖动，集群会频繁切换主从。 可能下线 (PFAIL-Possibly Fail) 与确定下线 (Fail) 由于 Cluster 是去中心化的，一个节点认为某个节点已经失联了并不代表所有的节点都认为它失联了。所以集群还要进行协商，当大多数节点认为某个节点失联，才认为该节点需要主从切换来容错。 Redis 集群通过 Gossip 协议来广播自己的状态以及自己对整个集群认知的改变。比如当一个节点认为某个节点已经失联（PFail），它就会向集群广播，其他节点也收到了这条失联消息。当集群中节点认为某个节点的失联的数量（PFail count）达到一定值时，就会标记这个节点失联（Fail），强迫其他节点接受这个事实。并立即对这个节点进行主从切换 安装及使用 基本配置 Redis Cluster 需要 Redis 版本在 3.0 以上。 首先配置全新的（Redis）6 台。(指 dir \"/var/lib/redis\" 这个目录下，没有 dump.rdb 和 conde-XXXX.conf) 修改其中一台 redis-cluster-6379.conf port 6379 pidfile \"/var/run/redis/redis-server-6379.pid\" logfile \"/var/log/redis/redis-server-6379.log\" dbfilename \"dump-6379.rdb\" # 开启Cluster cluster-enabled yes # 集群配置文件 cluster-config-file nodes-6379.conf # 集群超时时间 cluster-node-timeout 15000 复制 6 份。 sudo cp redis-cluster-6379.conf redis-cluster-6381.conf sudo cp redis-cluster-6379.conf redis-cluster-6382.conf sudo cp redis-cluster-6379.conf redis-cluster-6383.conf sudo cp redis-cluster-6379.conf redis-cluster-6384.conf sudo cp redis-cluster-6379.conf redis-cluster-6385.conf sudo cp redis-cluster-6379.conf redis-cluster-6386.conf 将其中的相关端口修改，然后启动。 sudo redis-server redis-cluster-6381.conf sudo redis-server redis-cluster-6382.conf sudo redis-server redis-cluster-6383.conf sudo redis-server redis-cluster-6384.conf sudo redis-server redis-cluster-6385.conf sudo redis-server redis-cluster-6386.conf 然后启动集群，这里有两种方法。 redis-cli 启动 这个方法需要 redis-server 5.0 以上版本，不用安装其他插件。 replicas 后面的参数 1 ，代表 redis 集群主节点和从节点的比值，如主节点 3 个，从节点 3 个，比值就是 1；如主节点 3 个，从节点 6 个，就是 0.5。 vagrant@homestead:/etc/redis$ redis-cli --cluster create 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 127.0.0.1:6385 127.0.0.1:6386 --cluster-replicas 1 >>> Performing hash slots allocation on 6 nodes... Master[0] -> Slots 0 - 5460 Master[1] -> Slots 5461 - 10922 Master[2] -> Slots 10923 - 16383 Adding replica 127.0.0.1:6385 to 127.0.0.1:6381 Adding replica 127.0.0.1:6386 to 127.0.0.1:6382 Adding replica 127.0.0.1:6384 to 127.0.0.1:6383 >>> Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: 4f3f01eb51c84b5ef3e81c0fbf55e46c2aa4125d 127.0.0.1:6381 slots:[0-5460] (5461 slots) master M: 6dce3bd147bab5fdf7cebe5cf65420b695da8e4a 127.0.0.1:6382 slots:[5461-10922] (5462 slots) master M: 5c473eba795c085d5298482adbfc09ad55dfaa38 127.0.0.1:6383 slots:[10923-16383] (5461 slots) master S: 7d8b215baa6d7d073381acaa2d69a43644206fed 127.0.0.1:6384 replicates 6dce3bd147bab5fdf7cebe5cf65420b695da8e4a S: 77c51a1c66af1f19f1c439c833e8fcafa80afc8a 127.0.0.1:6385 replicates 5c473eba795c085d5298482adbfc09ad55dfaa38 S: 411c963a3537c6efdb302f91184ed16f34418c4e 127.0.0.1:6386 replicates 4f3f01eb51c84b5ef3e81c0fbf55e46c2aa4125d Can I set the above configuration? (type 'yes' to accept): yes >>> Nodes configuration updated >>> Assign a different config epoch to each node >>> Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join ...... >>> Performing Cluster Check (using node 127.0.0.1:6381) M: 4f3f01eb51c84b5ef3e81c0fbf55e46c2aa4125d 127.0.0.1:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: 7d8b215baa6d7d073381acaa2d69a43644206fed 127.0.0.1:6384 slots: (0 slots) slave replicates 6dce3bd147bab5fdf7cebe5cf65420b695da8e4a S: 411c963a3537c6efdb302f91184ed16f34418c4e 127.0.0.1:6386 slots: (0 slots) slave replicates 4f3f01eb51c84b5ef3e81c0fbf55e46c2aa4125d M: 6dce3bd147bab5fdf7cebe5cf65420b695da8e4a 127.0.0.1:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: 77c51a1c66af1f19f1c439c833e8fcafa80afc8a 127.0.0.1:6385 slots: (0 slots) slave replicates 5c473eba795c085d5298482adbfc09ad55dfaa38 M: 5c473eba795c085d5298482adbfc09ad55dfaa38 127.0.0.1:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. redis-trib 启动 需要安装 ruby 环境 sudo apt-get install ruby sudo apt-get install gems sudo gem install redis redis-trib.rb 目录在 /usr/local/redis/src/redis-trib.rb。执行文件后发现，推荐我们使用 redis-cli 来启动，可能是由于我更新了 redis5.0 的缘故。 vagrant@homestead:/etc/redis$ /usr/local/redis/src/redis-trib.rb create --replicas 1 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 127.0.0.1:6385 127.0.0.1:6386 WARNING: redis-trib.rb is not longer available! You should use redis-cli instead. All commands and features belonging to redis-trib.rb have been moved to redis-cli. In order to use them you should call redis-cli with the --cluster option followed by the subcommand name, arguments and options. Use the following syntax: redis-cli --cluster SUBCOMMAND [ARGUMENTS] [OPTIONS] Example: redis-cli --cluster create 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384 127.0.0.1:6385 127.0.0.1:6386 --cluster-replicas 1 To get help about all subcommands, type: redis-cli --cluster help 测试 使用 redis-cli 连接 6385，-c 表示使用集群模式。 vagrant@homestead:/etc/redis$ redis-cli -c -p 6382 127.0.0.1:6382> set demo hello-world -> Redirected to slot [903] located at 127.0.0.1:6381 OK 127.0.0.1:6381> get demo \"hello-world\" 通过 cluster info 指令，可以看到有 6 个节点，16384 个槽位。 127.0.0.1:6381> CLUSTER INFO cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:1 cluster_stats_messages_ping_sent:978 cluster_stats_messages_pong_sent:959 cluster_stats_messages_sent:1937 cluster_stats_messages_ping_received:954 cluster_stats_messages_pong_received:978 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:1937 通过 cluster nodes 查看节点，可以看到 6381 6382 6383 三个是主节点，其他为从节点。 127.0.0.1:6381> CLUSTER NODES 5096a076abd4b99d5c07e489109c83be929a4228 127.0.0.1:6382@16382 master - 0 1569570388000 2 connected 5461-10922 a2d9574fba2437b416aecf1d80bc9b422a8cb678 127.0.0.1:6385@16385 slave 5096a076abd4b99d5c07e489109c83be929a4228 0 1569570390638 5 connected c2ddacc6944d9ffd05ba7286a06e01ec46d76b04 127.0.0.1:6384@16384 slave b17e98228b179ea3b572f6106d2fed6ab75b07f0 0 1569570389000 4 connected b0a9bce8cb1225dbb7d3a0bef210c3deae55ffc2 127.0.0.1:6383@16383 master - 0 1569570389626 3 connected 10923-16383 b17e98228b179ea3b572f6106d2fed6ab75b07f0 127.0.0.1:6381@16381 myself,master - 0 1569570385000 1 connected 0-5460 147622225bd356f70331e6c67f30e9454de77d5a 127.0.0.1:6386@16386 slave b0a9bce8cb1225dbb7d3a0bef210c3deae55ffc2 0 1569570390000 6 connected 接下来我们使 6381 失效。 vagrant@homestead:/etc/redis$ redis-cli -p 6381 debug segfault Error: Server closed the connection 稍等片刻，重新连接 6381，并且再次查看集群节点。 vagrant@homestead:/etc/redis$ sudo redis-server redis-cluster-6381.conf vagrant@homestead:/etc/redis$ redis-cli -c -p 6382 127.0.0.1:6382> cluster nodes 147622225bd356f70331e6c67f30e9454de77d5a 127.0.0.1:6386@16386 slave b0a9bce8cb1225dbb7d3a0bef210c3deae55ffc2 0 1569570753000 6 connected 5096a076abd4b99d5c07e489109c83be929a4228 127.0.0.1:6382@16382 myself,master - 0 1569570754000 2 connected 5461-10922 b17e98228b179ea3b572f6106d2fed6ab75b07f0 127.0.0.1:6381@16381 slave c2ddacc6944d9ffd05ba7286a06e01ec46d76b04 0 1569570755984 7 connected c2ddacc6944d9ffd05ba7286a06e01ec46d76b04 127.0.0.1:6384@16384 master - 0 1569570754941 7 connected 0-5460 b0a9bce8cb1225dbb7d3a0bef210c3deae55ffc2 127.0.0.1:6383@16383 master - 0 1569570753850 3 connected 10923-16383 a2d9574fba2437b416aecf1d80bc9b422a8cb678 127.0.0.1:6385@16385 slave 5096a076abd4b99d5c07e489109c83be929a4228 0 1569570753000 5 connected 我们可以发现，6381 变成了从节点，6384 变成了主节点。 扩容 新复制两份配置文件，并修改相关值，然后启动 sudo cp redis-cluster-6379.conf redis-cluster-6387.conf sudo cp redis-cluster-6379.conf redis-cluster-6388.conf sudo redis-server redis-cluster-6387.conf sudo redis-server redis-cluster-6388.conf 添加到集群中 redis-cli --cluster add-node 127.0.0.1:6387 127.0.0.1:6381 redis-cli --cluster add-node 127.0.0.1:6388 127.0.0.1:6381、 vagrant@homestead:/etc/redis$ redis-cli --cluster add-node 127.0.0.1:6387 127.0.0.1:6381 >>> Adding node 127.0.0.1:6387 to cluster 127.0.0.1:6381 >>> Performing Cluster Check (using node 127.0.0.1:6381) M: b1710ed597bbc38e66105b429aced4fbfc04281a 127.0.0.1:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) M: 0a68b1d915956c322119843b1ed848bc95096b0f 127.0.0.1:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: 09265af88d3d715803c2d018e97fbe7e0b56c847 127.0.0.1:6386 slots: (0 slots) slave replicates 45cc61a214983f387e1159a9b6a918be6fe36b90 M: 45cc61a214983f387e1159a9b6a918be6fe36b90 127.0.0.1:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: 61a48d614c0216765a89b8f7ce2ec999255be333 127.0.0.1:6385 slots: (0 slots) slave replicates b1710ed597bbc38e66105b429aced4fbfc04281a S: 10d6657b18b4ae6c01ce1fc87b5735e9725dccba 127.0.0.1:6384 slots: (0 slots) slave replicates 0a68b1d915956c322119843b1ed848bc95096b0f [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. >>> Send CLUSTER MEET to node 127.0.0.1:6387 to make it join the cluster. [OK] New node added correctly. 此时添加进的两个节点都是 master 节点。 vagrant@homestead:/etc/redis$ redis-cli -c -p 6381 127.0.0.1:6381> cluster nodes 0a68b1d915956c322119843b1ed848bc95096b0f 127.0.0.1:6383@16383 master - 0 1569576631297 3 connected 10923-16383 c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 127.0.0.1:6387@16387 master - 0 1569576627000 7 connected 09265af88d3d715803c2d018e97fbe7e0b56c847 127.0.0.1:6386@16386 slave 45cc61a214983f387e1159a9b6a918be6fe36b90 0 1569576630287 6 connected b1710ed597bbc38e66105b429aced4fbfc04281a 127.0.0.1:6381@16381 myself,master - 0 1569576629000 1 connected 0-5460 e72718e00fb36c63cd7e04a6e266897c698dddea 127.0.0.1:6388@16388 master - 0 1569576628000 0 connected 45cc61a214983f387e1159a9b6a918be6fe36b90 127.0.0.1:6382@16382 master - 0 1569576627000 2 connected 5461-10922 61a48d614c0216765a89b8f7ce2ec999255be333 127.0.0.1:6385@16385 slave b1710ed597bbc38e66105b429aced4fbfc04281a 0 1569576628092 5 connected 10d6657b18b4ae6c01ce1fc87b5735e9725dccba 127.0.0.1:6384@16384 slave 0a68b1d915956c322119843b1ed848bc95096b0f 0 1569576629176 4 connected 然后我们将 6388 作为 6387 的从节点。首先连接到 6388 节点，指定 6387 为从节点。通过 cluster replicate [id] 这个指令完成。 vagrant@homestead:/etc/redis$ redis-cli -c -p 6388 127.0.0.1:6388> cluster replicate c04144606c0b6c12aaa92f8022e4a6dbe2109ffd OK 127.0.0.1:6388> cluster nodes 61a48d614c0216765a89b8f7ce2ec999255be333 127.0.0.1:6385@16385 slave b1710ed597bbc38e66105b429aced4fbfc04281a 0 1569576756973 1 connected e72718e00fb36c63cd7e04a6e266897c698dddea 127.0.0.1:6388@16388 myself,slave c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 0 1569576754000 0 connected 10d6657b18b4ae6c01ce1fc87b5735e9725dccba 127.0.0.1:6384@16384 slave 0a68b1d915956c322119843b1ed848bc95096b0f 0 1569576757984 3 connected 45cc61a214983f387e1159a9b6a918be6fe36b90 127.0.0.1:6382@16382 master - 0 1569576758000 2 connected 5461-10922 c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 127.0.0.1:6387@16387 master - 0 1569576757000 7 connected 0a68b1d915956c322119843b1ed848bc95096b0f 127.0.0.1:6383@16383 master - 0 1569576753000 3 connected 10923-16383 b1710ed597bbc38e66105b429aced4fbfc04281a 127.0.0.1:6381@16381 master - 0 1569576757000 1 connected 0-5460 09265af88d3d715803c2d018e97fbe7e0b56c847 127.0.0.1:6386@16386 slave 45cc61a214983f387e1159a9b6a918be6fe36b90 0 1569576758992 2 connected 然后进行自动槽位迁移 vagrant@homestead:/etc/redis$ redis-cli --cluster reshard 127.0.0.1:6381 >>> Performing Cluster Check (using node 127.0.0.1:6381) ... ... # 省略 节点 id 及槽位信息 # 需要转移多少个槽位 How many slots do you want to move (from 1 to 16384)? 4096 # 需要转移到哪个节点 What is the receiving node ID? c04144606c0b6c12aaa92f8022e4a6dbe2109ffd # 从哪几个节点进行转移 Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node #1: b1710ed597bbc38e66105b429aced4fbfc04281a Source node #2: 45cc61a214983f387e1159a9b6a918be6fe36b90 Source node #3: 0a68b1d915956c322119843b1ed848bc95096b0f Source node #4: done # 最后会有一个迁移方案，输入yes表示同意，迁移开始。输入no表示不同意，重新设置迁移方案。 通过 redis-cli --cluster rebalance 检测节点间槽的均衡性 vagrant@homestead:/etc/redis$ redis-cli --cluster rebalance 127.0.0.1:6381 >>> Performing Cluster Check (using node 127.0.0.1:6381) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. *** No rebalancing needed! All nodes are within the 2.00% threshold. 可以看出，节点负责的槽数据差异在2%以内，因此槽分配均衡。 节点失效 先杀死 6381，连接 6382 看集群状况 vagrant@homestead:/etc/redis$ redis-cli -c -p 6382 127.0.0.1:6382> cluster nodes c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 127.0.0.1:6387@16387 master - 0 1569647721351 7 connected 0-1364 5461-6826 10923-12287 0a68b1d915956c322119843b1ed848bc95096b0f 127.0.0.1:6383@16383 master - 0 1569647721000 3 connected 12288-16383 45cc61a214983f387e1159a9b6a918be6fe36b90 127.0.0.1:6382@16382 myself,master - 0 1569647718000 2 connected 6827-10922 10d6657b18b4ae6c01ce1fc87b5735e9725dccba 127.0.0.1:6384@16384 slave 0a68b1d915956c322119843b1ed848bc95096b0f 0 1569647722363 4 connected e72718e00fb36c63cd7e04a6e266897c698dddea 127.0.0.1:6388@16388 slave c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 0 1569647720000 7 connected b1710ed597bbc38e66105b429aced4fbfc04281a 127.0.0.1:6381@16381 master - 1569647712751 1569647710000 1 disconnected 1365-5460 09265af88d3d715803c2d018e97fbe7e0b56c847 127.0.0.1:6386@16386 slave 45cc61a214983f387e1159a9b6a918be6fe36b90 0 1569647720000 2 connected 61a48d614c0216765a89b8f7ce2ec999255be333 127.0.0.1:6385@16385 slave b1710ed597bbc38e66105b429aced4fbfc04281a 0 1569647721000 5 connected 127.0.0.1:6382> cluster nodes c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 127.0.0.1:6387@16387 master - 0 1569647735000 7 connected 0-1364 5461-6826 10923-12287 0a68b1d915956c322119843b1ed848bc95096b0f 127.0.0.1:6383@16383 master - 0 1569647734501 3 connected 12288-16383 45cc61a214983f387e1159a9b6a918be6fe36b90 127.0.0.1:6382@16382 myself,master - 0 1569647732000 2 connected 6827-10922 10d6657b18b4ae6c01ce1fc87b5735e9725dccba 127.0.0.1:6384@16384 slave 0a68b1d915956c322119843b1ed848bc95096b0f 0 1569647733494 4 connected e72718e00fb36c63cd7e04a6e266897c698dddea 127.0.0.1:6388@16388 slave c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 0 1569647733000 7 connected b1710ed597bbc38e66105b429aced4fbfc04281a 127.0.0.1:6381@16381 master,fail - 1569647712751 1569647710000 1 disconnected 09265af88d3d715803c2d018e97fbe7e0b56c847 127.0.0.1:6386@16386 slave 45cc61a214983f387e1159a9b6a918be6fe36b90 0 1569647734000 2 connected 61a48d614c0216765a89b8f7ce2ec999255be333 127.0.0.1:6385@16385 master - 0 1569647735513 8 connected 1365-5460 会发现 6381 先是 disconnected 表示节点发现 6381 可能挂了，通知其他节点也去看下,当大部分节点都觉得 6381 挂了之后，6381 变成了 fail ，表明确定下线了。然后启用了 6381 的从节点 6385 作为 master。 当我们再启动 6381。发现，6381 已经变成了 slave。 127.0.0.1:6382> cluster nodes c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 127.0.0.1:6387@16387 master - 0 1569648018000 7 connected 0-1364 5461-6826 10923-12287 0a68b1d915956c322119843b1ed848bc95096b0f 127.0.0.1:6383@16383 master - 0 1569648018000 3 connected 12288-16383 45cc61a214983f387e1159a9b6a918be6fe36b90 127.0.0.1:6382@16382 myself,master - 0 1569648017000 2 connected 6827-10922 10d6657b18b4ae6c01ce1fc87b5735e9725dccba 127.0.0.1:6384@16384 slave 0a68b1d915956c322119843b1ed848bc95096b0f 0 1569648017000 4 connected e72718e00fb36c63cd7e04a6e266897c698dddea 127.0.0.1:6388@16388 slave c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 0 1569648018000 7 connected b1710ed597bbc38e66105b429aced4fbfc04281a 127.0.0.1:6381@16381 slave 61a48d614c0216765a89b8f7ce2ec999255be333 0 1569648019000 8 connected 09265af88d3d715803c2d018e97fbe7e0b56c847 127.0.0.1:6386@16386 slave 45cc61a214983f387e1159a9b6a918be6fe36b90 0 1569648017000 2 connected 61a48d614c0216765a89b8f7ce2ec999255be333 127.0.0.1:6385@16385 master - 0 1569648019862 8 connected 1365-5460 # 删除节点 在 6382 中创建几条数据后，对节点进行删除操作。由于 6382 是 master，会告诉你 6382 不为空，请先转移槽在重试。 vagrant@homestead:/etc/redis$ redis-cli --cluster del-node 127.0.0.1:6382 45cc61a214983f387e1159a9b6a918be6fe36b90 >>> Removing node 45cc61a214983f387e1159a9b6a918be6fe36b90 from cluster 127.0.0.1:6382 [ERR] Node 127.0.0.1:6382 is not empty! Reshard data away and try again. 转移槽，从 6382 转到 6382 从节点 6386 上，发现不能转移，只能转移给 master 节点 vagrant@homestead:/etc/redis$ redis-cli --cluster reshard 127.0.0.1:6382 --cluster-from 45cc61a214983f387e1159a9b6a918be6fe36b90 --cluster-to 09265af88d3d715803c2d018e97fbe7e0b56c847 --cluster-slots 4096 --cluster-yes >>> Performing Cluster Check (using node 127.0.0.1:6382) M: 45cc61a214983f387e1159a9b6a918be6fe36b90 127.0.0.1:6382 slots:[6827-10922] (4096 slots) master 1 additional replica(s) M: c04144606c0b6c12aaa92f8022e4a6dbe2109ffd 127.0.0.1:6387 slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master 1 additional replica(s) M: 0a68b1d915956c322119843b1ed848bc95096b0f 127.0.0.1:6383 slots:[12288-16383] (4096 slots) master 1 additional replica(s) S: 10d6657b18b4ae6c01ce1fc87b5735e9725dccba 127.0.0.1:6384 slots: (0 slots) slave replicates 0a68b1d915956c322119843b1ed848bc95096b0f S: e72718e00fb36c63cd7e04a6e266897c698dddea 127.0.0.1:6388 slots: (0 slots) slave replicates c04144606c0b6c12aaa92f8022e4a6dbe2109ffd S: b1710ed597bbc38e66105b429aced4fbfc04281a 127.0.0.1:6381 slots: (0 slots) slave replicates 61a48d614c0216765a89b8f7ce2ec999255be333 S: 09265af88d3d715803c2d018e97fbe7e0b56c847 127.0.0.1:6386 slots: (0 slots) slave replicates 45cc61a214983f387e1159a9b6a918be6fe36b90 M: 61a48d614c0216765a89b8f7ce2ec999255be333 127.0.0.1:6385 slots:[1365-5460] (4096 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. *** The specified node (09265af88d3d715803c2d018e97fbe7e0b56c847) is not known or not a master, please retry. 将槽位转换到 6387 后，删除 6382,则成功。 vagrant@homestead:/etc/redis$ redis-cli --cluster del-node 127.0.0.1:6382 45cc61a214983f387e1159a9b6a918be6fe36b90 >>> Removing node 45cc61a214983f387e1159a9b6a918be6fe36b90 from cluster 127.0.0.1:6382 >>> Sending CLUSTER FORGET messages to the cluster... >>> SHUTDOWN the node. 然后使用 rebalance 命令平均下槽位。 vagrant@homestead:/etc/redis$ redis-cli --cluster rebalance 127.0.0.1:6383 >>> Performing Cluster Check (using node 127.0.0.1:6383) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. >>> Rebalancing across 3 nodes. Total weight = 3.00 ... ... ... vagrant@homestead:/etc/redis$ redis-cli --cluster info 127.0.0.1:6383 127.0.0.1:6383 (0a68b1d9...) -> 3 keys | 5462 slots | 1 slaves. 127.0.0.1:6385 (61a48d61...) -> 2 keys | 5461 slots | 1 slaves. 127.0.0.1:6387 (c0414460...) -> 3 keys | 5461 slots | 2 slaves. [OK] 8 keys in 3 masters. 0.00 keys per slot on average. vagrant@homestead:/etc/redis$ 可以发现 6387 有两个 slave，可以推断出 之前 6382 的 salve 转移到了 6387 上。 在 laravel 上进行测试 首先修改 database.php 配置文件，改为 cluster 模式。 [ 'client' => 'predis', 'cluster' => true, 'options' => [ 'cluster' => 'redis' ], 'default' => [ 'host' => env('REDIS_HOST_FIRST', '127.0.0.1'), 'password' => env('REDIS_PASSWORD', null), 'port' => env('REDIS_PORT', 6379), 'database' => 1, 'read_timeout' => env('REDIS_TIMEOUT', 5), ], 'clusters' => [ 'mycluster1' => [ [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6381, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6382, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6383, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6384, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6385, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6386, 'database' => 0, ], ], ], ], ]; 使用 set('name','jack'); $redis->set('sex','女'); $redis->set('age','18'); $redis->set('class','一班'); dump($redis->get('sex')); dump($redis->get('age')); dump($redis->get('class')); dd($redis->get('name')); 参考 redis实战第九篇 集群扩容自动迁移槽（redis-cli 三张图秒懂Redis集群设计原理 全面剖析Redis Cluster原理和应用 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/14.集群-Codis.html":{"url":"Redis/14.集群-Codis.html","title":"14.集群-Codis","keywords":"","body":"14.集群-Codis 简介 在大数据高并发场景下，单个 redis 实例性能往往捉襟见肘。 首先单个 redis 实例内存往往过大，导致 rdb 文件过大，会导致主从同步时，增量同步时间过长，实例重启恢复时间也更长。 其次 CPU 利用率上，单个 redis 实例只能利用单个 cpu 核心。单个核心处理海量数据和任务压力会非常大。 Codis 是由国人开发，与 redis-cluster 一样，去中心化。不过槽位没有 redis-cluster 分的精细。 Codis 是由 Go 语言开发。它是一个代理中间件 proxy。它和 redis 一样使用 redis 协议对外提供服务。当客户端向 Codis 发送一条指令时，Codis 负责将指令转发到后面的 redis 实例来执行，并将返回的结果转发回客户端。 客户端操纵 Codis 同 redis 几乎没有任何区别。 因为 Codis 是无状态的，它只是一个转发代理中间件，这表示可以启动多个 Codis 实例，每个实例都是对等的。多个实例可以增加整体 QPS。还能起到容灾效果，当一个 Codis 实例挂掉时，还有很多实例可以继续提供服务。 Codis 分片原理 Codis 将所有 key 划分到 1024 个槽位（slot）上，它首先将客户端传过来的 key 进行 crc32 运算出 hash 值，然后用得到的值对 1024 进行取模，得到的余数为槽位。 每个槽位后面对应多个 Redis 实例，Codis 会负责维护槽位和 Codis 之间的映射关系。 不同 Codis 实例间槽位关系如何同步 Codis 需要一个分布式配置存储数据库来专门持久化槽位关系。支持 zookeeper 和 etcd。 Codis 将槽位关系存储在 zookeeper 中，并提供了 Dashboard 用来观察和修改槽位关系。当槽位关系变化时，codis proxy 会监听到变化并同步槽位关系，从而时间多个 codis 实例共享槽位关系。 扩容 最开始 codis 只有一个 redis，所有的槽位都这个 redis，当增加 redis 实例后，槽位关系需要调整。需要将一半的槽位分给新的实例。 通过使用 slotsscan 指令，遍历所有的 key，然后挨个取出 key 放到新实例中。 在迁移过程中，codis 无法判断 key 在哪个实例中。当 Codis 发现 key 在正在迁移的槽中时，会强制先对当前 key 进行迁移，迁移完成后，再次请求新的实例。 scan 无法避免重复，但是并不影响迁移。因为单个 key 被迁移后，在旧实例中就被删除了，从而无法再被扫描出来。 自动均衡 Redis 新增实例，手动调整 slot 太繁琐。所以 codis 提供了自动均衡的功能，当系统比较空闲时，会观察每个 redis 实例的 slot 数量，如过不均衡，将启动自动均衡功能。 Codis 的代价 Codis 的 key 分散在不同实例中，所以不支持事务。 还有一些不支持的命令列表。 为了扩容，Codis 的单个 key 不能过大，否则迁移会卡顿。 Codis 集群配置中心使用 zk 实现，增加了 zk 运维的代价。 mget 指令的操作过程 mget 指令用于批量获取 key 的值，这些 key 可能分散到多个 key 中。Codis 的策略是，分别从每个实例中调用 mget 方法，然后再汇总返回给客户端。 Codis 的尴尬 Codis 不是官方项目，所以每次 Redis 官方更新后，Codis 都要看看自己少了什么。 Codis 总是比 官方慢一拍。 安装 # 安装 Go $ sudo apt-get install golang # 检测版本 $ go version go version go1.10.2 linux/amd64 # 获取 go 安装地址 $ go env GOPATH /home/vagrant/go # 下载 codis 源码 $ mkdir -p $GOPATH/src/github.com/CodisLabs $ cd $_ && git clone https://github.com/CodisLabs/codis.git -b release3.2 # 编译安装 $ cd $GOPATH/src/github.com/CodisLabs/codis $ make # 快速启动 $ ./admin/codis-dashboard-admin.sh start $ ./admin/codis-proxy-admin.sh start $ ./admin/codis-fe-admin.sh start 配置 codis 的 redis 实例，在 ./config 下创建 redis-codis-6481.conf，修改以下这些配置，还有一些多余的，通过命令行提示注释。 protected-mode no port 6483 pidfile \"/var/run/codis/redis-server-6483.pid\" logfile \"/var/log/codis/redis-server-6483.log\" dbfilename \"dump-6483.rdb\" dir \"/var/lib/codis\" 然后启动 codis 的 redis $ sudo ./bin/codis-server ./config/redis-codis-6481.conf 接下来就在 fe 进行操作。fe 地址:127.0.0.1:9090。Homestead 需要打开 9090 端口。 参考 官网 Codis的安装与使用 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/15.Stream.html":{"url":"Redis/15.Stream.html","title":"15.Stream","keywords":"","body":"15.Stream 简介 Redis 5.0 开始支持 Stream，它狠狠的借鉴了 Kafka 的设计。 Stream 的结构如图所示，它是一个消息链表。 每个消息都有一个消息 ID 与之对应。 消息持久化，重启后，消息仍在。 每个 Stream 都有一个名字，即 Redis 的 key。 每个 Stream 都可以挂多个消费组。 每个消费组上都会有一个 last_delivered_id 来记录消费到哪了。 每个消费组的状态都相互独立，互不影响。同一份 stream 可以被每个消费组都消费到。 每个消费组（Consumer Group）都可以挂多个消费者（Consumer），这些消费者是竞争关系。任何一个消费者消费 Stream 都会使 last_delivered_id 向前移动。每个消费者在一个组里都有唯一名称。 消费者（Consumer）内部有个变量 pending_ids ，它记录了当前已经被客户端读取的消息，但是还没有 ack（确认返回）。 只有 ack 后，pending_ids 才会开始减少。 这个 pending_ids 被 Redis 官方成为 PEL（Pending Entries List）。 它是被用来确保客户端至少消费了一次，而不是因为网络问题导致没处理。 消息 ID 消息 ID 的形式是 timestampInMillis-sequence，例如1527846880572-5，表示在 1527846880572 时产生的第 5 条消息。 消息 id 是系统自动生成。 也可以手动生成，但形式必须是 「整数-整数」，并且后面加入的消息 id 必须大于前面加入的消息 id。 消息内容 消息内容就是键值对，形如 hash 结构的键值对。 指令-增删改查 追加消息 - xadd，如果 id 为 * 则代表不指定 id。返回 id，格式：xadd [stream_name] [id] [key-value]... 127.0.0.1:6383> xadd demo * name jk age 27 \"1570511860225-0\" 127.0.0.1:6383> xadd demo * name jk4905 age 28 \"1570511869366-0\" 127.0.0.1:6383> xadd demo * name kagami age 29 \"1570511875999-0\" 消息长度 - xlen 127.0.0.1:6383> xlen demo (integer) 3 获取消息列表，自动过滤已删除的消息。 xrange [key] [start] [end] # - 为最小值，+ 为最大值 127.0.0.1:6383> xrange demo - + 1) 1) \"1570511860225-0\" 2) 1) \"name\" 2) \"jk\" 3) \"age\" 4) \"27\" 2) 1) \"1570511869366-0\" 2) 1) \"name\" 2) \"jk4905\" 3) \"age\" 4) \"28\" 3) 1) \"1570511875999-0\" 2) 1) \"name\" 2) \"kagami\" 3) \"age\" 4) \"29\" # 指定最小值 127.0.0.1:6383> xrange demo 1570511869366-0 + 1) 1) \"1570511869366-0\" 2) 1) \"name\" 2) \"jk4905\" 3) \"age\" 4) \"28\" 2) 1) \"1570511875999-0\" 2) 1) \"name\" 2) \"kagami\" 3) \"age\" 4) \"29\" # 指定最大值 127.0.0.1:6383> xrange demo - 1570511869366-0 1) 1) \"1570511860225-0\" 2) 1) \"name\" 2) \"jk\" 3) \"age\" 4) \"27\" 2) 1) \"1570511869366-0\" 2) 1) \"name\" 2) \"jk4905\" 3) \"age\" 4) \"28\" 删除消息，逻辑删除，设置删除标记位，不影响消息总长度 127.0.0.1:6383> xdel name 1570511869366-0 (integer) 1 # 长度不受影响 127.0.0.1:6383> xlen demo (integer) 3 # 被删除的消息没了 127.0.0.1:6383> xrange demo - + 1) 1) \"1570511860225-0\" 2) 1) \"name\" 2) \"jk\" 3) \"age\" 4) \"27\" 2) 1) \"1570511875999-0\" 2) 1) \"name\" 2) \"kagami\" 3) \"age\" 4) \"29\" 删除 Stream 127.0.0.1:6383> del name (integer) 1s 独立消费 当我们不定义消费组（Consutmer Group）时，进行 Stream 的独立消费。此时把 Stream 当做一个普通列表（list）。当 Stream 没有消息时，甚至可以阻塞。 读取 - xread xread [COUNT count] [BLOCK milliseconds] STREAMS [key] [id].. # 从 Stream 头部读取两条消息 127.0.0.1:6383> xread count 2 streams demo 0-0 1) 1) \"demo\" 2) 1) 1) \"1570512984508-0\" 2) 1) \"name\" 2) \"jk\" 3) \"age\" 4) \"27\" 2) 1) \"1570512989666-0\" 2) 1) \"name\" 2) \"jk4905\" 3) \"age\" 4) \"28\" # 从 Stream 尾部读取一条消息，毫无疑问，这里不会返回任何消息 127.0.0.1:6379> xread count 1 Streams demo $ (nil) # 从尾部阻塞等待新消息到来，下面的指令会堵住，直到新消息到来 127.0.0.1:6379> xread count 1 block 0 streams demo $ # 从尾部阻塞等待新消息到来，下面的指令会堵住，直到新消息到来 127.0.0.1:6379> xadd demo * name kagam1 age 30 \"1570514266119-0\" # 再切换到前面的窗口，我们可以看到阻塞解除了，返回了新的消息内容 # 而且还显示了一个等待时间，这里我们等待了 36s 127.0.0.1:6379> xread count 1 block 0 streams demo $ 1) 1) \"demo\" 2) 1) 1) \"1570514266119-0\" 2) 1) \"name\" 2) \"kagam1\" 3) \"age\" 4) \"30\" (36.18s) 注意：如果要使用 xread 消费，一定要记录消费到了哪个位置。 block 0 表示永远阻塞，直到消息到来。如果 block 1000 表示阻塞 1s，也就是说 1s 之内如果没有消息来就返回 nil。 127.0.0.1:6379> xread count 1 block 1000 streams demo $ (nil) (1.01s) 创建消费组 # 从开头开始消费 127.0.0.1:6379> xgroup create demo cg1 0-0 OK # 从尾部开始消费，只接受新消息，当前的 Stream 会全忽略 127.0.0.1:6379> xgroup create demo cg2 $ OK # 获取当前 stream 的信息 127.0.0.1:6379> xinfo stream demo 1) \"length\" 2) (integer) 4 # 共 4 个消息 3) \"radix-tree-keys\" 4) (integer) 1 5) \"radix-tree-nodes\" 6) (integer) 2 7) \"groups\" 8) (integer) 2 # 两个消费组 9) \"last-generated-id\" 10) \"1570514266119-0\" 11) \"first-entry\" # 第一个消息 12) 1) \"1570513760437-0\" 2) 1) \"name\" 2) \"jk\" 3) \"age\" 4) \"27\" 13) \"last-entry\" # 最后一个消息 14) 1) \"1570514266119-0\" 2) 1) \"name\" 2) \"kagam1\" 3) \"age\" 4) \"30\" # 查询当前消费组信息 127.0.0.1:6379> xinfo groups demo 1) 1) \"name\" 2) \"cg1\" 3) \"consumers\" 4) (integer) 0 # 该消费组还没有消费者 5) \"pending\" 6) (integer) 0 # 该消费组没有正在处理的消息 7) \"last-delivered-id\" 8) \"0-0\" 2) 1) \"name\" 2) \"cg2\" 3) \"consumers\" 4) (integer) 0 # 该消费组还没有消费者 5) \"pending\" 6) (integer) 0 # 该消费组没有正在处理的消息 7) \"last-delivered-id\" 8) \"1570514266119-0\" 消费 指令： xreadgroup GROUP [group_name] [consumer] [COUNT count] [BLOCK milliseconds] STREAMS [key] ID 基本用法与 「xread」 一样。也可以阻塞消息。当读到新消息后，会将对应的 id 加入到PEL（正在处理消息）结构里，然后客户端处理完后通过 xack 指令通知服务器，消息处理完毕，则消息 ID 会从 PEL 中删除。 # > 表示从当前 last_delivered_id 后面开始读取 # 每当消费者读取一条信息，last_delivered_id 就会往前移动 127.0.0.1:6379> xreadgroup group cg1 c1 count 1 streams demo > 1) 1) \"demo\" 2) 1) 1) \"1570513760437-0\" 2) 1) \"name\" 2) \"jk\" 3) \"age\" 4) \"27\" 127.0.0.1:6379> xreadgroup group cg1 c1 count 1 streams demo > 1) 1) \"demo\" 2) 1) 1) \"1570513764246-0\" 2) 1) \"name\" 2) \"jk4905\" 3) \"age\" 4) \"28\" 127.0.0.1:6379> xreadgroup group cg1 c1 count 1 streams demo > 1) 1) \"demo\" 2) 1) 1) \"1570513767145-0\" 2) 1) \"name\" 2) \"kagami\" 3) \"age\" 4) \"29\" 127.0.0.1:6379> xreadgroup group cg1 c1 count 1 streams demo > 1) 1) \"demo\" 2) 1) 1) \"1570514266119-0\" 2) 1) \"name\" 2) \"kagam1\" 3) \"age\" 4) \"30\" # 再继续读取，就没有新消息了 127.0.0.1:6379> xreadgroup group cg1 c1 count 1 streams demo > (nil) # 那就阻塞等待吧 127.0.0.1:6379> xreadgroup GROUP cg1 c1 block 0 count 1 streams demo > # 开启另一个窗口，往里塞消息 127.0.0.1:6379> xadd demo * name lanying age 61 1527854062442-0 # 回到前一个窗口，发现阻塞解除，收到新消息了 127.0.0.1:6379> xreadgroup GROUP cg1 c1 block 0 count 1 streams demo > 1) 1) \"codehole\" 2) 1) 1) 1527854062442-0 2) 1) \"name\" 2) \"lanying\" 3) \"age\" 4) \"61\" (36.54s) # 查看消费组的信息 127.0.0.1:6379> xinfo groups demo 1) 1) \"name\" 2) \"cg1\" 3) \"consumers\" 4) (integer) 1 # 一个消费者 5) \"pending\" 6) (integer) 4 # 共 4 条信息已经被读取，但是没返回 ack 7) \"last-delivered-id\" 8) \"1570514266119-0\" 2) 1) \"name\" 2) \"cg2\" 3) \"consumers\" 4) (integer) 0 # 消费组 cg2 没有任何变化，因为前面我们一直在操纵 cg1 5) \"pending\" 6) (integer) 0 7) \"last-delivered-id\" 8) \"1570514266119-0\" # 查看同一个消费组中的消费者信息 127.0.0.1:6379> xinfo consumers demo cg1 1) 1) \"name\" 2) \"c1\" 3) \"pending\" 4) (integer) 4 # 共 4 条信息待处理 5) \"idle\" 6) (integer) 289904 # 空闲了多长时间 ms 没有读取消息了 # ack 一条信息回去 127.0.0.1:6379> xack demo cg1 1570513760437-0 (integer) 1 # 再次查看消费者 127.0.0.1:6379> xinfo consumers demo cg1 1) 1) \"name\" 2) \"c1\" 3) \"pending\" 4) (integer) 3 # 已经变成 3 条了，说明返回了一条。 5) \"idle\" 6) (integer) 886880 # 处理完所有信息 127.0.0.1:6379> xack demo cg1 1570513764246-0 1570513767145-0 1570514266119-0 (integer) 3 127.0.0.1:6379> xinfo consumers demo cg1 1) 1) \"name\" 2) \"c1\" 3) \"pending\" 4) (integer) 0 # 现在已经没有未处理的信息了 5) \"idle\" 6) (integer) 1089756 注意点 Stream 消息太多 Redis 提供了一个定长功能。指定 Stream 的最大长度。 # 当前长度为 4 127.0.0.1:6379> xlen demo (integer) 4 # 添加一个新的消息，并且将最大长度设置为 3 127.0.0.1:6379> xadd demo maxlen 3 * name xiaorui age 1 \"1570517209621-0\" # 再次查看长度变为了 3，老消息被砍掉 127.0.0.1:6379> xlen demo (integer) 3 消息如果忘记 ACK Stream 保存了正在处理中的消息 id 列表 PEL，如果忘记处理，则保存 PEL 会不断增加。 PEL 如何避免消息丢失? 当客户端读取 Stream 消息时，Redis 服务器将消息回复给客户端的过程中，网络断开，消息就丢失了。但是 PEL 保存了已经发出去的消息 id。待重新连接后，可以再次接受 PEL 中的消息 ID 列表。不过此时 xreadgroup 的起始消息 id 不能用 >,而是任意有效的消息 id，一般为 0-0，表示读取所有的 PEL 消息以及自 last_delivered_id 之后的新消息。 Stream 的高可用 Stream 的高可用是建立在主从复制基础上的。支持 sentinel 和 cluster 集群环境下高可用方案。 但是由于 Redis 的主从复制是异步的，所以在 failover 发生时，Redis 可能失去极小部分数据，这点和 Redis 其他数据结构一样。 使用 PHP 操纵 Stream laravel 需要使用 phpredis，predis 不支持。 安装 phpredis # 下载源码 git clone https://github.com/phpredis/phpredis.git # 执行，如果失败通过 使用 sudo apt-get install php7.2-dev phpize ./configure sudo make sudo make install # 编译好的文件存在 ./module 下 redis.sos 将 redis.so 放入 /usr/lib/php/20170718/ 下 在 /etc/php/7.2/fpm/conf.d/ 创建 20-redis.ini 文件。 并在文件中添加 extension=redis.so 保存后重启 php-fpm 和 nginx service php7.2-fpm restart service nginx restart # 查看是否安装成功 php -m|grep redis 使用 修改 laravel 中的 config/database.php [ // 'client' => 'predis', 'client' => 'phpredis', 'cluster' => true, 'options' => [ 'cluster' => 'redis' ], 'default' => [ 'host' => env('REDIS_HOST_FIRST', '127.0.0.1'), 'password' => env('REDIS_PASSWORD', null), 'port' => env('REDIS_PORT', 6379), 'database' => 1, 'read_timeout' => env('REDIS_TIMEOUT', 5), ], 'clusters' => [ 'mycluster1' => [ [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6381, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6382, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6383, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6384, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6385, 'database' => 0, ], [ 'host' => '127.0.0.1', 'password' => null, 'port' => 6386, 'database' => 0, ], ], ], ], 操作 del('codehole'); $redis->xGroup('DESTROY', 'codehole', 'cg1'); $redis->xAdd('codehole','*',['name'=>'laoqian','age'=>'30']); $redis->xAdd('codehole','*',['name'=>'xiaoyu','age'=>'29']); $redis->xAdd('codehole','*',['name'=>'xiaoqian','age'=>'1']); $xrange = $redis->xrange('codehole','-','+'); $xgroup1 = $redis->xGroup('Create','codehole','cg1','0',true); $xlen = $redis->xLen('codehole'); $xinfoGroups = $redis->xInfo('GROUPS','codehole'); $xinfoStreams = $redis->xInfo('STREAM','codehole'); dump($xgroup1); dump($xlen); dump($xrange); dump($xinfoGroups); dump($xinfoStreams); $xread1 = $redis->xReadGroup('cg1','c1',['codehole'=>'>'],1,1); $xread2 = $redis->xReadGroup('cg1','c1',['codehole'=>'>'],1,1); $xread3 = $redis->xReadGroup('cg1','c1',['codehole'=>'>'],1,1); dump($xread1); dump($xread2); dump($xread3); dump($redis->xInfo('GROUPS','codehole')); $xreadkey1 = array_keys($xread1['codehole'])[0]; $xreadkey2 = array_keys($xread2['codehole'])[0]; $xreadkey3 = array_keys($xread3['codehole'])[0]; $xack1 = $redis->xAck('codehole','cg1',[$xreadkey1]); $xack2 = $redis->xAck('codehole','cg1',[$xreadkey2]); $xack3 = $redis->xAck('codehole','cg1',[$xreadkey3]); dump($xack1); dump($xack2); dump($xack3); dump($redis->xInfo('GROUPS','codehole')); exit; Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/16.Info.html":{"url":"Redis/16.Info.html","title":"16.Info","keywords":"","body":"16.Info 简介 Info 指令可以获取 Redis 的信息，分别是： Server - 服务器运行环境参数 Clients - 客户端相关信息 Memory - 服务器运行内存统计数据 Persistence - 持久化信息 Stats - 通用统计数据 Replication - 主从复制相关信息 CPU - CPU 信息 Cluster - 集群信息 KeySpace - 键值对统计数量信息 # 获取所有信息 > info # 获取内存相关信息 > info memory # 获取复制相关信息 > info replication 每秒执行多少次指令 # ops_per_sec: operations per second，也就是每秒操作数 > redis-cli info stats |grep ops instantaneous_ops_per_sec:789 极限情况下，Redis 每秒能处理 10W 条指令，CPU 被完全榨干。 如果 qps 过高，可以通过 monitor 来查看哪些 key 被访问的比较频繁，业务上进行优化。 > redis-cli monitor Redis 连接了多少客户端 127.0.0.1:6379> info clients # Clients connected_clients:3 # 连接了 3 个客户端 client_recent_max_input_buffer:2 client_recent_max_output_buffer:0 blocked_clients:0 然后可以通过 client list 来查看有哪些客户端 127.0.0.1:6379> client list id=8 addr=127.0.0.1:47512 fd=8 name= age=958 idle=39 flags=O db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=monitor id=10 addr=127.0.0.1:47516 fd=9 name= age=822 idle=39 flags=O db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=monitor id=13 addr=127.0.0.1:47522 fd=10 name= age=553 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client 这里还有个参数 rejected_connections ，它表示因为超出最大连接数而被拒绝的客户端连接次数，如果数字过大，就要调整配置文件 maxclients 参数。 vagrant@homestead:~$ redis-cli info stats |grep reject rejected_connections:0 Redis 内存多大 vagrant@homestead:~$ redis-cli info memory |grep used |grep human used_memory_human:875.09K # 内存分配器 (jemalloc) 从操作系统分配的内存总量 used_memory_rss_human:5.24M # 操作系统看到的内存占用 ,top 命令看到的内存 used_memory_peak_human:875.99K # Redis 内存消耗的峰值 used_memory_lua_human:37.00K # lua 脚本引擎占用的内存大小 如果单个 Redis 占用空间过大，就可以考虑集群。 复制积压缓冲区多大 vagrant@homestead:~$ redis-cli info replication |grep backlog repl_backlog_active:0 repl_backlog_size:1048576 # 这个就是积压缓冲区大小 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 复制积压缓冲区就是前面所说的复制 buffer。它的大小非常重要，影响主从复制的效率，设置不恰当会导致「增量同步-快照同步」无限循环。 首先，复制积压缓冲区是个环形的，后面的指令会覆盖前面的指令。 如果从库断开时间过长，或者缓冲区设置太小，导致从库无法快速恢复主从同步过程，导致主从全量同步，非常耗资源和时间。 如果有多个从库复制，复制积压缓冲区是共享的，不会因为增加从库而线性增长大小。一般几十 M 就够用了。 vagrant@homestead:~$ redis-cli info stats|grep sync sync_full:0 sync_partial_ok:0 sync_partial_err:0 # 半同步失败次数 sync_partial_err 表示主从同步复制失败次数。如果 sync_partial_err 过大，可以考虑扩大积压缓冲区。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:44 "},"Redis/17.再谈分布式锁.html":{"url":"Redis/17.再谈分布式锁.html","title":"17.再谈分布式锁","keywords":"","body":"17.再谈分布式锁 单点 Redis 锁 复习一下，单点 Redis 锁。 加锁 set [key] [value] [EX second|PX millisecond] [NX|XX] EX|PX：过期时间 NX：若 key 不存在，则设置。 XX：若 key 存在，则设置。 删除锁 if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 缺陷：如果实例挂了，依赖它的服务将都挂掉。 主从架构 如果把单点 Redis 锁变成主从模式。那么可能会存在这样的情况。 客户端 A 申请在 master 上获取锁。 master 同步数据给 slave 时挂掉了。没有同步成功 slave 取代 master。 客户端 B 申请获取锁，获取成功。分布式锁失效。 redlock 使用 redlock 需要多个实例，这些实例间没有主从关系。 加锁时，会向过半节点发送 set(key, value, nx=True, ex=xxx) 指令,只要过半节点 set 成功，那就加锁成功。不过中间还考虑了出错重试和时间漂移的问题。 删除时，直接像所有节点发送 del 指令。 redlock 需要引入额外的库，并且性能下降（毕竟操作多个节点）。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:44 "},"Redis/18.过期策略.html":{"url":"Redis/18.过期策略.html","title":"18.过期策略","keywords":"","body":"18.过期策略 Redis 所有数据结构都可以设置过期时间，但是有没有想过，如果大量的 key 在同一时间过期，而 Redis 又是单线程，会不会因为处理这些过期 key 导致线上服务暂时卡顿。 过期的 key 集合 Redis 会将每个设置了过期时间的 key 单独放入一个字典中。以后会定期扫描这个字典来删除过期的 key。 除了会定时删除过期的 key 之外，还会使用惰性删除来删除过期的 key。 惰性删除：当客户端访问这个 key 时，会检测 key 是否过期，如果过期直接删除。 定时删除就是集中处理，惰性删除是零散处理。 定时扫描策略 Redis 默认每秒进行 10 次过期扫描，但是不是全量扫描，而是采用另一种策略： 从过期字典中随机取 20 个 key。 删除这 20 个 key 中过期的。 如果删除的 key 比例没有超过 1/4，则重复执行步骤 1。 可能会发生这种情况，如果每次都没有超过 1/4,那岂不是会无限循环下去。导致线程卡死。导致线程卡死的另一个原因还可能是，内存管理器需要频繁回收内存，这也会产生 CPU 消耗。 为了保证不过度循环，导致线程卡死，算法上增加了扫描时间上限，默认不超过 25ms。 当有大量的 key 在同一时间过期，恰好客户端有请求过来，那么客户端至少需要等待 25ms，如果客户端将超过时间设置的比较短如 10ms，那么将会有大量的连接因为超时而关闭，导致服务异常。这个时候也没办法通过 slow log 查看，因为慢查询指的是逻辑处理过程慢，不包含等待时间。 所以为了解决这个问题，开发人员可以在设置过期时间时，增加一个随机范围，来确保 key 不会在同一时间过期。 $redis->set('key','value',[\"nx\",\"px\"=>100000+random_int(0,3000)); 从库的过期策略 从库的过期策略是被动的，是根据主库的 del 指令同步到从库，从而从库进行 key 的删除。但是有可能存在 del 指令没有同步到从库，导致主从 key 不一致。这也是导致分布式锁算法漏洞的原因。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-17 19:28:44 "},"Redis/19.LRU-淘汰算法.html":{"url":"Redis/19.LRU-淘汰算法.html","title":"19.LRU-淘汰算法","keywords":"","body":"19.LRU-淘汰算法 简介 当 Redis 超出内存限制时，内存数据会开始与磁盘发生频繁交换（swap）。交换行为会让 Redis 的性能急剧下降，这样的龟速可以说 Redis 不可用。 可以通过配置参数 「maxmemory」 来控制 Redis 内存超出的期望大小。 当实际内存超过 maxmemory 时，Redis 提供了 6 种策略（maxmemory-policy）来让用户决定该如何腾出空间以提供持续写服务： noeviction（默认）：不会继续提供写服务，DEL 和读请求可以继续进行。保证数据不丢失，但是会让线上业务无法继续进行。 volatile-lru：尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证持久化的数据不会突然丢失。 volatile-ttl：跟上面一样，除了淘汰策略不是 LRU，而是 key 的剩余寿命 tll，ttl 越小越先被淘汰。 volatile-random：跟上面一样，不过淘汰的 key 是过期 key 集合中随机选出的。 allkeys-lru：区别于 volatile-lru，这个淘汰策略针对所有 key 而不止过期的 key，这意味着持久化的 key 也会被淘汰。 allkeys-random：跟上面一样，不过淘汰策略是随机的全体 key。 volatile-xxx 策略只针对带有过期时间的 key。 allkeys-xxx 策略针对的是所有 key。 所以，如果 Redis 是做缓存，那么应该使用 allkeys；如果还想做持久化那么必须采用 volatile-xxx 策略。 在 Redis 4.0 之后新增了两个算法： volatile-lfu：跟上面一样，不过淘汰的是，使用频率最小的有过期时间的 key。 allkeys-lfu：跟上线一样，不过 key 是全体 key。 LRU（Least Recently Used） 算法 实现 LRU 算法： key/value 字典外，和一个链表。 链表中的元素按照一定的顺序进行排列。 当空间满的时候，会踢掉链表尾部的元素。 当字典中的某个元素被访问时，将此元素的位置移动到表头。 位于链表尾部的元素就是访问频率较少的元素，会被踢掉。表头的元素是访问频繁的元素，所以不会被踢。 所以链表的顺序就是最近被访问的时间顺序。 近似 LRU 算法 Redis 采用的事一种近似的 LRU 算法。LRU 算法由于需要消耗大量额外内存，还需要对现有的数据结构进行较大改造，所以不使用。 近似的 LRU 算法很简单，在现有的数据结构基础上使用随机采样法来淘汰元素，就能达到与 LRU 算法近似的效果。 Redis 为实现近似 LRU 算法，它给每个 key 增加一个额外的小字段，这个字段长度 24 bit，也就是最后一次被访问的时间戳。 LRU 只进行懒惰处理。当 Redis 执行写操作时，发现内存超出了 maxmemory，就会执行一次 LRU 淘汰算法。随机取出 5（可以配置） 个 key，然后淘汰掉最旧的 key，如果淘汰后的内存还是超出 maxmemory，则继续执行，直到内存低于 maxmemory。 如何采样就看 maxmemory-policy 的配置（前面说的 6 种）。 图中绿色的事新加入的 key，深灰色是老 key，浅灰色是被淘汰的 key。 可以看出，采样数量越大，近似 LRU 算法的效果越接近严格 LRU 算法。同时 Redis 3.0 在算法中增加了淘汰池，进一步提升了近似 LRU 算法的效果。 淘汰池也是一个数组，它的大小是 maxmemory-samples，在每一次淘汰循环中，新随机出来的 key 列表会和淘汰池中的 key 列表进行融合，淘汰掉最旧的一个 key 之后，剩余的较旧的 key 列表放入淘汰池中等待下一个循环使用。 PHP 版实现 LRU 算法 自己写的 php 版的实现方式，很简洁 _maxLen = $maxLen; $this->_capacity = []; } public function set($key) { // 如果容器中有当前 key,则删除,并添加到容器头部 if (in_array($key, $this->_capacity)) { $this->get($key); } elseif (count($this->_capacity) _maxLen) { // 如果容器长度小于最大长度,则直接添加 $this->add($key); } else { // 否则,删除末尾元素,并将新元素放入头部 array_shift($this->_capacity); $this->add($key); } return $this->_capacity; } public function get($key) { if (in_array($key, $this->_capacity)) { unset($this->_capacity[$key]); $this->add($key); } return $key; } public function add($key) { $this->_capacity[$key] = $key; } } 使用 set(1)); dump($server->set(2)); dump($server->set(3)); dump($server->set(4)); dump($server->get(1)); dump($server->_capacity); dump($server->set(5)); exit; } 结果: array:1 [▼ 1 => 1 ] array:2 [▼ 1 => 1 2 => 2 ] array:3 [▼ 1 => 1 2 => 2 3 => 3 ] array:4 [▼ 1 => 1 2 => 2 3 => 3 4 => 4 ] 1 array:4 [▼ 2 => 2 3 => 3 4 => 4 1 => 1 ] array:5 [▼ 2 => 2 3 => 3 4 => 4 1 => 1 5 => 5 ] 在 github 上看到一个更好实现方式：https://github.com/rogeriopvl/php-lrucache PHP 版实现 LFU 算法 _maxLen = $maxLen; $this->_capacity = []; } public function set($key) { // 如果容器中有当前 key,则删除,并添加到容器头部 if (in_array($key, $this->_capacity)) { $this->get($key); } elseif (count($this->_capacity) _maxLen) { // 如果容器长度小于最大长度,则直接添加 $this->add($key); } else { // 否则,删除末尾元素,并将新元素放入头部 array_shift($this->_capacity); $this->add($key); } return $this->_capacity; } public function get($key) { if (in_array($key, $this->_capacity)) { unset($this->_capacity[$key]); $this->add($key); } return $key; } public function add($key) { $this->_capacity[$key] = $key; } } set(1)); dump($server->set(2)); dump($server->set(3)); dump($server->set(3)); dump($server->set(4)); dump($server->get(1)); dump($server->get(1)); dump($server->get(2)); dump($server->get(2)); dump($server->get(2)); dump($server->get(3)); dump($server->_capacity); dump($server->set(5)); exit; 结果： array:1 [▼ 1 => 1 ] array:2 [▼ 1 => 1 2 => 1 ] array:3 [▼ 1 => 1 2 => 1 3 => 1 ] array:3 [▼ 1 => 1 2 => 1 3 => 2 ] array:4 [▼ 1 => 1 2 => 1 3 => 2 4 => 1 ] 1 1 2 2 2 3 array:4 [▼ 2 => 4 1 => 3 3 => 3 4 => 1 ] array:5 [▼ 2 => 4 1 => 3 3 => 3 4 => 1 5 => 1 ] Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/20.懒惰删除.html":{"url":"Redis/20.懒惰删除.html","title":"20.懒惰删除","keywords":"","body":"20.懒惰删除 Redis 是单线程，单线程为 Redis 带来了代码的简洁性和丰富多样的数据结构。不过 Redis 的内部实际上不止一个主线程，还有几个异步线程来专门处理一些耗时的操作。 为什么要懒惰删除（lazy free） 删除指令 del，是非常快的操作，没有延迟。但是当如果一个 key 非常的大，如包含了千万元素的 hash 那么删除就会造成单线程卡顿。 为了解决这个问题，Redis 4.0 引入了 「unlink」指令，它能对删除操作进行懒处理，也就是丢给后台异步回收内存。 127.0.0.1:6379> set name jk4905 OK 127.0.0.1:6379> unlink name (integer) 1 可能有人会担心，如果多个线程同时修改数据结果该怎么办。关于这个，可以这样理解， Redis 的内存所有有效数据当做一个大树，当执行了 unlink 之后，它只是把这个大树的一个树枝折断了，扔到火堆里焚烧（异步线程池），树枝离开大树的一瞬间，它就不能再被主线程的其他指令访问到了，因为主线程只会沿着这个大树访问。 flush Redis 提供了 「flushdb」 「flushall」 指令，用来清空数据库，但这个也是非常缓慢的操作。 Redis4.0 之后，也将这两个指令异步化，在指令后加入 「async」参数就可以将整颗大树连根拔起。 127.0.0.1:6379> flushall async OK 异步队列 主线程将对象的引用从「大树」中摘除后，会将这个 key 的内存回收操作包装成一个任务，塞进异步任务队列，后台线程会读取这个异步任务队列中的任务。 不是所有的 「unlink」操作都会延后处理，如果对应的 key 占用非常的小，还是会想 del 指令一样，立刻执行回收。 AOF Sync 也很慢 Redis 需要每秒一次（可配置）的同步 AOF 日志到磁盘，确保消息不丢失，需要调用 sync 函数，但是这个操作会比较耗时，会导致主线程的效率下降，所以 Redis 也将这个操作异步化。执行 AOF Sync 操作是一个独立线程（区别于前面的懒惰删除），同样也有属于自己的任务队列，队列中只用来存放 AOF Sync 任务。 更多异步删除点 Redis 回收内存除了 del 和 flush 之外，还会对过期的 key，LRU 淘汰、rename 指令以及从库全量同步接收完后立即执行的 flush 操作。 Redis3.0 还带来异步删除机制，打开这些需要额外的配置选项： slave-lazy-flush 从库接受完 rdb 文件后的 flush 操作 lazyfree-lazy-eviction 内存达到 maxmemory 时进行淘汰 lazyfree-lazy-expire key 过期删除 lazyfree-lazy-server-del rename 指令删除 destKey Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/21.Redis 的安全.html":{"url":"Redis/21.Redis 的安全.html","title":"21.Redis 的安全","keywords":"","body":"21.Redis 的安全 指令安全 Redis 中有一些非常危险的指令，如 keys 指令造成卡顿，flush 指令会让数据清空。 Redis 为了避免这些问题，提供了 rename-command 指令用于将某些危险的饿指令修改成特别的名称。避免人为无操作。 在配置文件中 security 块中增加下面的内容： # 可以自行修改命令 rename-command flushall allflushall rename-command keys \"searchkeys\" # 如果想完全封杀某条指令则可以使用这个 rename-command flushdb \"\" 127.0.0.1:6379> flushall (error) ERR unknown command `flushall`, with args beginning with: 127.0.0.1:6379> allflushall OK 127.0.0.1:6379> keys * (error) ERR unknown command `keys`, with args beginning with: `*`, 127.0.0.1:6379> searchkeys name (empty list or set) 127.0.0.1:6379> flushdb (error) ERR unknown command `flushdb`, with args beginning with: 端口安全 Redis 默认会监听 *:6379,很容易被黑客扫描出来。一旦被扫描出来，就可以通过外网访问，内部数据就丧失了安全性。 所以需要指定监听的 ip 地址。增加密码访问限制，这样就算知道了 ip 和端口也无法对 Redis 进行操作。 # 绑定指定的监听地址 bind 10.100.20.13 # 增加密码 requirepass yoursecurepasswordhereplease 密码控制也会影响到从库复制，从库必须在配置文件中使用 masterauth 指令配置对应的密码才可以进行复制操作。 masterauth yoursecurepasswordhereplease lua 脚本 必须禁止 lua 脚本由用户输入的内容生成，因为可能会被黑客利用植入攻击代码。 必须要让 Redis 以普通身份启动，这样即使存在恶意代码黑客也无法拿到 root 权限。 SSL 代理 Redis 不支持 SSL 链接，意味着客户端和服务器之间交互的数据不应该直接暴露在公网上传输，否则会有被窃听的风险，如果必须要使用公网，可以考虑 SSL 代理。 SSL 代理常见的事 ssh，不过 Redis 官方推荐使用 spiped 工具，因为其功能单一简单容易理解。 spiped 对 ssh 通道进行二次加密（因为 ssh 通道也可能存在 bug） 同样 SSL 代理可以用在主从复制上，如果 Redis 主从实例需要跨机房，也可以使用 spiped。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/22.spiped 原理.html":{"url":"Redis/22.spiped 原理.html","title":"22.spiped 原理","keywords":"","body":"22.spiped 原理 简介 假如公司有两个机房，因为紧急需求需要跨机房读取 Redis 数据。应用部署在 A 机房，存储在 B 机房。如果使用普通的 tcp 连接，会将传输的数据暴露在公网，非常不安全，容易被窃听。 当有了 SSL 代理软件，就想给 Redis 穿上了隐形的外套。spiped 是 Redis 官方推荐的 SSL 代理软件。 原理 左边的 spiped 进程 A 负责接收来自 Client 发送过来的请求，加密后传给右边的 spiped 进程 B。spiped B 接收到数据后解密传递给 Server。然后 Server 再走一次反向流程回复给 Client。 spiped 进程需要成对出现，相互之间使用相同的共享秘钥来加密解密消息。 安装 mac 用户 > brew install spiped linux > apt-get install spiped > yum install spiped 生成随机秘钥文件 # 随机的 32 个字节 > dd if=/dev/urandom bs=32 count=1 of=spiped.key 1+0 records in 1+0 records out 32 bytes transferred in 0.000079 secs (405492 bytes/sec) > ls -l rw-r--r-- 1 qianwp staff 32 7 24 18:13 spiped.key 使用密钥文件启动服务器 spiped 进程，172.16.128.81是我本机的公网 IP 地址； # -d 表示 decrypt(对输入数据进行解密)，-s 为源监听地址，-t 为转发目标地址 > spiped -d -s '[172.16.128.81]:6479' -t '[127.0.0.1]:6379' -k spiped.key > ps -ef|grep spiped 501 30673 1 0 7:29 下午 ?? 0:00.04 spiped -d -s [172.16.128.81]:6479 -t [127.0.0.1]:6379 -k spiped.key 这个 spiped 进程监听公网 IP 的 6479 端口接收公网上的数据，将数据解密后转发到本机回环地址的 6379 端口，也就是 redis-server 监听的端口。 使用密钥文件启动客户端 spiped 进程，172.16.128.81是我本机的公网 IP 地址； # -e 表示 encrypt，对输入数据进行加密 > spiped -e -s '[127.0.0.1]:6579' -t '[172.16.128.81]:6479' -k spiped.key > ps -ef|grep spiped 501 30673 1 0 7:29 下午 ?? 0:00.04 spiped -d -s [172.16.128.81]:6479 -t [127.0.0.1]:6379 -k spiped.key 501 30696 1 0 7:30 下午 ?? 0:00.03 spiped -e -s [127.0.0.1]:6579 -t [172.16.128.81]:6479 -k spiped.key Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/23.LUA 脚本执行原理.html":{"url":"Redis/23.LUA 脚本执行原理.html","title":"23.LUA 脚本执行原理","keywords":"","body":"23.LUA 脚本执行原理 简介 有些时候，Redis 的指令可能不满足于我们的需要，所以 Redis 提供了 Lua 脚本支持。 Redis 会单线程原子性的执行 lua 脚本，确保执行过程中，不会被其他请求中断。 首先，编写好脚本如， if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 然后单行化，执行。 127.0.0.1:6379> set foo bar OK 127.0.0.1:6379> eval 'if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end' 1 foo bar (integer) 1 127.0.0.1:6379> eval 'if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end' 1 foo bar (integer) 0 EVAL 指令的第一个参数是脚本字符串，第二个参数是需要传入参数的数量，然后是 key 串和对应的 value 串 EVAL SCRIPT KEY_NUM KEY1 KEY2 ... KEYN ARG1 ARG2 .... SCRIPT LOAD 和 EVALSHA 指令 当如果脚本内容很长，并且需要频繁执行的话，每次都需要传冗长的脚本势必会造成网络资源浪费，所以 Redis 提供了 SCRIPT LOAD 和 EVALSHA 指令。 SCRIPT LOAD 会将脚本内容传入服务器，但是不执行，并且得到一个 sha1 算法算出的字符串，这个字符串就是传入脚本的 id。 然后，通过 EVALSHA 指令来反复执行这个脚本。 # 加载脚本 127.0.0.1:6379> script load 'local curVal = redis.call(\"get\", KEYS[1]); if curVal == false then curVal = 0 else curVal = tonumber(curVal) end; curVal = curVal * tonumber(ARGV[1]); redis.call(\"set\", KEYS[1], curVal); return curVal' \"be4f93d8a5379e5e5b768a74e77c8a4eb0434441\" # 得到 id # 执行 127.0.0.1:6379> evalsha be4f93d8a5379e5e5b768a74e77c8a4eb0434441 1 notexistskey 5 (integer) 0 127.0.0.1:6379> evalsha be4f93d8a5379e5e5b768a74e77c8a4eb0434441 1 notexistskey 5 (integer) 0 127.0.0.1:6379> set foo 1 OK 127.0.0.1:6379> evalsha be4f93d8a5379e5e5b768a74e77c8a4eb0434441 1 foo 5 (integer) 5 127.0.0.1:6379> evalsha be4f93d8a5379e5e5b768a74e77c8a4eb0434441 1 foo 5 (integer) 25 错误处理 上面的脚本必须传整数，如果不是整数则会报错。 127.0.0.1:6379> evalsha be4f93d8a5379e5e5b768a74e77c8a4eb0434441 1 foo bar (error) ERR Error running script (call to f_be4f93d8a5379e5e5b768a74e77c8a4eb0434441): @user_script:1: user_script:1: attempt to perform arithmetic on a nil value 当 Redis 报错时，前面执行了的 redis.call 产生的影响是无法 rollback 。 lua 的替代方案是内置了 pcall(f) 函数调用。pcall 的意思是 protected call，它会让 f 函数运行在保护模式下，f 如果出现了错误，pcall 调用会返回 false 和错误信息。而普通的 call(f) 调用在遇到错误时只会向上抛出异常。在 Redis 的源码中可以看到 lua 脚本的执行被包裹在 pcall 函数调用中。 错误传递 当使用 call 函数时出错了，只会得到一个通用的错误 127.0.0.1:6379> hset foo x 1 y 2 (integer) 2 127.0.0.1:6379> eval 'return redis.call(\"incr\", \"foo\")' 0 (error) ERR Error running script (call to f_8727c9c34a61783916ca488b366c475cb3a446cc): @user_script:1: WRONGTYPE Operation against a key holding the wrong kind of value 当我们将 call 换成 pcall 则会给出具体的错误提示。 127.0.0.1:6379> eval 'return redis.pcall(\"incr\", \"foo\")' 0 (error) WRONGTYPE Operation against a key holding the wrong kind of value 脚本死循环怎么办？ 127.0.0.1:6379> eval 'while(true) do print(\"hello\") end' 0 当执行上面的命令后，Redis 会出现明显卡死。打开 redis 服务器日志可以看到疯狂输出 hello 。这个时候需要另开一个 redis-cli 执行 script kill 指令。 127.0.0.1:6379> script kill OK (2.58s) 而原窗口则会： 127.0.0.1:6379> eval 'while(true) do print(\"hello\") end' 0 (error) ERR Error running script (call to f_d395649372f578b1a0d3a1dc1b2389717cadf403): @user_script:1: Script killed by user with SCRIPT KILL... (6.99s) 我们可能会发现有以下问题： script kill 为什么执行了 2.58 秒。 redis 明明被卡死，怎么可以执行 script kill。 redis-cli 建立链接有点慢，大约顿了 1 秒。 Script Kill 的原理 lua 脚本提供了各种钩子函数，它允许在内部虚拟机执行指令时运行钩子代码。比如每执行 N 条指令执行一次某个钩子函数。 Redis 在钩子函数里会忙里偷闲处理客户端的请求，并且只有发现 lua 脚本执行超时之后才会去处理请求，这个时间默认是 5s。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/24.命令行工具.html":{"url":"Redis/24.命令行工具.html","title":"24.命令行工具","keywords":"","body":"24.命令行工具 执行单条命令 如果输出内容较大，可以输出到文件中。 $ redis-cli info > info.txt $ wc -l info.txt 120 info.txt 可以指定某个库 # -n 2 表示使用第2个库，相当于 select 2 $ redis-cli -h localhost -p 6379 -n 2 ping PONG 批量执行命令 使用管道将 cat 命令的标准输出连接到 redis-cli 的标准输入中 $ cat cmds.txt set foo1 bar1 set foo2 bar2 set foo3 bar3 ...... $ cat cmds.txt | redis-cli OK OK OK ... 或者使用 输入重定向 $ redis-cli set 多行字符串 可以使用 -x 选项，将标准输入的内容作为最后一个参数 $ cat str.txt Ernest Hemingway once wrote, \"The world is a fine place and worth fighting for.\" I agree with the second part. $ redis-cli -x set foo 重复执行指令 // 间隔1s，执行5次，观察qps的变化 $ redis-cli -r 5 -i 1 info | grep ops instantaneous_ops_per_sec:43469 instantaneous_ops_per_sec:47460 instantaneous_ops_per_sec:47699 instantaneous_ops_per_sec:46434 instantaneous_ops_per_sec:47216 如果将次数设置为 -1，那么将一直执行下去，如果不设置 -i 选项，那么将不会有间隔，直接输出 5 次。 在交互模式下也可以这样做 127.0.0.1:6379> 5 ping PONG PONG PONG PONG PONG # 下面的指令很可怕，你的屏幕要愤怒了 127.0.0.1:6379> 10000 info ....... 导出 csv redis 不能一次性导出所有内容为 csv，不过单条指令的可以。 $ redis-cli rpush lfoo a b c d e f g (integer) 7 $ redis-cli --csv lrange lfoo 0 -1 \"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\" $ redis-cli hmset hfoo a 1 b 2 c 3 d 4 OK $ redis-cli --csv hgetall hfoo \"a\",\"1\",\"b\",\"2\",\"c\",\"3\",\"d\",\"4\" --csv ，只是将输出结果用「逗号」分割。 执行 lua 脚本 $ cat mset.txt return redis.pcall('mset', KEYS[1], ARGV[1], KEYS[2], ARGV[2]) $ cat mget.txt return redis.pcall('mget', KEYS[1], KEYS[2]) $ redis-cli --eval mset.txt foo1 foo2 , bar1 bar2 OK $ redis-cli --eval mget.txt foo1 foo2 1) \"bar1\" 2) \"bar2\" 监控服务器状态 可以使用 -i 来控制输出间隔（单位 s）。 $ redis-cli --stat ------- data ------ --------------------- load -------------------- - child - keys mem clients blocked requests connections 2 6.66M 100 0 11591628 (+0) 335 2 6.66M 100 0 11653169 (+61541) 335 2 6.66M 100 0 11706550 (+53381) 335 2 6.54M 100 0 11758831 (+52281) 335 2 6.66M 100 0 11803132 (+44301) 335 2 6.66M 100 0 11854183 (+51051) 335 扫描大 KEY 使用 --bigkeys 可以很快扫描出内存中的大 key，使用 -i 控制间隔，避免扫描过程中 ops 徒增报警。 agrant@homestead:/etc/redis$ redis-cli --bigkeys -i 0.01 # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest string found so far 'foo1' with 4 bytes [00.00%] Biggest list found so far 'lfoo' with 7 items -------- summary ------- Sampled 4 keys in the keyspace! Total key length in bytes is 16 (avg len 4.00) Biggest list found 'lfoo' has 7 items Biggest string found 'foo1' has 4 bytes 1 lists with 7 items (25.00% of keys, avg size 7.00) 0 hashs with 0 fields (00.00% of keys, avg size 0.00) 3 strings with 12 bytes (75.00% of keys, avg size 4.00) 0 streams with 0 entries (00.00% of keys, avg size 0.00) 0 sets with 0 members (00.00% of keys, avg size 0.00) 0 zsets with 0 members (00.00% of keys, avg size 0.00) redis-cli 会对每一种类型都记录最大长度的 key。对于每个对象刷新一次就会立即输出一次，它能保证输出 top1 的key，但是 top2 top3 的无法保证，一般都是多扫几次或者清除top1 的 key 之后再重新扫描一次。 采样服务器指令 如果有一台 redis 服务器 ops 过高，如何判断是哪个业务导致的。可以使用 monitor 指令来判断。 $ redis-cli --host 192.168.x.x --port 6379 monitor 1539853410.458483 [0 10.100.90.62:34365] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.459212 [0 10.100.90.61:56659] \"PFADD\" \"growth:dau:20181018\" \"2klxkimass8w\" 1539853410.462938 [0 10.100.90.62:20681] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.467231 [0 10.100.90.61:40277] \"PFADD\" \"growth:dau:20181018\" \"2kei0to86ps1\" 1539853410.470319 [0 10.100.90.62:34365] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.473927 [0 10.100.90.61:58128] \"GET\" \"6yax3eb6etq8:{-7}\" 1539853410.475712 [0 10.100.90.61:40277] \"PFADD\" \"growth:dau:20181018\" \"2km8sqhlefpc\" 1539853410.477053 [0 10.100.90.62:61292] \"GET\" \"6yax3eb6etq8:{-7}\" 判断服务器时延 虽然 linux 有 ping 命令，可以判断时延。但是 Redis 提供的原理与 ping 不太一样。它不仅判断了当前机器与 Redis 服务器之间的指令时延，还要判断和当前 Redis 主线程是否忙碌有关。如果 ping 命令发现时延很小，但是 Redis 时延很大，说明 Redis 在执行指令时会有轻微卡顿。 $ redis-cli --host 192.168.x.x --port 6379 --latency min: 0, max: 5, avg: 0.08 (305 samples) 时延单位是 ms。redis-cli 还能显示时延的分布情况，而且是图形化输出。 $ redis-cli --latency-dist 远程备份 可以让远程的 Redis 实例备份到本机上，远程就会执行一次 bgsave ，然后将 rdb 传输到客户端。 $ ./redis-cli --host 192.168.x.x --port 6379 --rdb ./user.rdb SYNC sent to master, writing 2501265095 bytes to './user.rdb' Transfer finished with success. 模拟从库 如果想看到主从服务器之间都同步了哪些数据，可以使用 redis-cli 模拟从库 $ ./redis-cli --host 192.168.x.x --port 6379 --slave SYNC with master, discarding 51778306 bytes of bulk transfer... SYNC done. Logging commands from master. ... 从库连上主库的第一件事是全量同步，所以看到上面的指令卡顿这很正常，待首次全量同步完成后，就会输出增量的 aof 日志。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:44:58 "},"Redis/缓存穿透-缓存击穿-缓存雪崩.html":{"url":"Redis/缓存穿透-缓存击穿-缓存雪崩.html","title":"缓存穿透-缓存击穿-缓存雪崩","keywords":"","body":"缓存穿透-缓存击穿-缓存雪崩 正常情况下一个查询请求是这样一个流程。 缓存穿透 缓存穿透是指，查询一个不存在的数据，而用户不断发送请求，不会经过 Redis ，直接查询数据库，导致数据库压力过大。 解决方案： 对于 id 当如果在数据库中没有查到时，可以设置一个值为 null 的 key 放入 redis，下次查询时直接走 redis。避免恶意攻击。 缓存击穿 对于一个设置了过期时间的 key，当有超高并发对其访问时，缓存过期，这时同时读取数据库导致数据库压力过大甚至崩溃。 解决方案： 设置分布式锁。 热点数据永不过期。 请求时，对 key 的剩余时间进行检查，快要过期的 key 加过期时间。 缓存雪崩 缓存设置了相同时间的过期时间。导致某一时刻突然失效，对 redis 服务器造成卡顿阻塞。对数据库产生大量请求，导致数据库压力过大甚至崩溃。（与「缓存击穿」不同的是「缓存击穿是指一个 key 失效」，「缓存雪崩」是不同 key 失效。） 解决办法： 设置缓存时间增加随机值。避免设置成同一过期时间。 热点数据永不过期。 分布式锁解决。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 19:16:28 "},"Redis/缓存的使用.html":{"url":"Redis/缓存的使用.html","title":"缓存的使用","keywords":"","body":"缓存的使用 缓存针对高频率访问的、面向大部分用户的数据。（如商品信息，banner，新闻等等） 对于单个用户的，如（我的订单，我的收藏等等），不要使用缓存，因为这种缓存的命中率很低，浪费空间。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-11-19 22:30:50 "},"Redis/扣库存.html":{"url":"Redis/扣库存.html","title":"扣库存","keywords":"","body":"高并发 秒杀实现 限流 nginx 自带限流， limit_req_zone：限制单位时间的请求数量，漏斗限流算法 limit_req_conn：限制同一时间连接数，并发连接。 扣库存 将库存分为多份，存入 redis ，（然后使用分布式锁），在 redis 中进行扣减，并放入消息队列中执行。此过程全部都在内存中执行，非常快。 最终扣减 消费者拿到数据后，进行 mysql 的扣减内存。 异步相应 客户端轮询，每秒查询一次结果。 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-10-28 21:23:16 "},"Swoole/1.task 和 timer.html":{"url":"Swoole/1.task 和 timer.html","title":"1.task 和 timer","keywords":"","body":"Task 和 Timer 进程 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-09 10:43:52 "},"Swoole/安装 Swoole.html":{"url":"Swoole/安装 Swoole.html","title":"安装 Swoole","keywords":"","body":"安装 Swoole 通过 PECL 安装 pecl channel-update pecl.php.net pecl install swoole 再查找下 php.ini 位置 php -i |grep php.ini 在 php.ini 中添加 extension=swoole.so 最后重启 nginx，查看是否成功加载 swoole php -m | grep swoole Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 14:36:41 "},"Swoole/快速开始 tcp.html":{"url":"Swoole/快速开始 tcp.html","title":"快速开始 tcp","keywords":"","body":"快速开始 TCP 服务器 set([ 'worker_num'=>8, //worker 进程数 = cpu 的 1~4 倍 'max_request'=>10000, // 最大请求数 ]); /** * 监听连接进入事件 * $fd 客户端连接的唯一标识 * $reator_id 线程 id */ $serv->on('Connect', function ($serv, $fd, $reactor_id) { echo \"Client:{$reactor_id} - {$fd}Connect.\\n\"; }); //监听数据接收事件 $serv->on('Receive', function ($serv, $fd, $reactor_id, $data) { $serv->send($fd, \"Server: {$reactor_id} - {$fd}\" . $data); }); //监听连接关闭事件 $serv->on('Close', function ($serv, $fd) { echo \"Client: Close.\\n\"; }); //启动服务器 $serv->start(); worker_num 代表服务启用了几个线程 使用 ps aft|grep server.php 当修改 worker_num 时，再次使用命令可以查看到线程数改变 4种PHP回调函数风格 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 15:31:02 "},"Swoole/快速开始 udp.html":{"url":"Swoole/快速开始 udp.html","title":"快速开始 udp","keywords":"","body":"快速开始 UDP server_udp.php on('Packet', function ($server, $data, $clientInfo) { $server->sendto($clientInfo['address'], $clientInfo['port'], \"Server \" . $data); var_dump($clientInfo); }); // 开始监听 $server->start(); udp 服务不需要连接，直接监听 使用下面命令进行测试 netcat -u 127.0.0.1 5200 结果： 服务器端： vagrant@homestead:~/Code$ php server_udp.php array(4) { [\"server_socket\"]=> int(3) [\"server_port\"]=> int(5200) [\"address\"]=> string(9) \"127.0.0.1\" [\"port\"]=> int(32951) } 客户端： vagrant@homestead:~/Code$ netcat -u 127.0.0.1 5200 11 Server 11 Copyright © Kagami丶 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 16:42:15 "}}